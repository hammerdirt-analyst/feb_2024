{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c16819-72da-4b47-a3aa-988d3f5a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Type, Optional, Callable\n",
    "from typing import List, Dict, Union, Tuple\n",
    "\n",
    "from review_methods_tests import collect_vitals, find_missing, find_missing_loc_dates\n",
    "from review_methods_tests import use_gfrags_gfoams_gcaps, make_a_summary\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "\n",
    "import setvariables as conf_\n",
    "import methods_iqaasl as mi"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4558dc36-9186-4446-a24e-22cd22678f63",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def slice_data_by_date(data: pd.DataFrame, start: str, end: str):\n",
    "    mask = (data.date >= start) & (data.date <= end)\n",
    "    return data[mask]\n",
    "\n",
    "\n",
    "\n",
    "# format_kwargs = dict(precision=2, thousands=\"'\", decimal=\",\")\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "# header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; font-size:12px; text-align:left;'}\n",
    "# even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "# odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "# table_font = {'selector': 'tr', 'props': 'font-size: 10px;'}\n",
    "# table_data = {'selector': 'td', 'props': 'padding:4px; font-size:12px;'}\n",
    "# table_css_styles = [even_rows, odd_rows, table_font, header_row, table_data]\n",
    "\n",
    "def aggregate_dataframe(df: pd.DataFrame,\n",
    "                        groupby_columns: List[str],\n",
    "                        aggregation_functions: Dict[str, Union[str, callable]],\n",
    "                        index: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate specified columns in a Pandas DataFrame using given aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        groupby_columns (List[str]): List of column names to group by.\n",
    "        aggregation_functions (Dict[str, Union[str, callable]]): \n",
    "            A dictionary where keys are column names to aggregate, \n",
    "            and values are either aggregation functions (e.g., 'sum', 'mean', 'max', 'min')\n",
    "            or custom aggregation functions (callable functions).\n",
    "        index (bool, optional): Whether to use the groupby columns as an index.\n",
    "            Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with aggregated values.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(groupby_columns, as_index=index).agg(aggregation_functions)\n",
    "    \n",
    "    return grouped\n",
    "    \n",
    "def merge_dataframes_on_column_and_index(left_df: pd.DataFrame,\n",
    "                                         right_df: pd.DataFrame,\n",
    "                                         left_column: str,\n",
    "                                         how: str = 'inner',\n",
    "                                         validate: str = 'many_to_one') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge two DataFrames where the left DataFrame is merged on a specified column and \n",
    "    the right DataFrame is merged on its index.\n",
    "\n",
    "    Args:\n",
    "        left_df (pd.DataFrame): The left DataFrame to be merged.\n",
    "        right_df (pd.DataFrame): The right DataFrame to be merged on its index.\n",
    "        left_column (str): The column in the left DataFrame to merge on.\n",
    "        how (str, optional): The type of merge to be performed ('left', 'right', 'outer', or 'inner'). \n",
    "            Default is 'inner'.\n",
    "        validate (str, optional): Whether to perform merge validation checks. \n",
    "            Default is 'many_to_one'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame resulting from the merge operation.\n",
    "    \"\"\"\n",
    "  \n",
    "    merged_df = left_df.merge(right_df, left_on=left_column, right_index=True, how=how)\n",
    "    return merged_df\n",
    "\n",
    "def get_top_x_records_with_max_quantity(df: pd.DataFrame, quantity_column: str, id_column: str, x: int):\n",
    "    \"\"\"\n",
    "    Get the top x records with the greatest quantity and their proportion to the total from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        quantity_column (str): The name of the quantity column.\n",
    "        id_column (str): The name of the ID column.\n",
    "        x (int): The number of records to return.\n",
    "\n",
    "    Returns:\n",
    "        A data frame\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by the quantity column in descending order, take the top x records, and select the ID column\n",
    "    top_x_records = df.nlargest(x, quantity_column)[[id_column, quantity_column]]\n",
    "    top_x_records[\"%\"] = top_x_records[quantity_column]/top_x_records[quantity_column].sum()\n",
    "    \n",
    "    return top_x_records[[id_column, quantity_column, \"%\"]]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rate_per_unit(df: pd.DataFrame,\n",
    "                            objects_to_check: List[str],\n",
    "                            column_of_interest: str = \"code\",\n",
    "                            groupby_columns: List[str] = ['code'],\n",
    "                            method: Dict[str, str] = {\"pcs_m\": \"median\", \"quantity\":\"sum\"},\n",
    "                            )-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the rate of occurence of object(s) for a given unit measurement. Adds the label\n",
    "    'all' to each record.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "        objects_to_check (List[str]): The list of objects to calculate proportions for.\n",
    "        column_of_interest (str): The column label of the objects being compared.\n",
    "        groupby_columns Dict[str]: The columns used for the aggregation.\n",
    "        method (Dict[str]): Dictionary specifying the aggregation functions for the unit_measurement.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe where index is column_of_interest and the value column is the rate\n",
    "            and the label is 'all'.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where 'object' is in 'objects_to_check'\n",
    "    filtered_df = df[df[column_of_interest].isin(objects_to_check)]\n",
    "\n",
    "    # Calculate the total quantity for each object\n",
    "    object_rates = filtered_df.groupby(groupby_columns, as_index=False).agg(method)\n",
    "\n",
    "    # Calculate the proportion for each object\n",
    "    rates = object_rates[[column_of_interest, *method.keys()]].set_index(column_of_interest, drop=True)\n",
    "    rates[\"label\"] = \"all\"    \n",
    "\n",
    "    return rates\n",
    "\n",
    "\n",
    "# fail rate: quantity > 0\n",
    "def count_objects_with_positive_quantity(df: pd.DataFrame, value_column: str = 'quantity', object_column: str = 'code') -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count how many times each object had a quantity greater than zero in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the count of positive quantity occurrences for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where quantity is greater than zero\n",
    "    positive_quantity_df = df[df[value_column] > 0]\n",
    "    no_count_df = df[(df[value_column] == 0)]\n",
    "\n",
    "    # Count the occurrences of positive quantities for each object\n",
    "    object_counts = positive_quantity_df[object_column].value_counts()\n",
    "    failed = object_counts/df.loc_date.nunique()\n",
    "\n",
    "    # identify the objects with a zero count\n",
    "    no_counts = no_count_df[object_column].value_counts()\n",
    "    zeroes = no_counts[~no_counts.index.isin(object_counts.index)]\n",
    "    zeroes.loc[:] = 0\n",
    "\n",
    "    return pd.concat([failed, zeroes])\n",
    "\n",
    "# pieces per merter for a set of data\n",
    "def rate_per_unit_cumulative(df: pd.DataFrame, groupby_columns: List, object_labels: List, objects: List, agg_methods: Dict)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate cumulative rates per unit for specific objects and aggregation methods.\n",
    "\n",
    "    This function takes a DataFrame and calculates cumulative rates per unit based on\n",
    "    the specified groupby columns, object labels, objects of interest, and aggregation methods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data for analysis.\n",
    "        groupby_columns (List): List of columns to group by in the DataFrame.\n",
    "        object_labels (List): List of labels to identify objects of interest.\n",
    "        objects (List): List of objects for which cumulative rates are calculated.\n",
    "        agg_methods (Dict): Dictionary specifying aggregation methods for calculating rates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the cumulative rates per unit.\n",
    "\n",
    "    Example:\n",
    "        groupby_columns = ['Region', 'Year']\n",
    "        object_labels = ['Object A', 'Object B']\n",
    "        objects = ['A', 'B']\n",
    "        agg_methods = {'Value': 'sum', 'Count': 'count'}\n",
    "\n",
    "        cumulative_rates = rate_per_unit_cumulative(df, groupby_columns, object_labels, objects, agg_methods)\n",
    "    \"\"\"\n",
    "    parent_summary = aggregate_dataframe(df, groupby_columns, agg_methods)\n",
    "    parent_boundary_summary = calculate_rate_per_unit(parent_summary, object_labels, objects[0], objects)\n",
    "    parent_boundary_summary.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return parent_boundary_summary\n",
    "\n",
    "\n",
    "def aggregate_boundaries(df: pd.DataFrame, unit_columns: list, unit_agg: dict, boundary_labels: list, boundary_columns: list, group_agg: dict)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate data from a dataframe by boundaries and groups.\n",
    "\n",
    "    Aggregates a dataframe in two steps. First, it performs\n",
    "    aggregation at the 'unit' level defined by 'unit_columns' and 'unit_agg' to obtain\n",
    "    test statistics. Then, it aggregates these 'unit' statistics further at the\n",
    "    'boundary' level defined by 'boundary_labels' and 'boundary_columns', and computes\n",
    "    the test statistics for each boundary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data to be aggregated.\n",
    "        unit_columns (list): List of columns for 'unit' level aggregation.\n",
    "        unit_agg (dict): Dictionary specifying the aggregation functions for 'unit' level.\n",
    "        boundary_labels (list): List of boundary labels to define 'boundaries' for further aggregation.\n",
    "        boundary_columns (list): List of columns for 'boundary' level aggregation.\n",
    "        group_agg (dict): Dictionary specifying the aggregation functions for 'boundary' level.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing aggregated data at the 'boundary' level with\n",
    "        additional 'label' column indicating the boundary label. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    unit_aggregate = aggregate_dataframe(df, unit_columns, unit_agg)\n",
    "    boundary_summaries = []\n",
    "    for label in boundary_labels:\n",
    "        boundary_mask = unit_aggregate[unit_columns[0]] == label\n",
    "        boundary_aggregate = unit_aggregate[boundary_mask].groupby(boundary_columns, as_index=False).agg(agg_groups)\n",
    "        boundary_aggregate['label'] = label\n",
    "        boundary_summaries.append(boundary_aggregate)\n",
    "\n",
    "    return pd.concat(boundary_summaries)\n",
    "\n",
    "def color_gradient(val, cmap: ListedColormap = None, min: float = 0.0, max: float = .9):\n",
    "    \"\"\"\n",
    "    Apply a color gradient to a numerical value for cell styling.\n",
    "\n",
    "    This function takes a numerical value 'val' and applies a color gradient based on the provided\n",
    "    colormap ('cmap') and the specified range defined by 'min' and 'max'. It returns a CSS style\n",
    "    for cell background color.\n",
    "\n",
    "    Args:\n",
    "        val (float): The numerical value to be colored using the gradient.\n",
    "        cmap (ListedColormap, optional): The colormap to use for the color gradient. Defaults to None.\n",
    "        min (float, optional): The minimum value of the data range. Defaults to 0.0.\n",
    "        max (float, optional): The maximum value of the data range. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        str: A CSS style string for cell background color and text color.\n",
    "\n",
    "    Example:\n",
    "        # Apply a color gradient using a custom colormap 'cmap' to the DataFrame\n",
    "        df.style.applymap(color_gradient, cmap=my_colormap, min=0.0, max=100.0)\n",
    "    \"\"\"\n",
    "    # print(type(cmap))\n",
    "    # Normalize the value to a range [0, 1] \n",
    "    # min, max should be the min max for the\n",
    "    # data frame in question\n",
    "    normalized_val = (val - min) / max\n",
    "    \n",
    "    r, g, b, a = cmap(normalized_val)\n",
    "\n",
    "    \n",
    "    # Calculate the color based on the normalized value    \n",
    "    hex_color = f\"rgba({int(r*255)},{int(g*255)},{int(b*255)}, .5)\"\n",
    "    \n",
    "       \n",
    "    # Return the CSS style with the background color\n",
    "    return f'background-color: {hex_color}; color:black'\n",
    "\n",
    "\n",
    "def boundary_summary(parent_boundary: pd.DataFrame, boundary_summary: pd.DataFrame, object_columns: List, unit: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a boundary summary DataFrame based on parent and boundary summaries.\n",
    "\n",
    "    This function combines parent and boundary summaries to create a consolidated boundary summary\n",
    "    DataFrame. The aggregation is based on the specified object columns and the 'unit' of interest.\n",
    "\n",
    "    Args:\n",
    "        parent_boundary (pd.DataFrame): The parent boundary summary DataFrame.\n",
    "        boundary_summaries (pd.DataFrame): The boundary summaries for individual objects.\n",
    "        object_columns (List): List of columns identifying the objects.\n",
    "        unit (str): The unit of interest for aggregation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A boundary summary DataFrame that combines parent and individual object summaries.\n",
    "\n",
    "    Example:\n",
    "        parent_boundary = ...\n",
    "        boundary_summaries = ...\n",
    "        object_columns = ['Object']\n",
    "        unit = 'pcs_m'\n",
    "\n",
    "        boundary_result = boundary_summary(parent_boundary, boundary_summaries, object_columns, unit)\n",
    "    \"\"\"\n",
    "    boundary_limits = pd.concat([parent_boundary, boundary_summary])\n",
    "    objects = boundary_limits[object_columns[0]].nunique()\n",
    "    boundaries = boundary_limits.label.nunique()\n",
    "    if objects >= boundaries:\n",
    "        b = boundary_limits.pivot(index=object_columns[0], columns=\"label\", values=unit)\n",
    "        b = b[[*boundary_summary.label.unique(), *parent_boundary.label.unique()]]\n",
    "    else:\n",
    "        b = boundary_limits.pivot(columns=object_columns[0], index=\"label\", values=unit)\n",
    "        \n",
    "    return b\n",
    "\n",
    "\n",
    "def translate_word(X: str, amap: pd.DataFrame, lan: str):\n",
    "    \"\"\"\n",
    "    Translate a word or phrase using a language mapping DataFrame.\n",
    "\n",
    "    This function takes a word or phrase 'X' and attempts to translate it into another language\n",
    "    specified by 'lan' using a language mapping DataFrame 'map'. If the word is found in the index\n",
    "    of the mapping DataFrame, the translation is returned; otherwise, the original word is returned.\n",
    "\n",
    "    Args:\n",
    "        X (str): The word or phrase to be translated.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated word or phrase, or the original word if not found in the mapping.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame for language mapping\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana', 'cherry'],\n",
    "                                    'French': ['pomme', 'banane', 'cerise']})\n",
    "\n",
    "        # Translate a word into French\n",
    "        translated_word = translate_word('apple', language_map, 'French')\n",
    "    \"\"\"    \n",
    "    if X in amap.index:\n",
    "        return amap.loc[X,  lan]\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def capitalize_index(X):\n",
    "    return X.title()\n",
    "\n",
    "def translate_for_display(df: pd.DataFrame, amap: pd.DataFrame, lan: str):\n",
    "    \"\"\"\n",
    "    Translate column names and index labels of a DataFrame for display.\n",
    "\n",
    "    This function takes a DataFrame 'df' and translates its column names and index labels using a\n",
    "    language mapping DataFrame 'map' for display in a specified language 'lan'. The translated\n",
    "    column names are used as new column names in the DataFrame, and the index labels are replaced\n",
    "    with their translations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame for translation.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with translated column names and index labels for display.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame to be translated\n",
    "        data = {'apple': [1, 2, 3], 'banana': [4, 5, 6]}\n",
    "        original_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create a language mapping DataFrame\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana'],\n",
    "                                    'French': ['pomme', 'banane']})\n",
    "\n",
    "        # Translate the column names and index labels for display in French\n",
    "        translated_df = translate_for_display(original_df, language_map, 'French')\n",
    "    \"\"\"\n",
    "    \n",
    "    old_columns = df.columns    \n",
    "    changed_c = [x for x in old_columns if x in amap.index]\n",
    "        \n",
    "    new_columns = {x: amap.loc[x, lan] for x in changed_c}\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    \n",
    "    new_index = {x:translate_word(x, amap, lan) for x in df.index}\n",
    "    df['new_index'] = df.index.map(lambda x: new_index[x])\n",
    "    df.set_index('new_index', drop=True, inplace=True)\n",
    "    df.index.name = None\n",
    "    df.columns.name = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "def translated_and_style_for_display(df, amap, lan, gradient: bool = True):\n",
    "    \"\"\"\n",
    "    Translate, style, and format a DataFrame for display.\n",
    "\n",
    "    This function translates column names and index labels, applies styling, and optionally\n",
    "    adds a color gradient to a DataFrame to prepare it for display in a specified language 'lan'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be translated and styled.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "        gradient (bool, optional): Whether to apply a color gradient to the DataFrame. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Styler: A styled DataFrame ready for display with translated labels and styling.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame to be translated and styled\n",
    "        data = {'apple': [1, 2, 3], 'banana': [4, 5, 6]}\n",
    "        original_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create a language mapping DataFrame\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana'],\n",
    "                                    'French': ['pomme', 'banane']})\n",
    "\n",
    "        # Translate, style, and format the DataFrame for display in French\n",
    "        styled_df = translated_and_style_for_display(original_df, language_map, 'French', gradient=True)\n",
    "    \"\"\"\n",
    "    d = translate_for_display(df, amap, lan)\n",
    "    d = d.style.format(**format_kwargs).set_table_styles(table_css_styles)\n",
    "    if gradient:\n",
    "        d = d.applymap(color_gradient, cmap=newcmp)\n",
    "    return d.format_index(str.title, axis=1).format_index(str.title, axis=0)\n",
    "\n",
    "\n",
    "def display_tabular_data_by_column_values(df, column_one: dict, column_two: dict, index: str):\n",
    "    \"\"\"\n",
    "    Display tabular data based on column values.\n",
    "\n",
    "    This function filters a DataFrame 'df' to include rows where either 'column_one' or 'column_two'\n",
    "    meet specified conditions. The resulting DataFrame is then set to have 'index' as the index, and\n",
    "    the index name is removed for cleaner tabular display.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing tabular data.\n",
    "        column_one (dict): A dictionary specifying the column and value condition for 'column_one'.\n",
    "        column_two (dict): A dictionary specifying the column and value condition for 'column_two'.\n",
    "        index (str): The column to be set as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame with 'index' as the index and the index name removed.\n",
    "\n",
    "    Example:\n",
    "        # Create a sample DataFrame 'data_df'\n",
    "        data_df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "                                'Age': [25, 30, 35],\n",
    "                                'Salary': [50000, 60000, 70000]})\n",
    "\n",
    "        # Define filtering conditions for 'Age' and 'Salary'\n",
    "        column_one = {'column': 'Age', 'val': 30}\n",
    "        column_two = {'column': 'Salary', 'val': 65000}\n",
    "\n",
    "        # Display filtered tabular data by 'Name' where either 'Age' or 'Salary' meets the conditions\n",
    "        filtered_data = display_tabular_data_by_column_values(data_df, column_one, column_two, 'Name')\n",
    "    \"\"\"\n",
    "    d = df[(df[column_one[\"column\"]] >= column_one[\"val\"]) | (df[column_two[\"column\"]] >= column_two[\"val\"])].copy()\n",
    "    d.set_index(index, inplace=True, drop=True)\n",
    "    d.index.name = None       \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def summary_of_parent_and_child_features(df: pd.DataFrame,\n",
    "                                         cumulative_columns: List = None,\n",
    "                                         boundary_labels: List = None,\n",
    "                                         object_labels: List = None,\n",
    "                                         object_columns: List = None,\n",
    "                                         unit_agg: dict = None,\n",
    "                                         unit_columns: List = None,\n",
    "                                         agg_groups: dict = None)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a summary of parent and child features based on a DataFrame.\n",
    "\n",
    "    This function computes a summary of parent and child features based on the provided DataFrame 'df'.\n",
    "    It calculates cumulative values, aggregates boundary summaries, and generates a comprehensive summary\n",
    "    DataFrame that includes both parent and child features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data for analysis.\n",
    "        cumulative_columns (List, optional): List of columns to be considered for cumulative values.\n",
    "        boundary_labels (List, optional): List of labels for boundary summaries.\n",
    "        object_labels (List, optional): List of labels for individual objects.\n",
    "        object_columns (List, optional): List of columns identifying objects.\n",
    "        unit_agg (dict, optional): Aggregation methods for unit summaries.\n",
    "        unit_columns (List, optional): List of columns for unit summaries.\n",
    "        agg_groups (dict, optional): Aggregation methods for boundary summaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of parent and child features with comprehensive information.\n",
    "\n",
    "    Example:\n",
    "        # Define parameters for generating the summary\n",
    "        cumulative_columns = ['quantity', 'total_weight']\n",
    "        boundary_labels = ['Boundary 1', 'Boundary 2']\n",
    "        object_labels = ['Object 1', 'Object 2']\n",
    "        object_columns = ['object_id', 'object_name']\n",
    "        unit_agg = {'quantity': 'sum', 'total_weight': 'mean'}\n",
    "        unit_columns = ['unit_id', 'unit_name']\n",
    "        agg_groups = {'quantity': 'sum', 'total_weight': 'mean'}\n",
    "\n",
    "        # Generate the summary of parent and child features\n",
    "        summary_df = summary_of_parent_and_child_features(data_df, cumulative_columns, boundary_labels,\n",
    "                                                         object_labels, object_columns, unit_agg, unit_columns, agg_groups)\n",
    "    \"\"\"\n",
    "                                            \n",
    "\n",
    "    parent_boundary = rate_per_unit_cumulative(df, cumulative_columns, object_labels, object_columns, unit_agg)\n",
    "    boundary_summaries = aggregate_boundaries(df, unit_columns, unit_agg, boundary_labels, object_columns, agg_groups)\n",
    "    x = boundary_summary(parent_boundary, boundary_summaries, object_columns, \"pcs_m\")\n",
    "\n",
    "    return x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "df4ea30e-5e1c-4265-8baa-38da0eb455d3",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def collect_survey_data_for_report(a_func: Callable = None, **kwargs)-> pd.DataFrame:\n",
    "    \n",
    "    if a_func is not None:\n",
    "        return a_func(**kwargs)\n",
    "    else:\n",
    "        survey_files = conf_.survey_files\n",
    "        data = mi.combine_survey_files(survey_files)\n",
    "    return data\n",
    "\n",
    "def collect_env_data_for_report(a_func: Callable = None, **kwargs)-> pd.DataFrame:\n",
    "    \n",
    "    if a_func is not None:\n",
    "        return a_func(**kwargs)\n",
    "    else:\n",
    "        codes = pd.read_csv(conf_.code_data).set_index(\"code\")\n",
    "        beaches = pd.read_csv(conf_.beach_data).set_index(\"slug\")\n",
    "        land_cover = pd.read_csv(conf_.land_cover_data)\n",
    "        land_use = pd.read_csv(conf_.land_use_data)\n",
    "        streets = pd.read_csv(conf_.street_data)\n",
    "        river_intersect_lakes = pd.read_csv(conf_.intersection_attributes)\n",
    "        \n",
    "    return codes, beaches, land_cover, land_use, streets, river_intersect_lakes\n",
    "\n",
    "def language_maps(func: Callable = None, **kwargs):\n",
    "    if func is not None:\n",
    "        return func(**kwargs)\n",
    "    else:\n",
    "        maps = {k: pd.read_csv(v).set_index('en') for k, v in conf_.language_maps.items()}\n",
    "        return maps\n",
    "\n",
    "def check_for_top_label(alabel: str = None, df: pd.DataFrame = None, a_map: pd.DataFrame = None)-> pd.DataFrame:\n",
    "    if alabel in df.columns:\n",
    "        return df\n",
    "    else:\n",
    "        new_map = a_map[alabel]\n",
    "        newdf = df.merge(new_map, left_on='slug', right_index=True, validate='many_to_one')\n",
    "        return newdf\n",
    "    \n",
    "def use_parent_groups_or_gfrags(df, label: str = None, gfrags: bool = True, parent_group: bool = False, func: Callable = None, **kwargs)-> pd.DataFrame:\n",
    "    \n",
    "    if func is not None:\n",
    "        return func(**kwargs)\n",
    "    if gfrags and parent_group:\n",
    "        d = use_gfrags_gfoams_gcaps(df, codes)\n",
    "    if gfrags and not parent_group:\n",
    "        d = use_gfrags_gfoams_gcaps(df, codes)\n",
    "    \n",
    "    # the surveys need to be aggregated to the object level\n",
    "    # after changeing code names there will be duplicates on\n",
    "    # the columns loc_date and code. Which is not allowed.\n",
    "    groupby_cols = list(set([label, *conf_.code_result_columns]))\n",
    "    d = aggregate_dataframe(d, groupby_cols, conf_.unit_agg)\n",
    "    \n",
    "    return d\n",
    "def add_column_to_work_data(df, key: str = 'slug', feature: str = None, amap: pd.DataFrame = None)-> pd.DataFrame:\n",
    "    \n",
    "    d = df.merge(amap[feature], left_on=key, right_index=True, validate='many_to_one')\n",
    "    \n",
    "    return d\n",
    "\n",
    "def add_columns_to_work_data(df, keys_features)-> pd.DataFrame:\n",
    "    \n",
    "    d = df.copy()\n",
    "    \n",
    "    for k_f in keys_features:\n",
    "        d = add_column_to_work_data(d, key=k_f['key'], feature=k_f['feature'], amap=k_f['map'])\n",
    "    \n",
    "    d.reset_index(inplace=True, drop=True)\n",
    "    return d\n",
    "\n",
    "def report_data(a_start, df, add_columns: List = None, use_gfrags: bool = True):\n",
    "    \n",
    "    # the first input variable sets the limit ot the report\n",
    "    # that means we are interested about the summary of this data\n",
    "    # or something contained withing it. This variable is used\n",
    "    # the reporting process\n",
    "    top_label = [list(a_start.keys())[0], list(a_start.values())[0]]\n",
    "    \n",
    "    # slice the survey data by the provided date\n",
    "    # do this straight away is save memory\n",
    "    w_d = slice_data_by_date(df.copy(), start=a_start['start_date'], end=a_start['end_date'])\n",
    "    \n",
    "    # check for and add to the survey data the group\n",
    "    # and label for this report if it is missing\n",
    "    w_d = check_for_top_label(top_label[0], df=w_d, a_map=beaches)\n",
    "    \n",
    "    # use gfrags or add columns to the survey data\n",
    "    # by default the feature_type and code groupname is\n",
    "    # is added to the survey data.\n",
    "    if use_gfrags:\n",
    "        w_d = use_parent_groups_or_gfrags(w_d, label=top_label[0])\n",
    "    \n",
    "    if add_columns is not None:\n",
    "        w_d = add_columns_to_work_data(w_d, add_columns)\n",
    "    \n",
    "    # this is the data for report\n",
    "    w_df = w_d[w_d[top_label[0]].isin([top_label[1]])].copy()\n",
    "    \n",
    "    return top_label, a_start['language'], w_df, w_d\n",
    "\n",
    "\n",
    "geo_h = conf_.geo_h\n",
    "\n",
    "def categorize_work_data(df, labels, columns_of_interest: List[str] = geo_h, sample_id: str = 'loc_date'):\n",
    "    # print(labels)\n",
    "    \n",
    "    data = df[df[labels[0]] == labels[1]].copy()\n",
    "    \n",
    "    summaries = columns_of_interest\n",
    "    \n",
    "    if labels[0] == columns_of_interest[-1]:\n",
    "        summaries = columns_of_interest[:-2]\n",
    "    if labels[0] == columns_of_interest[-2]:\n",
    "        summaries = [*columns_of_interest[:-2], columns_of_interest[-1]]\n",
    "    \n",
    "    new_columns = list(set([sample_id, *summaries]))\n",
    "    d = data[new_columns].copy()\n",
    "    \n",
    "    res = {}\n",
    "    for an_attribute in new_columns:\n",
    "        datt = d[an_attribute].unique()\n",
    "        res.update({an_attribute: datt})\n",
    "    \n",
    "    res['samples'] = res.pop('loc_date')\n",
    "    \n",
    "    return {labels[1]:res}\n",
    "\n",
    "def a_summary_of_one_vector(df, unit_columns, unit_agg, describe='pcs_m'):\n",
    "    \n",
    "    sample_totals = aggregate_dataframe(df, unit_columns, unit_agg)\n",
    "    sample_summary = sample_totals[describe].describe()\n",
    "    sample_summary[\"total\"] = sample_totals.quantity.sum()\n",
    "    sample_summary = pd.DataFrame(sample_summary)\n",
    "    sample_summary[describe] = sample_summary[describe].astype(object)\n",
    "    sample_summary.loc['count', describe] = int(sample_summary.loc['count', describe])\n",
    "    sample_summary.loc['total', describe] = int(sample_summary.loc['total',describe])\n",
    "    \n",
    "    return sample_summary"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca0862-a1e5-4fd3-ae17-65ac2fd7cd0a",
   "metadata": {},
   "source": [
    "# a report class\n",
    "\n",
    "## basic requirements\n",
    "\n",
    "1. define the limits of the request\n",
    "   * temporal\n",
    "   * geographic (includes features and parent boundaries)\n",
    "   * object types\n",
    "   * level of aggregation\n",
    "\n",
    "2. define what codes are being used\n",
    "\n",
    "The default setting is to combine all the fragmented plastics into one group (all sizes) and the same for fragmented expanded polystyrene and plastic bottle tops. This results in three codes that represent objects that are very similar. This topic has been addressed many times. These groups register not-trivial quantities at most surveys. However, the differentiation of these objects into their respective subgroups ie. plastic caps for drinnking v/s plastic caps for household cleaners is not considered a priority by all groups that have collected data in the past.\n",
    "\n",
    "* Gfrags\n",
    "* Gfoams\n",
    "* Gcaps\n",
    "\n",
    "3. define the reporting language\n",
    "\n",
    "The reporting language can be either French, German or English. We would like italian but we have no resource for that service in Ticino.\n",
    "\n",
    "__Note:__ The reporting language is only applied at the moment of display. The column names, feature labels and other underlying identifying criteria for the data remain unchanged. The column name definitions and translations are in the _random variables_ section.\n",
    "   \n",
    "From the testing_data_models note book it is shown that given the following set of variables summary reports and test statistics can be generated for any combination of data:\n",
    "\n",
    "   * `df (pd.DataFrame)`: The input DataFrame containing data for analysis.\n",
    "   * `cumulative_columns (List, optional)`: List of columns to be considered for cumulative values.\n",
    "   * `boundary_labels (List, optional)`: List of labels for boundary summaries.\n",
    "   * `object_labels (List, optional)`: List of labels for individual objects.\n",
    "   * `object_columns (List, optional)`: List of columns identifying objects.\n",
    "   * `unit_agg (dict, optional)`: Aggregation methods for unit summaries.\n",
    "   * `unit_columns (List, optional)`: List of columns for unit summaries.\n",
    "   * `agg_groups (dict, optional)`: Aggregation methods for boundary summaries.\n",
    "\n",
    "\n",
    "### Work data\n",
    "\n",
    "A report can be defined by providing the temporal and geographic bounds of interest. Below is the current method. \n",
    "\n",
    "```python\n",
    "\n",
    "# request\n",
    "canton = 'Bern'\n",
    "start_date = '2019-01-01'\n",
    "end_date = '2022-01-01'\n",
    "language = 'fr'\n",
    "\n",
    "# starting data, can be MySQL or NoSQL calls or API calls\n",
    "# the three methods accept a callable as long as the output\n",
    "# is pd.DataFrame \n",
    "c_l = language_maps()\n",
    "surveys = collect_survey_data_for_report()\n",
    "codes, beaches, land_cover, land_use, streets, river_intersect_lakes = collect_env_data_for_report()\n",
    "\n",
    "# temporal and geographic boundaries\n",
    "# user defined input in dictionary format\n",
    "boundaries = dict(canton='Bern', language='fr', start_date='2019-01-01', end_date='2022-01-01')\n",
    "\n",
    "# columns to be added to the survey data that are not stored with the survey data. Note that codes\n",
    "# and beaches are part of the initial data. These operations can be performed either on the\n",
    "# server side or the client side. The codes and beach data are small and can be stored on a \n",
    "# remote device. The same for data from any single region. If using a browser IndexedDB is very\n",
    "# efficient for this type of operation. The index of <codes> contains the values of surveys.code\n",
    "# and the index of <beaches> contains the values of surveys.slug\n",
    "add_columns = [\n",
    "    {'key':'code', 'feature':'groupname', 'map':codes},\n",
    "    {'key':'slug', 'feature':'feature_type', 'map':beaches}\n",
    "]\n",
    "\n",
    "# the level and label of the report\n",
    "# the language for display\n",
    "# the data for the report and all other\n",
    "# available data from the data range\n",
    "top_label, language, w_df, w_di = report_data(boundaries, surveys, add_columns=add_columns)\n",
    "\n",
    "# define the language map\n",
    "l_map = c_l[language].copy()\n",
    "w_df.head()\n",
    "```\n",
    "\n",
    "Which produces the following untranslated output."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "92be0747-b59f-484b-8b60-f5e9ceaeafa5",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "ename": "KeyError",
     "evalue": "\"None of ['code'] are in the columns\"",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 6\u001b[0m\n\u001b[1;32m      4\u001b[0m c_l \u001b[38;5;241m=\u001b[39m language_maps()\n\u001b[1;32m      5\u001b[0m surveys \u001b[38;5;241m=\u001b[39m collect_survey_data_for_report()\n\u001b[0;32m----> 6\u001b[0m codes, beaches, land_cover, land_use, streets, river_intersect_lakes \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_env_data_for_report\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;66;03m# temporal and geographic boundaries\u001b[39;00m\n\u001b[1;32m      9\u001b[0m \u001b[38;5;66;03m# user defined input\u001b[39;00m\n\u001b[1;32m     10\u001b[0m boundaries \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(canton\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mValais\u001b[39m\u001b[38;5;124m'\u001b[39m, language\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfr\u001b[39m\u001b[38;5;124m'\u001b[39m, start_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2019-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m, end_date\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m2022-01-01\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
      "Cell \u001b[0;32mIn[3], line 15\u001b[0m, in \u001b[0;36mcollect_env_data_for_report\u001b[0;34m(a_func, **kwargs)\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a_func(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)\n\u001b[1;32m     14\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 15\u001b[0m     codes \u001b[38;5;241m=\u001b[39m \u001b[43mpd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mread_csv\u001b[49m\u001b[43m(\u001b[49m\u001b[43mconf_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcode_data\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mset_index\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mcode\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     beaches \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(conf_\u001b[38;5;241m.\u001b[39mbeach_data)\u001b[38;5;241m.\u001b[39mset_index(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mslug\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     17\u001b[0m     land_cover \u001b[38;5;241m=\u001b[39m pd\u001b[38;5;241m.\u001b[39mread_csv(conf_\u001b[38;5;241m.\u001b[39mland_cover_data)\n",
      "File \u001b[0;32m~/anaconda3/envs/plastock/lib/python3.9/site-packages/pandas/core/frame.py:5867\u001b[0m, in \u001b[0;36mDataFrame.set_index\u001b[0;34m(self, keys, drop, append, inplace, verify_integrity)\u001b[0m\n\u001b[1;32m   5864\u001b[0m                 missing\u001b[38;5;241m.\u001b[39mappend(col)\n\u001b[1;32m   5866\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m missing:\n\u001b[0;32m-> 5867\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNone of \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mmissing\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m are in the columns\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   5869\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m inplace:\n\u001b[1;32m   5870\u001b[0m     frame \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\n",
      "\u001b[0;31mKeyError\u001b[0m: \"None of ['code'] are in the columns\""
     ]
    }
   ],
   "source": [
    "# starting data, can be MySQL or NoSQL calls\n",
    "# the three methods accept Callables, as long\n",
    "# as the out put is pd.DataFrame\n",
    "c_l = language_maps()\n",
    "surveys = collect_survey_data_for_report()\n",
    "codes, beaches, land_cover, land_use, streets, river_intersect_lakes = collect_env_data_for_report()\n",
    "\n",
    "# temporal and geographic boundaries\n",
    "# user defined input\n",
    "boundaries = dict(canton='Valais', language='fr', start_date='2019-01-01', end_date='2022-01-01')\n",
    "\n",
    "# columns to be added to the survey data\n",
    "# not stored with the survey data. Note that codes\n",
    "# and beaches are part of the initial data. The index of\n",
    "# codes contains the values of surveys.code and the index\n",
    "# of beaches contains the values of surves.slug\n",
    "add_columns = [\n",
    "    {'key':'code', 'feature':'groupname', 'map':codes},\n",
    "    {'key':'slug', 'feature':'feature_type', 'map':beaches}\n",
    "]\n",
    "\n",
    "# the level and label of the report\n",
    "# the language for display\n",
    "# the data for the report and all other\n",
    "# from the data range\n",
    "top_label, language, w_df, w_di = report_data(boundaries, surveys, add_columns=add_columns)\n",
    "\n",
    "# define the language map\n",
    "l_map = c_l[language].copy()\n",
    "w_df.head().style.set_table_styles(conf_.table_css_styles)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d45bcb7-667a-49f2-afa0-8e486c0ddf74",
   "metadata": {},
   "source": [
    "### Reporting categories\n",
    "\n",
    "The first variable of the input is used to define the hierarchy of the report. For administrative purposes a vertical approach that reflects areas of responsibility is important. For estimating values the geographic/topographic attributes are more important.\n",
    "\n",
    "The survey data is labeled for these purposes. The columns `parent_boundary`, `feature_type` and `feature_name` are the topographic features. \n",
    "\n",
    "1. `parent_boundary`: the name of the: river basin, catchment area, park, name of geograhphic region or other zone defined by swiss geo admin.\n",
    "2. `feature_type`: lake, river or park\n",
    "3. `feature_name`: the name of the lake, river or park\n",
    "\n",
    "The `geo_h` array sets the order for reporting. Reports for cantons can contain subreports for all the values in the array, by default the cantonal results will reference the IQAASL report for threshold or prior results. Reports for cities will contain only geographic categories with reference to cantonal results.\n",
    "\n",
    "```python\n",
    "\n",
    "\n",
    "geo_h = ['parent_boundary', 'feature_type',  'feature_name','canton', 'city']\n",
    "\n",
    "\n",
    "def categorize_work_data(df, labels, columns_of_interest: List[str] = geo_h, sample_id: str = 'loc_date'):\n",
    "    # print(labels)\n",
    "    \n",
    "    data = df[df[labels[0]] == labels[1]].copy()\n",
    "    \n",
    "    summaries = columns_of_interest\n",
    "    \n",
    "    # if city is selected the available boundaries\n",
    "    # are geographic. A city is in only one canton\n",
    "    # if canton is selected then city becomes a category\n",
    "    # for which a report can be produced    \n",
    "    if labels[0] == columns_of_interest[-1]:\n",
    "        summaries = columns_of_interest[:-2]\n",
    "    if labels[0] == columns_of_interest[-2]:\n",
    "        summaries = [*columns_of_interest[:-2], columns_of_interest[-1]]\n",
    "    \n",
    "    new_columns = list(set([sample_id, *summaries]))\n",
    "    d = data[new_columns].copy()\n",
    "    \n",
    "    res = {}\n",
    "    for an_attribute in new_columns:\n",
    "        datt = d[an_attribute].unique()\n",
    "        res.update({an_attribute: datt})\n",
    "    \n",
    "    res['samples'] = res.pop('loc_date')\n",
    "    \n",
    "    return {labels[1]:res}\n",
    "\n",
    "# this categorizes the survey data into search terms\n",
    "# the available data or reporting categories are retrieved\n",
    "# by getting the length of the array for each category\n",
    "# if the category is not present then the data is not available\n",
    "parent_categories = categorize_work_data(w_df, top_label)\n",
    "p_vals = parent_categories[boundaries[top_label[0]]]\n",
    "\n",
    "# the type and number of reports available\n",
    "reporting_categories = {k:len(v) for k, v in p_vals.items()}\n",
    "reporting_categories\n",
    "```\n",
    "\n",
    "Which gives the following result:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0154049a-d836-4bbc-8430-abff11313a88",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# this categorizes the survey data into search terms\n",
    "# the available data or reporting categories are retrieved\n",
    "# by getting the length of the array for each category\n",
    "# if the category is not present then the data is not available\n",
    "parent_categories = categorize_work_data(w_df, top_label)\n",
    "p_vals = parent_categories[boundaries[top_label[0]]]\n",
    "\n",
    "# the type and number of reports available\n",
    "reporting_categories = {k:len(v) for k, v in p_vals.items()}\n",
    "reporting_categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63536da8-9ec0-433c-be16-eb7d4ca35dd0",
   "metadata": {},
   "outputs": [],
   "source": [
    "river_features = categorize_work_data(w_df[w_df.feature_type == p_vals['feature_type'][0]], top_label)\n",
    "lake_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b086235b-df25-4179-899d-40ca4e2e4e80",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b829d8ef-11bc-48e7-bd76-b6b7463cda40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beaches.loc[['lavey-les-bains-2','lavey-les-bains'] , 'canton'] = 'Vaud'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bff61eca-2438-4bad-a716-5098de34e512",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdd410a-d203-4f6c-a10e-5ee2519053db",
   "metadata": {},
   "outputs": [],
   "source": [
    "# beaches.to_csv(\"data/end_process/codes.csv\", index=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7793d405-4123-4f75-a929-02f6738585df",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "# the big picture\n",
    "\n",
    "\n",
    "def identify_parent_child(label, geo_labels):\n",
    "    child_ = geo_labels[label] + 1\n",
    "    parent_ = geo_labels[label] - 1\n",
    "    if child_ > max(geo_labels.values()):\n",
    "        child_ = max(geo_labels.values())\n",
    "    if parent_ < 0:\n",
    "        parent_ = 0\n",
    "    return child_, parent_\n",
    "        \n",
    "    \n",
    "\n",
    "# print(geo_h[parent], parent, geo_h[child], child)\n",
    "\n",
    "\n",
    "def display_summary_of_work_data(a_summary: dict = None, a_map: pd.DataFrame = None, not_translated: List[str] = ['samples'], **kwargs):\n",
    "    \n",
    "    d = a_summary.copy()\n",
    "    # the first item in the array must be equal to sample\n",
    "    # this eliminates the integer values from the translation\n",
    "    # we can select on data type also\n",
    "    anum = len(d[not_translated[0]])\n",
    "    d.update({not_translated[0]: anum})\n",
    "    \n",
    "    t = list(d.keys())\n",
    "    \n",
    "    translate = [x for x in t if x not in not_translated]\n",
    "    \n",
    "    for label in translate:\n",
    "        d.update({label: len(d[label])})\n",
    "            \n",
    "    return d\n",
    "\n",
    "def translate_array(X, func: Callable = translate_word, **kwargs):\n",
    "    return [func(x, **kwargs) for x in X]\n",
    "\n",
    "geo_labels = dict(zip(geo_h, np.arange(len(geo_h))))\n",
    "\n",
    "child, parent = identify_parent_child(top_label[0], geo_labels)\n",
    "\n",
    "# parent_categories = categorize_work_data(w_df, top_label)\n",
    "# canton_class = list(summary_canton.keys())\n",
    "# p_categories = parent_categories[top_label[1]]\n",
    "\n",
    "# summary_canton = display_summary_of_work_data(parent_categories[top_label[1]], a_map=c_l, lan='fr')\n",
    "\n",
    "# summary_canton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "04e82e60-ce1d-4d57-a41f-0df07f413bd7",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'codes' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[5], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mcodes\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'codes' is not defined"
     ]
    }
   ],
   "source": [
    "codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d2053c5-593b-4072-8f7b-015e53100c1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "parent_categories = categorize_work_data(w_df, top_label)\n",
    "parent_categories[boundaries['canton']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91e7a843-1980-44d3-9ded-0970045a298e",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "p_categories = parent_categories[top_label[1]]\n",
    "p_categories[canton_class[0]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d0327df-e073-4fa9-a6e8-5484ff9b0cf0",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_categories[canton_class[1]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8d25714-20f4-40f3-980e-b12d1a22ca1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_categories[canton_class[2]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c88ea7a-0cae-450e-9954-2d32f4ddf941",
   "metadata": {},
   "outputs": [],
   "source": [
    "p_categories[canton_class[3]]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a429e47-eacd-4c4f-8af3-256daee63261",
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "children = w_df[geo_h[child]].unique()\n",
    "parents = w_df[geo_h[parent]].unique()\n",
    "chil_label = list(tuple(zip([*[geo_h[child]]*len(children)], children)))\n",
    "p_labels = list(tuple(zip([*[geo_h[parent]]*len(children)], parents)))\n",
    "\n",
    "print(child)\n",
    "print(geo_h[child])\n",
    "print(children)\n",
    "print(parent)\n",
    "print(geo_h[parent])\n",
    "print(parents)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5c227ff5-e09d-4068-810d-5b35b1969519",
   "metadata": {},
   "outputs": [],
   "source": [
    "child_sum = {}\n",
    "for alabel in p_labels:\n",
    "    \n",
    "    child_sum.update(categorize_work_data(w_df, alabel))\n",
    "\n",
    "display_sums = {x:display_summary_of_work_data(child_sum[x], a_map=c_l, lan='fr') for x in child_sum.keys()}\n",
    "\n",
    "pd.DataFrame(display_sums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c2e6be8b-06bd-40b0-ac97-eff2b7350bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_of_child_data = {}\n",
    "pair_label = list(tuple(zip([*[geo_h[parent]]*len(parents)], parents)))\n",
    "\n",
    "# d = w_df.columns\n",
    "# print(d)\n",
    "for alabel in chil_label:    \n",
    "    summary_of_child_data.update(categorize_work_data(w_df, alabel))\n",
    "\n",
    "    display_sums = {x:display_summary_of_work_data(summary_of_child_data[x], a_map=c_l, lan='fr') for x in summary_of_child_data.keys()}\n",
    "\n",
    "pd.DataFrame(display_sums).T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f45ffedd-0047-4d0f-8d47-c662c923d965",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_columns = [\"loc_date\", \"slug\", top_label[0]]\n",
    "# a_summary_of_one_vector(w_df[w_df.parent_boundary.isin(summary_of_parent_data['p']['parent_boundary'])].copy(), unit_columns, conf_r.unit_agg, describe='pcs_m')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61c05f-baa2-4e9f-91ab-4d6840198579",
   "metadata": {},
   "source": [
    "## Aggregate a set of data by sample (location and date)\n",
    "\n",
    "Use the loc_date column in the survey data. Use the IQAASL period and four river baisns test against the federal report.\n",
    "\n",
    "### Before aggregating does the number of locations, cities, samples and quantity match the federal report?\n",
    "\n",
    "__The feature types include lakes and rivers, alpes were condsidered separately__\n",
    "\n",
    "From https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/lakes_rivers.html#\n",
    "\n",
    "1. cities = yes\n",
    "2. samples = yes\n",
    "3. locations = yes\n",
    "4. quantity = No it is short 50 pieces\n",
    "5. start and end date = yes"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "57569da8-0cbe-4acc-bfbd-95d660052c5e",
   "metadata": {},
   "source": [
    "### Number of lakes, rivers, parcs, cities and cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc084a-579d-4da4-a147-11f4f7c21ac1",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_of_work_data = categorize_work_data(code_result_df, code_result_df.canton.unique())\n",
    "summary_of_work_data = summary_of_work_data[['samples', 'cities', 'lakes', 'rivers', 'parks', 'quantity']]\n",
    "translated_and_style_for_display(summary_of_work_data, l_mapi, lang, gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69cc1-b826-47a6-a2fc-abe599e4481c",
   "metadata": {},
   "source": [
    "### aggregate to sample\n",
    "\n",
    "The assessments are made on a per sample basis. That means that we can look at an individual object value at each sample. The sum of all the individual objects in a survey is the total for that survey. Dividing the totals by the length of the survey gives the assessment metric: _pieces of trash per meter_.\n",
    "\n",
    "1. Are the quantiles of the current data  = to the federal report? Yes\n",
    "2. Are the material totals = to the federal report? No,plastics if off by 50 pcs\n",
    "3. Are the fail rates of the most common objects = to the federal report? Yes\n",
    "4. Is the % of total of the most common objects = to the fedral report? yes\n",
    "5. Is the median pieces/meter of the most common objects = to the federal report? yes\n",
    "6. Is the quantity of the most common objects = to the federal report? yes\n",
    "\n",
    "#### The summary of survey totals\n",
    "\n",
    "fig 1.5 in IQAASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfec738d-1a5b-4e14-98a5-765cd0a285b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# the sample is the basic unit\n",
    "# loc_date is the unique identifier for each sample\n",
    "unit_columns = [\"loc_date\", \"slug\", \"parent_boundary\"]\n",
    "\n",
    "# the quantiles of the sample-total pcs/m  \n",
    "vector_summary = a_summary_of_one_vector(code_result_df.copy(), unit_columns, unit_agg, describe='pcs_m')\n",
    "\n",
    "translated_and_style_for_display(vector_summary,l_mapi, lang, gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5c6dd92c-60b1-47e9-a99f-036f0ab37272",
   "metadata": {},
   "source": [
    "#### Material totals and proportions\n",
    "\n",
    "fig 1.5 iqaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92698c51-4640-41d1-be48-466c177d65f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# add the material label to each code\n",
    "merged_result = merge_dataframes_on_column_and_index(code_result_df.copy(), codes[\"material\"], 'code', how='inner', validate=True)\n",
    "\n",
    "# sum the materials for the data frame\n",
    "materials = aggregate_dataframe(merged_result.copy(), [\"material\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "# add 5 of total for display\n",
    "materials[\"%\"] = materials.quantity/materials.quantity.sum()\n",
    "\n",
    "translated_and_style_for_display(materials.set_index('material', drop=True),l_mapi, lang, gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "086c5091-d3cd-45b3-b18f-43bffca65c11",
   "metadata": {},
   "source": [
    "#### Quantity, median pcs/m, fail rate, and % of total\n",
    "\n",
    "Sumary results for all the codes in the parent_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f996669d-61fb-480f-849b-a14acda37a03",
   "metadata": {},
   "outputs": [],
   "source": [
    "# sum the cumulative quantity for each code and calculate the median pcs/meter\n",
    "code_totals = aggregate_dataframe(code_result_df.copy(), [\"code\"], {\"quantity\":\"sum\", \"pcs_m\":\"median\"})\n",
    "\n",
    "# collect \n",
    "abundant = get_top_x_records_with_max_quantity(code_totals.copy(), \"quantity\", \"code\", len(code_totals))\n",
    "\n",
    "# identify the objects that were found in at least 50% of the samples\n",
    "# calculate the quantity per sample for each code and sample\n",
    "occurrences = aggregate_dataframe(code_result_df, [\"loc_date\", \"code\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "# count the number of times that an object was counted > 0\n",
    "# and divide it by the total number of samples \n",
    "event_counts  = count_objects_with_positive_quantity(occurrences)\n",
    "\n",
    "# calculate the rate of occurence per unit of measure\n",
    "rates = calculate_rate_per_unit(code_result_df, code_result_df.code.unique())\n",
    "\n",
    "# add the unit rates and fail rates\n",
    "abundance = merge_dataframes_on_column_and_index(abundant, rates[\"pcs_m\"], left_column=\"code\", validate=\"one_to_one\")\n",
    "abundance[\"fail rate\"] = abundance.code.apply(lambda x: event_counts.loc[x])\n",
    "\n",
    "# this is the complete inventory with summary\n",
    "# statistics for each objecabundance.sort_values(by=\"quantity\", inplace=True, ascending=False)\n",
    "abundance.reset_index(inplace=True, drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a7b3443-343b-4ea6-864f-1f33ab081b5d",
   "metadata": {},
   "outputs": [],
   "source": [
    "# codes = pd.read_csv(code_data)\n",
    "# codes.rename(columns={\"en\":\"code\"}, inplace=True)\n",
    "# codes.set_index(\"code\", drop=True, inplace=True)\n",
    "# codes.drop('code', inplace=True)\n",
    "# codes.to_csv('data/end_process/codes.csv', index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2dd31a37-c5a7-40ec-9a8e-413a661530be",
   "metadata": {},
   "source": [
    "### The most common objects\n",
    "\n",
    "fig 1.6 iqaasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "136b5ef1-7a8b-4bf0-8a64-e68d46e9b30c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# arguments to slice the data by column\n",
    "column_one = {\n",
    "    'column': 'quantity',\n",
    "    'val': abundance.loc[10, 'quantity']\n",
    "}\n",
    "\n",
    "column_two = {\n",
    "    'column':'fail rate',\n",
    "    'val': 0.5\n",
    "}\n",
    "\n",
    "# use the inventory to find the most common objects\n",
    "the_most_common = display_tabular_data_by_column_values(abundance.copy(), column_one, column_two, 'code')\n",
    "\n",
    "translated_and_style_for_display(the_most_common.copy(),l_mapi, lang, gradient=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "060a06c9-fbc2-4764-b7d7-9074b43df7d8",
   "metadata": {},
   "source": [
    "### Results by groupname and feature boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a5147fd7-7ed0-4fd3-8197-4e9df1554080",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_columns = [\"loc_date\", \"groupname\"]\n",
    "unit_columns = [\"parent_boundary\", \"loc_date\", \"groupname\"]\n",
    "object_labels = code_result_df.groupname.unique()\n",
    "object_columns = [\"groupname\"]\n",
    "boundary_labels = code_result_df.parent_boundary.unique()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':object_labels,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(code_result_df.copy(), **args)\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b6b87fb-b4e3-49ec-8854-d6e3bb3baeb5",
   "metadata": {},
   "source": [
    "### Most common codes by feature boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a848a62e-48a7-4afe-8ef5-07b5407cda8c",
   "metadata": {},
   "outputs": [],
   "source": [
    "cumulative_columns = [\"loc_date\", \"code\"]\n",
    "unit_columns = [\"parent_boundary\", \"loc_date\", \"code\"]\n",
    "codes_of_interest = the_most_common.index\n",
    "object_columns = [\"code\"]\n",
    "boundary_labels = code_result_df.parent_boundary.unique()\n",
    "\n",
    "data = code_result_df[code_result_df.code.isin(codes_of_interest)].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(data.copy(), **args)\n",
    "\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a7b6f06d-a746-40f2-b947-571e173bba14",
   "metadata": {},
   "source": [
    "### Most common codes by canton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7070f656-04e6-4cf0-b7cd-f36da7d040c2",
   "metadata": {},
   "outputs": [],
   "source": [
    "unit_columns = [\"canton\", \"loc_date\", \"code\"]\n",
    "object_columns = [\"code\"]\n",
    "boundary_labels = code_result_df.canton.unique()\n",
    "\n",
    "data = code_result_df[code_result_df.code.isin(codes_of_interest)].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(data, **args)\n",
    "\n",
    "translated_and_style_for_display(tix.T,l_mapi, lang, gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "09390343-28af-4e42-9a46-f65eeff30efa",
   "metadata": {},
   "source": [
    "### Most common codes: canton-municipal\n",
    "\n",
    "#### Bern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8309b868-964c-4f28-a9fd-d8dbbfda16af",
   "metadata": {},
   "outputs": [],
   "source": [
    "canton = \"Bern\"\n",
    "\n",
    "with_cantons = code_result_df[code_result_df.canton == canton].copy()\n",
    "\n",
    "unit_columns = [\"city\", \"loc_date\", \"code\"]\n",
    "# the column that holds the labels of interest\n",
    "object_columns = [\"code\"]\n",
    "# the labels of interest for the boundary conditions\n",
    "boundary_labels = with_cantons.city.unique()\n",
    "\n",
    "ddata = with_cantons[(with_cantons.code.isin(codes_of_interest)) & (with_cantons.canton == \"Bern\")].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(ddata, **args)\n",
    "translated_and_style_for_display(tix.T,l_mapi, lang, gradient=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c2758c89-b920-46e9-b5b7-74bef904b377",
   "metadata": {},
   "source": [
    "#### Valais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eab462fb-d5cb-4914-86cf-50e59aa26a07",
   "metadata": {},
   "outputs": [],
   "source": [
    "canton = \"Valais\"\n",
    "\n",
    "with_cantons = code_result_df[code_result_df.canton == canton].copy()\n",
    "\n",
    "unit_columns = [\"city\", \"loc_date\", \"code\"]\n",
    "# the column that holds the labels of interest\n",
    "object_columns = [\"code\"]\n",
    "# the labels of interest for the boundary conditions\n",
    "boundary_labels = with_cantons.city.unique()\n",
    "\n",
    "ddata = with_cantons[(with_cantons.code.isin(codes_of_interest))].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(ddata, **args)\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "54fba949-faaf-45c5-a4b7-f0111a5c8872",
   "metadata": {},
   "outputs": [],
   "source": [
    "%watermark -a hammerdirt-analyst -co --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "067252ee-2db5-4287-b269-0aea3e4aadbc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# import re\n",
    "\n",
    "# def remove_special_characters_and_lowercase(input_string):\n",
    "#     # Use a regular expression to remove non-alphanumeric characters\n",
    "#     clean_string = re.sub(r'[^a-zA-Z0-9]', '', input_string)\n",
    "#     # Convert the cleaned string to lowercase\n",
    "#     lowercase_string = clean_string.lower()\n",
    "#     return lowercase_string\n",
    "\n",
    "# # Example usage:\n",
    "# input_string = \"Hello*.$, World! 123\"\n",
    "# result = remove_special_characters_and_lowercase(input_string)\n",
    "\n",
    "# def is_iso_date(date_string):\n",
    "#     # Define a regular expression pattern for ISO date (YYYY-MM-DD)\n",
    "#     iso_date_pattern = r'^\\d{4}-\\d{2}-\\d{2}$'\n",
    "\n",
    "#     if re.match(iso_date_pattern, date_string):\n",
    "#         return date_string\n",
    "#     else:\n",
    "#         return False\n",
    "\n",
    "\n",
    "\n",
    "# def validate_a_string_format(input_str, expected_format: str = r'^\\d{4}-\\d{2}-\\d{2}$', string_rep: str = 'YYYY-MM-DD'):\n",
    "#     # Define the expected format using a regular expression pattern\n",
    "    \n",
    "#     if not re.match(expected_format, input_str):\n",
    "#         raise ValueError(f\"Is not the right format. Should be: {string_rep}\")\n",
    "    \n",
    "#     return input_str\n",
    "    \n",
    "\n",
    "# def collect_survey_data_for_report(a_function: Callable = None, **kwargs)-> pd.DataFrame:\n",
    "#     # if a function is provided returns the called function\n",
    "#     # the function should return a data rame with the same column\n",
    "#     # names as the standard set of data otherwise the default method\n",
    "#     # and file locations are are used.\n",
    "#     if a_function:\n",
    "#         return a_function(**kwargs)\n",
    "#     else:\n",
    "#         surveys = combine_survey_files(conf_r.survey_files)\n",
    "#         return surveys\n",
    "        \n",
    "    \n",
    "\n",
    "# def collect_env_data_for_report(a_function: Callable = None, **kwargs)-> pd.DataFrame:\n",
    "#     # if a function is provided returns the called function\n",
    "#     # the function should return a data rame with the same column\n",
    "#     # names as the standard set of data otherwise the default method\n",
    "#     # and file locations are are used.\n",
    "#     if a_function:\n",
    "#         return a_function(**kwargs)\n",
    "#     if 'all' in kwargs.keys():\n",
    "#         codes = pd.read_csv(conf_r.code_data).set_index(\"code\")\n",
    "#         beaches = pd.read_csv(conf_r.beach_data).set_index(\"slug\")\n",
    "#         land_cover = pd.read_csv(conf_r.land_cover_data)\n",
    "#         land_use = pd.read_csv(conf_r.land_use_data)\n",
    "#         streets = pd.read_csv(conf_r.street_data)\n",
    "#         river_intersect_lakes = pd.read_csv(conf_r.intersection_attributes)\n",
    "    \n",
    "    \n",
    "    \n",
    "#     return codes, beaches, land_cover, land_use, streets, river_intersect_lakes\n",
    "            \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb4545f3-d006-4257-9391-c706fcf0db3b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# def apply_gradient_style(df, gradient_func: Callable = None, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Apply a custom gradient style to a DataFrame using a specified gradient function.\n",
    "\n",
    "#     Args:\n",
    "#         df (pd.DataFrame): The input DataFrame.\n",
    "#         gradient_func (callable): A function that takes a value and returns CSS styles for styling.\n",
    "\n",
    "#     Returns:\n",
    "#         pd.io.formats.style.Styler: A styled representation of the DataFrame.\n",
    "\n",
    "#     Example:\n",
    "#         def gradient(val):\n",
    "#             cmap = 'coolwarm'\n",
    "#             vmin = df.values.min().min()\n",
    "#             vmax = df.values.max().max()\n",
    "#             norm = plt.Normalize(vmin, vmax)\n",
    "#             colormap = plt.cm.get_cmap(cmap)\n",
    "#             color = colormap(norm(val))\n",
    "#             hex_color = \"#{:02x}{:02x}{:02x}\".format(int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
    "#             return f'background-color: {hex_color}'\n",
    "\n",
    "#         styled_df = apply_gradient_style(df, gradient)\n",
    "#     \"\"\"\n",
    "#     # Use the .style.applymap method to apply the gradient function to each cell in the DataFrame\n",
    "#     styled_df = df.style.applymap(gradient_func, **kwargs)\n",
    "\n",
    "#     return styled_df\n",
    "    \n",
    "# def apply_table_style_format(df, style: List = None, format_kwargs: Dict = None):\n",
    "    \n",
    "       \n",
    "#     return df.style.set_table_styles(style).format(**format_kwargs)\n",
    "\n",
    "# a decorator for data frames\n",
    "# def apply_style_to_dataframe(mymethod, **kwargs):\n",
    "#     \"\"\"\n",
    "#     Apply styling to a DataFrame using a custom method or gradient function.\n",
    "    \n",
    "#     This decorator function allows you to apply styling to a DataFrame returned by a function.\n",
    "#     It takes a custom method and a gradient function as input and applies these functions to\n",
    "#     the DataFrame to style it based on specified criteria.\n",
    "    \n",
    "#     Args:\n",
    "#     mymethod (callable): A custom method that applies styling to the DataFrame.\n",
    "#     gradient_func (callable): A function that defines a color gradient for styling.\n",
    "#     **kwargs: Additional keyword arguments to pass to the custom method.\n",
    "    \n",
    "#     Returns:\n",
    "#     callable: A decorator function that can be used to style a DataFrame returned by a function.\n",
    "    \n",
    "#     Example:\n",
    "#     def my_custom_styling(df, gradient_func, **kwargs):\n",
    "#         # Apply styling to the DataFrame using the gradient function and optional kwargs\n",
    "#         styled_df = df.style.applymap(gradient_func, **kwargs)\n",
    "#         return styled_df\n",
    "    \n",
    "#     @apply_style_to_dataframe(my_custom_styling, my_gradient_function, colormap='coolwarm')\n",
    "#     def create_dataframe():\n",
    "#         # Create and return a DataFrame\n",
    "#         ...\n",
    "    \n",
    "#     styled_result = create_dataframe()\n",
    "#     \"\"\"\n",
    "#     def decorator(function):\n",
    "    \n",
    "#         def wrapper(*args):\n",
    "#             # Call the original function to get the DataFrame\n",
    "#             df = function(*args)\n",
    "                                 \n",
    "#             return mymethod(df,**kwargs)\n",
    "        \n",
    "#         return wrapper\n",
    "#     return decorator\n",
    "\n",
    "\n",
    "# @apply_style_to_dataframe(apply_gradient_style, gradient_func=color_gradient, min=0, max=1, cmap=newcmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
