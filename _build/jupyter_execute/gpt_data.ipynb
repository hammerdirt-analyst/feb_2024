{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4e39e538-f20c-4fa0-954e-b17552001446",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import pandas as pd\n",
    "import setvariables as conf_\n",
    "import reportclass as r_class\n",
    "from typing import Type, Optional, Callable\n",
    "from typing import List, Dict, Union, Tuple\n",
    "from sklearn.preprocessing import MinMaxScaler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2a89bedc-1228-42ee-9d8d-75a50c9091bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b1c67cc-23ee-42e0-9220-215f60569d6a",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "1bb27b90-2bbe-4fc9-bb47-ed244e4bdba0",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "\n",
    "def match_topo_attributes_to_surveys(topo_data: pd.DataFrame, survey_data: pd.DataFrame)-> Tuple[pd.DataFrame,List]:\n",
    "    \"\"\"\n",
    "    Match topographic attributes to survey data for specific locations.\n",
    "\n",
    "    This function takes topographic attribute data and survey data and matches them based on the unique locations (slugs).\n",
    "    \n",
    "    Parameters:\n",
    "        topo_data (pd.DataFrame): A DataFrame containing topographic attribute data.\n",
    "        survey_data (pd.DataFrame): A DataFrame containing survey data.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, List]: A tuple containing two elements:\n",
    "            - A DataFrame containing topographic attribute data for locations found in both datasets.\n",
    "            - A list of locations (slugs) from the survey data for which there is no matching topographic data.\n",
    "\n",
    "    \"\"\"\n",
    "\n",
    "    locations = survey_data.slug.unique()\n",
    "    available = topo_data.index\n",
    "    # identify the locations that have no topo data\n",
    "    no_data = [x for x in locations if x not in available]\n",
    "\n",
    "    # take the available data and names of locations with no data\n",
    "    locations_with_data = [x for x in locations if x in available]\n",
    "    \n",
    "    return topo_data.loc[locations_with_data], no_data\n",
    "\n",
    "def merge_topodata_and_surveydata(topo, surveys, columns: List[str] = conf_.work_columns)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge survey data with topographic data using location information.\n",
    "\n",
    "    This function merges survey data with topographic data using the 'slug' column in the survey data\n",
    "    and the index of the topographic data. The merged DataFrame will contain the specified columns from the survey data.\n",
    "\n",
    "    Parameters:\n",
    "        topo (pd.DataFrame): A DataFrame containing topographic data with the location index.\n",
    "        surveys (pd.DataFrame): A DataFrame containing survey data with a 'slug' column for location matching.\n",
    "        columns (List[str]): A list of column names to include in the merged DataFrame (default is defined in conf.work_columns).\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A merged DataFrame containing the specified survey data columns and topographic data.\n",
    "    \"\"\"\n",
    "    # merges surveys to topo using the slug column in surveys\n",
    "    # and the index in topo\n",
    "    return surveys[columns].merge(topo, left_on='slug', right_index=True)\n",
    "\n",
    "def scale_a_column(df: pd.DataFrame, column_to_scale: str, column_name: str = 'length'):    \n",
    "\n",
    "    # Calculate the minimum and maximum values in the column\n",
    "    min_value = df[column_to_scale].min()\n",
    "    max_value = df[column_to_scale].max()\n",
    "\n",
    "    # Perform min-max scaling on a temp column\n",
    "    df['scalex'] = (df[column_to_scale] - min_value) / (max_value - min_value)\n",
    "    # reassign the value\n",
    "    df[column_name] = df['scalex']\n",
    "    # drop the temp\n",
    "    df.drop('scalex', axis=1, inplace=True)\n",
    "    return df\n",
    "\n",
    "def group_topographic_attributes(df: pd.DataFrame, list_of_labels: List = None, locations: List = None, coi: str = 'scale')-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Group and aggregate topographic attributes in a DataFrame.\n",
    "\n",
    "    This function groups and aggregates topographic attributes in the provided DataFrame. You can specify a list of labels\n",
    "    to group attributes, filter locations, and choose the column of interest for aggregation.\n",
    "\n",
    "    Parameters:\n",
    "        df (pd.DataFrame): A DataFrame containing topographic attributes.\n",
    "        list_of_labels (List, optional): A list of dictionaries with keys as new attribute names and values as properties to group.\n",
    "        locations (List, optional): A list of locations to filter the data (default is None, no location filtering).\n",
    "        coi (str, optional): The column of interest for aggregation (default is 'scale').\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with aggregated topographic attributes based on the specified grouping and filtering.\n",
    "   \"\"\"\n",
    "    \n",
    "    if locations is not None:\n",
    "        df = df.loc[df.slug.isin(locations)]    \n",
    "    # list of labels is a list of dictionaries\n",
    "    if list_of_labels is not None:\n",
    "        for new_labels in list_of_labels:\n",
    "            # the attributes, the dictionary values are \n",
    "            # properties being grouped\n",
    "            attributes = list(new_labels.values())\n",
    "            # the dictionary key is the new name of\n",
    "            # the attributes in the list\n",
    "            new_val = list(new_labels.keys())\n",
    "            df.loc[df['attribute'].isin(attributes[0]), 'attribute'] = new_val[0]\n",
    "    # sum the occurrences of the same attribute\n",
    "    r = df.groupby(['slug','attribute'], as_index=False)[coi].sum()\n",
    "\n",
    "    # pivot and set the index to the locations\n",
    "    # have the attributes\n",
    "    r = r.pivot(columns='attribute', index='slug')            \n",
    "            \n",
    "    return r.droplevel(0, axis=1).fillna(0)\n",
    "\n",
    "def statistic_of_critical_value(df, \n",
    "                                df_feature_columns, \n",
    "                                df_target_column, \n",
    "                                sample_id: str = 'loc_date',\n",
    "                                value_counts: bool = True,\n",
    "                                average: bool = False):\n",
    "    \"\"\"\n",
    "    Compute statistics of critical values for given data.\n",
    "\n",
    "    Parameters:\n",
    "    df (pd.DataFrame): The input DataFrame containing the data.\n",
    "    df_feature_columns (list): A list of columns to be used as feature columns.\n",
    "    df_target_column (list): A list of columns to be used as target columns.\n",
    "    sample_id (str, optional): The column representing sample identifiers (default is 'loc_date').\n",
    "    value_counts (bool, optional): If True, compute value counts as weights (default is True).\n",
    "    average (bool, optional): If True, compute the median for the feature columns (default is False).\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: A DataFrame containing computed statistics based on the specified options.\n",
    "    \"\"\"\n",
    "    d = pd.melt(df, value_vars=df_feature_columns, id_vars=[df_target_column, sample_id])\n",
    "    \n",
    "    if value_counts:\n",
    "        di = d.groupby('variable', as_index=False)['value'].value_counts()\n",
    "        di['weight'] = di['count']/d[sample_id].nunique()\n",
    "        di = di.pivot(columns='variable', index='value', values='weight')\n",
    "    if average:        \n",
    "        di = d.groupby(['variable', 'value'], as_index=False)[df_target_column].median()\n",
    "        di = di.pivot(columns='variable', index='value', values=df_target_column)\n",
    "                \n",
    "    return di\n",
    "\n",
    "class LandUse:\n",
    "    \"\"\"\n",
    "    A class for analyzing and transforming land use data.\n",
    "\n",
    "    This class provides methods to analyze land cover, land use, and transportation data.\n",
    "    It allows you to group attributes, scale data, and create ordinal rankings based on quantiles.\n",
    "\n",
    "    Parameters:\n",
    "        land_cover (pd.DataFrame): DataFrame containing land cover data.\n",
    "        land_use (pd.DataFrame): DataFrame containing land use data.\n",
    "        transportation (pd.DataFrame): DataFrame containing transportation data.\n",
    "        locations (List): List of locations for filtering data.\n",
    "        street_groups (List, optional): List of street groups (default is from configuration).\n",
    "        land_use_groups (List, optional): List of land use groups (default is from configuration).\n",
    "\n",
    "    Attributes:\n",
    "        quantiles (List): List of quantile values for ordinal ranking.\n",
    "        labels (List): List of labels corresponding to quantile groups.\n",
    "\n",
    "    Properties:\n",
    "        - land_cover: Grouped and aggregated land cover data.\n",
    "        - land_use: Grouped and aggregated land use data.\n",
    "        - trans: Grouped and aggregated transportation data.\n",
    "        - use_of_land: Combined data with the option to scale the columns between 0 and 1.\n",
    "        - ordinal_land_rank: Ordinal ranking based on quantiles for land use data.\n",
    "\n",
    "    Example:\n",
    "        land_use_data = LandUse(land_cover_data, land_use_data, transportation_data, locations)\n",
    "        land_cover = land_use_data.land_cover\n",
    "        land_use = land_use_data.land_use\n",
    "        trans_data = land_use_data.trans(new_labels=[{'new_attr': ['attr1', 'attr2']}])\n",
    "        land_rankings = land_use_data.ordinal_land_rank\n",
    "    \"\"\"\n",
    "    street_groups = conf_.street_groups\n",
    "    land_use_groups = conf_.lu_groups\n",
    "\n",
    "    def __init__(self, land_cover, land_use, transportation, locations, street_groups=street_groups, land_use_groups=land_use_groups):\n",
    "        self.lc = land_cover\n",
    "        self.lu = land_use\n",
    "        self.tr = transportation\n",
    "        self.locations = locations\n",
    "        self.lug = land_use_groups\n",
    "        self.stg = street_groups\n",
    "        self.quantiles = [0.0, 0.03, 0.25, 0.75, 0.97, 1.0]\n",
    "        self. labels = ['lowest', 'low', 'middle', 'high', 'highest']\n",
    "        \n",
    "    @property\n",
    "    def land_cover(self, list_of_labels=None):\n",
    "        return group_topographic_attributes(self.lc, locations=self.locations, list_of_labels=list_of_labels)\n",
    "\n",
    "    @property\n",
    "    def land_use(self, new_labels=None):\n",
    "        if new_labels is not None:\n",
    "            return group_topographic_attributes(self.lu, locations=self.locations, list_of_labels=new_labels)\n",
    "        else:\n",
    "            return group_topographic_attributes(self.lu, locations=self.locations, list_of_labels=self.slug)\n",
    "    \n",
    "    @property\n",
    "    def trans(self,new_labels=None):\n",
    "        if new_labels is not None:\n",
    "            return group_topographic_attributes(self.tr, locations=self.locations, list_of_labels=new_labels, coi='length')\n",
    "        else:\n",
    "            return group_topographic_attributes(self.tr, locations=self.locations, list_of_labels=self.stg, coi='length')\n",
    "\n",
    "    @property\n",
    "    def use_of_land(self):\n",
    "        a = self.land_cover.merge(self.land_use, left_index=True, right_index=True)\n",
    "        b = a.merge(self.trans, left_index=True, right_index=True)\n",
    "        \n",
    "        return b\n",
    "    \n",
    "    @property\n",
    "    def ordinal_land_rank(self):\n",
    "        ranked_df = self.use_of_land.copy()\n",
    "        columns_to_rank = ranked_df.columns\n",
    "        for column in columns_to_rank:\n",
    "            label = f'{column}_ordinal_rank'\n",
    "            ranked_df[label] = pd.cut(ranked_df[column], bins=self.quantiles, labels=self.labels, include_lowest=True)\n",
    "            ranked_df[column] = ranked_df[label]\n",
    "            ranked_df.drop(label, inplace=True, axis=1)\n",
    "        return ranked_df"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b47b189-f340-454c-805c-42827c6cdadf",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# Checking the assistant\n",
    "\n",
    "This page is a reference point for testing the accuracy of the GPT assigned to accompany readers of the federal report. The GPT should reproduce the calculations on this page at any time. This includes values not in the federal report. Stakeholders will need to apply these results to their proper geographic or administrative responsibilities. The hammerdirt GPT assists in this process.\n",
    "\n",
    "The product is a dataframe that is the combination of columns from the `ReportClass` and columns from the `LandUseClass`. The intention is to allow easy access to the magnitude of toprgaphical features within 1 500 m of the observed density for any object in the data.\n",
    "\n",
    "```{important}\n",
    "\n",
    "__April 17, 2023:__ The app that uses the hammerdirtGPT is in demo-form. We have abandoned the intial method of defining the prompt through the api. We are now developing a RAG application. A component of the context for the prompt is the results from users request. With this we combine the references from the federal report and any updated references that can be included.\n",
    "\n",
    "**Changes to class definitions:** Building a RAG application means that we have to consider both the user visualisation of the report and the consumption of that data for the AI model, data_frame or array for the former, .JSON for the latter. These considerations will have a transformative effect on all the code in this module.\n",
    "\n",
    "__PREVIOUS__\n",
    "\n",
    "November 20, 2023: There is a known issue we are working on now. Remind the assistant to follow intsructions. Specifically in the following cases:\n",
    "\n",
    "1. Always getting a value of zero for the median sample total\n",
    "   * The GPT has specific instructions on this\n",
    "2. Tells you the correct columns are not available\n",
    "   * The GPT has the column names and definitions from this page\n",
    "\n",
    "The data has a two column index, somtheing the GPT does not always recognize. An issue has been submitted [issue](https://github.com/hammerdirt-analyst/feb_2024/issues/1)\n",
    "```\n",
    "\n",
    "```{note}\n",
    "The assistants role is to provide mathematical and graphical representations of the data in response to the researchers request. This often involves aggregating values at different levels, combining attributes and the like.\n",
    "\n",
    "This page allows all users to verify that these complex transactions are happening correctly. The GPT may not use the same method to calculate the final result, but the results should be same.\n",
    "```\n",
    "## Default data of hammerdirt GPT:\n",
    "\n",
    "beta version = .01\n",
    "\n",
    "The default data for the GPT can be reproduced on the command line if the `hammerdirtgpt` package is installed:\n",
    "\n",
    "```python\n",
    "# Collecting required data to establish a report\n",
    "# This includes the language maps for all the common\n",
    "# abbreviations and columns or index labels.\n",
    "c_l = r_class.language_maps()\n",
    "\n",
    "# The survey data in units of pcs/m or pcs/m². The reports\n",
    "# are aggregated first to the sample_id. Which means that the operations\n",
    "# are the same wether using pcs/m or pcs/m².\n",
    "surveys = r_class.collect_survey_data_for_report()\n",
    "\n",
    "# The support or evnvironmental data. This includes plain text descriptions \n",
    "# of the Codes. Details for each survey location and topogrphical data\n",
    "# extracted from the buffer around each survey location.\n",
    "codes, beaches, land_cover, land_use, streets, river_intersect_lakes = r_class.collect_env_data_for_report()\n",
    "\n",
    "# Add columns to survey data. The support data contains information that can be used to\n",
    "# group objects or survey locations that may not be stored with the survey data. In this\n",
    "# example an adiminstrative label is attached to each survey_id. The cantonal label:\n",
    "survey_data = surveys.merge(beaches['canton'], left_on='slug', right_index=True, validate='many_to_one')\n",
    "# survey_data = survey_data.loc[survey_data.code == 'G27'].copy()\n",
    "survey_data = survey_data[survey_data.feature_name != 'aare'].copy()\n",
    "\n",
    "# ! USER DEFINED INPUT\n",
    "# Temporal and geographic boundaries.\n",
    "boundaries = dict(feature_type ='l', language='fr', start_date='2015-01-01', end_date='2022-01-01')\n",
    "# Make the report data and report\n",
    "top_label, language, w_df, w_di = r_class.report_data(boundaries, survey_data.copy(), beaches, codes)\n",
    "a_report = r_class.ReportClass(w_df, boundaries, top_label, 'fr', c_l)\n",
    "w_df_locations = w_df.slug.unique()\n",
    "\n",
    "# call the land use class on the two different location groups\n",
    "m_ui = LandUse(land_cover, land_use, streets, w_df_locations)\n",
    "\n",
    "# for the region of interest\n",
    "lcui = m_ui.use_of_land.copy()\n",
    "lc_sti, no_datai = match_topo_attributes_to_surveys(lcui, a_report.w_df)\n",
    "\n",
    "# the basic work data contains the survey results and the \n",
    "# topographical data merged on the <slug> column\n",
    "work_data_i = merge_topodata_and_surveydata(lc_sti, a_report.w_df)\n",
    "\n",
    "new_names = {\n",
    "    'slug':'location',\n",
    "    'loc_date':'sample_id',\n",
    "    'pcs_m':'pcs/m',\n",
    "    'Obstanlage': \"orchards\",\n",
    "    'Reben':'vineyards',\n",
    "    'Siedl':'buildings',\n",
    "    'Wald':'forest',\n",
    "    'land_use':'public services'\n",
    "}\n",
    "gptdf = work_data_i.rename(columns=new_names)\n",
    "```\n",
    "\n",
    "The preceding code produces the following table:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "27d7379c-e254-4680-ac6f-7040e6fd4b34",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'LandUse' object has no attribute 'slug'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[3], line 35\u001b[0m\n\u001b[1;32m     32\u001b[0m m_ui \u001b[38;5;241m=\u001b[39m LandUse(land_cover, land_use, streets, w_df_locations)\n\u001b[1;32m     34\u001b[0m \u001b[38;5;66;03m# for the region of interest\u001b[39;00m\n\u001b[0;32m---> 35\u001b[0m lcui \u001b[38;5;241m=\u001b[39m \u001b[43mm_ui\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse_of_land\u001b[49m\u001b[38;5;241m.\u001b[39mcopy()\n\u001b[1;32m     36\u001b[0m lc_sti, no_datai \u001b[38;5;241m=\u001b[39m match_topo_attributes_to_surveys(lcui, a_report\u001b[38;5;241m.\u001b[39mw_df)\n\u001b[1;32m     38\u001b[0m \u001b[38;5;66;03m# the basic work data contains the survey results and the \u001b[39;00m\n\u001b[1;32m     39\u001b[0m \u001b[38;5;66;03m# topographical data merged on the <slug> column\u001b[39;00m\n",
      "Cell \u001b[0;32mIn[2], line 197\u001b[0m, in \u001b[0;36mLandUse.use_of_land\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    195\u001b[0m \u001b[38;5;129m@property\u001b[39m\n\u001b[1;32m    196\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21muse_of_land\u001b[39m(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m--> 197\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mland_cover\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mland_use\u001b[49m, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    198\u001b[0m     b \u001b[38;5;241m=\u001b[39m a\u001b[38;5;241m.\u001b[39mmerge(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtrans, left_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m, right_index\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m)\n\u001b[1;32m    200\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m b\n",
      "Cell \u001b[0;32mIn[2], line 186\u001b[0m, in \u001b[0;36mLandUse.land_use\u001b[0;34m(self, new_labels)\u001b[0m\n\u001b[1;32m    184\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_topographic_attributes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlu, locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations, list_of_labels\u001b[38;5;241m=\u001b[39mnew_labels)\n\u001b[1;32m    185\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 186\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m group_topographic_attributes(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlu, locations\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlocations, list_of_labels\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mslug\u001b[49m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'LandUse' object has no attribute 'slug'"
     ]
    }
   ],
   "source": [
    "# Collecting required data to establish a report\n",
    "# This includes the language maps for all the common\n",
    "# abbreviations and columns or index labels.\n",
    "c_l = r_class.language_maps()\n",
    "\n",
    "# The survey data in units of pcs/m or pcs/m². The reports\n",
    "# are aggregated first to the sample_id. Which means that the operations\n",
    "# are the same wether using pcs/m or pcs/m².\n",
    "surveys = r_class.collect_survey_data_for_report()\n",
    "\n",
    "# The support or evnvironmental data. This includes plain text descriptions \n",
    "# of the Codes. Details for each survey location and topogrphical data\n",
    "# extracted from the buffer around each survey location.\n",
    "codes, beaches, land_cover, land_use, streets, river_intersect_lakes = r_class.collect_env_data_for_report()\n",
    "\n",
    "# Add columns to survey data. The support data contains information that can be used to\n",
    "# group objects or survey locations that may not be stored with the survey data. In this\n",
    "# example an adiminstrative label is attached to each survey_id. The cantonal label:\n",
    "survey_data = surveys.merge(beaches['canton'], left_on='slug', right_index=True, validate='many_to_one')\n",
    "# survey_data = survey_data.loc[survey_data.code == 'G27'].copy()\n",
    "survey_data = survey_data[survey_data.feature_name != 'aare'].copy()\n",
    "\n",
    "# ! USER DEFINED INPUT\n",
    "# Temporal and geographic boundaries.\n",
    "boundaries = dict(feature_type ='l', language='fr', start_date='2015-01-01', end_date='2022-01-01')\n",
    "# Make the report data and report\n",
    "top_label, language, w_df, w_di = r_class.report_data(boundaries, survey_data.copy(), beaches, codes)\n",
    "a_report = r_class.ReportClass(w_df, boundaries, top_label, 'fr', c_l)\n",
    "w_df_locations = w_df.slug.unique()\n",
    "\n",
    "# call the land use class on the two different location groups\n",
    "m_ui = LandUse(land_cover, land_use, streets, w_df_locations)\n",
    "\n",
    "# for the region of interest\n",
    "lcui = m_ui.use_of_land.copy()\n",
    "lc_sti, no_datai = match_topo_attributes_to_surveys(lcui, a_report.w_df)\n",
    "\n",
    "# the basic work data contains the survey results and the \n",
    "# topographical data merged on the <slug> column\n",
    "work_data_i = merge_topodata_and_surveydata(lc_sti, a_report.w_df)\n",
    "\n",
    "# column categories\n",
    "geo_features = ['feature_type', 'vineyards', 'orchards', 'buildings', 'forest', 'undefined', 'public services', 'streets','parent_boundary']\n",
    "admin_boundaries = ['city', 'canton', 'feature_name']\n",
    "sample_variables = ['location', 'sample_id', 'date']\n",
    "target_variables = ['pcs/m', 'quantity']\n",
    "\n",
    "new_names = {'slug':'location', 'loc_date':'sample_id', 'pcs_m':'pcs/m', 'Obstanlage': \"orchards\", 'Reben':'vineyards', 'Siedl':'buildings', 'Wald':'forest', 'land_use':'public services'}\n",
    "gptdf = work_data_i.rename(columns=new_names)\n",
    "\n",
    "groupby = ['sample_id','location',  'date', 'feature_name', 'parent_boundary',\n",
    "       'city', 'canton', 'feature_type',\n",
    "       'orchards', 'vineyards', 'buildings', 'forest', 'undefined',\n",
    "       'public services', 'streets', 'code']\n",
    "\n",
    "gptdfx = gptdf.groupby(groupby, as_index=False).agg({'pcs/m': 'sum', 'quantity':'sum'})\n",
    "gptdf.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aa30badb-aabb-439e-a52c-af5e1ee6fea1",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Hand file to assistant\n",
    "\n",
    "#### Add language definitions\n",
    "\n",
    "The language definitions ensure an efficient transmission of intent from the observer to the model. We could leave the translations and definitions up to a translator and thus reduce the weight of the .csv file or API request. Howver this would generate an additional service by the client to get the requested information translated. Providing the definitions according to the standard set in the Federal report is a good baseline. If their is support amongst stakeholders to change the definitions then this can be handled by a pull request or raising an issue on the repo.\n",
    "\n",
    "```python\n",
    "gptdf['fr'] = gptdf.code.map(lambda x: codes.loc[x, 'fr'])\n",
    "gptdf['en'] = gptdf.code.map(lambda x: codes.loc[x, 'en'])\n",
    "gptdf['de'] = gptdf.code.map(lambda x: codes.loc[x, 'de'])\n",
    "\n",
    "gptdf.to_csv('data/in_process/lakes.csv', index=False)\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc3aefe5-d723-4eca-bcca-a5609bc38724",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "gptdfx['fr'] = gptdfx.code.map(lambda x: codes.loc[x, 'fr'])\n",
    "gptdfx['en'] = gptdfx.code.map(lambda x: codes.loc[x, 'en'])\n",
    "gptdfx['de'] = gptdfx.code.map(lambda x: codes.loc[x, 'de'])\n",
    "\n",
    "gptdfx.to_csv('data/in_process/lakes.csv', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d70ee085-b786-4149-9a73-a6cfbe6ef408",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptdfx.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10bc04b5-0d5f-49c1-b80e-93551033c0c3",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptdfx[gptdfx.code == 'G79']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d84485-7dc0-440b-b400-df535e4f430d",
   "metadata": {},
   "outputs": [],
   "source": [
    "gptdfx.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2a7146-b12d-483e-a68f-2286aca48704",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Column names and definitions\n",
    "\n",
    "These column names and definitions are given to the GPT assistant.\n",
    "\n",
    "1. location: the name of the location used by people doing the survey\n",
    "2. sample_id: the combination of the location and date, the unique identifier of a sampling event\n",
    "3. date: the data of the sample\n",
    "4. feature_name: the name of the park, lake, or river where the sample was collected\n",
    "5. parent_boundary: a designated survey area, usually a river basin or regional label\n",
    "6. city: the muniicpality where the sample was taken\n",
    "7. canton: the canton where the sample was taken\n",
    "8. pcs/m: the number of objects identified by the column _code_ collected at the sampling event divided by the length of shoreline, river bank or trail that was sampled.\n",
    "9. quantity: the number of objects identified by the column _code_ collected at the sampling event\n",
    "10. code: the Marine Litter Watch object code\n",
    "11. feature_type: identifies the sample location as either a park, lake or river\n",
    "12. orchard: % of dry land attributed to this land-use within 1'500 m of the survey location\n",
    "13. vineyards: % of dry land attributed to this land-use within 1'500 m of the survey location\n",
    "14. buildings: % of dry land attributed to this land-use within 1'500 m of the survey location\n",
    "15. forest: % of dry land attributed to this land-use within 1'500 m of the survey location\n",
    "16. undefined: % of dry land with no land-use label\n",
    "17. public services: % of dry land attributed to hospitals, schools, sports, administration\n",
    "18. streets: the number of meters of streets within 1 500 m of the survey location. scaled between 0 - 1.\n",
    "19. fr: french code definitions\n",
    "20. en: english code definitions\n",
    "21. de: german code definitions\n",
    "\n",
    "```{note}\n",
    "The GPT will go through data exploration at the begining of the chat. These column defintions are given to the GPT and can be requested at any time. The definitions the GPT gives you should be very close to these definitions, it is not tell the GPT to use the provided definition in its instructions. These definitions should come back.\n",
    "``` "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88bd7255-0a46-4d97-b07b-a69069991e79",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Verifying the output\n",
    "\n",
    "### Test statistics\n",
    "\n",
    "Asking for each of these individually or telling the assistant to produce them all should yield the following results:\n",
    "\n",
    "* the median sample total of the data frame\n",
    "* the total quantity\n",
    "* the number of lakes\n",
    "* the number of samples\n",
    "* the number of cantons\n",
    "* the number of cities"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a41485c4-3e68-4d2d-901a-6792a0c1429f",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "outputs": [],
   "source": [
    "gp_dt = gptdfx.groupby(['sample_id', *geo_features], as_index=False).agg({'pcs/m':'sum', 'quantity':'sum'})\n",
    "\n",
    "lakes = gptdfx[gptdfx.feature_type == 'l'].feature_name.nunique()\n",
    "cities = gptdfx.city.nunique()\n",
    "quantity = gptdfx.quantity.sum()\n",
    "samples = gptdfx.sample_id.nunique()\n",
    "cantons = gptdfx.canton.nunique()\n",
    "pc_med = gp_dt['pcs/m'].median()\n",
    "\n",
    "test_1 = dict(lakes=lakes, cities=cities, quantity=quantity, samples=samples, cantons=cantons, median_pcs_m = pc_med)\n",
    "print(test_1)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f48cd38-507e-4594-b077-07696d956d4d",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Most common\n",
    "\n",
    "The most common codes are those codes that are either in the top ten by quantity or present in at lease 50% of the surveys."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0870ea9-048f-41ed-974e-3f53d39e6d45",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "most_common, weight = a_report.most_common\n",
    "most_common"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a90a3543-f306-46e9-a59c-4c16d4cf6da4",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Aggregating samples"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f68e7ff2-4df9-442d-bea0-1f67f1d68afd",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Sample total pcs/m"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ac02f74-d030-447e-afbe-f37fa824e1a1",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_dt['pcs/m'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97b116a5-d81c-43e8-ab38-942453a58280",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Single code\n",
    "\n",
    "cigarette ends"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ac66d601-f5b0-465d-b827-48c2b7b28d37",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_dtcode = gptdfx[gptdfx.code.isin(['G27'])].groupby(['sample_id', *geo_features], as_index=False).agg({'pcs/m':'sum', 'quantity':'sum'})\n",
    "gp_dtcode['pcs/m'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7264239-1087-468b-a543-ed6e53c94d7d",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Combining codes\n",
    "\n",
    "combining cigarette ends and snack wrappers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f9ea3ed-5c74-42b7-819e-e46b3c43fcea",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_dtcodes = gptdfx[gptdfx.code.isin(['G27', 'G30'])].groupby(['sample_id', *geo_features], as_index=False).agg({'pcs/m':'sum', 'quantity':'sum'})\n",
    "gp_dtcodes['pcs/m'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5864933e-6158-4593-822f-2fc74aa5801a",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Single feature\n",
    "\n",
    "the results on Bielersee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d62acea4-aff8-4782-a6eb-5f0176ae066d",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_dtbsee = gptdfx[gptdfx.feature_name == 'bielersee'].groupby(['sample_id', *geo_features], as_index=False).agg({'pcs/m':'sum', 'quantity':'sum'})\n",
    "gp_dtbsee['pcs/m'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01d68c66-2f88-414a-9e18-77b5df32d0fa",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Combined features\n",
    "\n",
    "Bielersee and Thunersee"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "793c6b5b-89b7-4dff-9ede-7edd36b00ad7",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "gp_dtbt = gptdfx[gptdfx.feature_name.isin(['bielersee', 'thunersee'])].groupby(['sample_id', *geo_features], as_index=False).agg({'pcs/m':'sum', 'quantity':'sum'})\n",
    "gp_dtbt['pcs/m'].describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1adcba67-1943-44f8-a9d1-4a655fb85d1c",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%% md\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Land use\n",
    "\n",
    "Correlation matrix of the land use variables with each other"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce56857b-cd4b-4f40-8888-baa857db833c",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "corrs = gp_dtbt[geo_features[1:-1]].corr()\n",
    "corrs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf508f4a-8861-484b-88bb-a7febdb1b8c2",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "%watermark -a hammerdirt-analyst -co --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "88e87d93-8bdc-4c38-be8d-3cd1faf16c03",
   "metadata": {
    "editable": true,
    "pycharm": {
     "name": "#%%\n"
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}