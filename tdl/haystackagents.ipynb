{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "52d3b43f-0252-4d38-8858-6cd87f02d7d0",
   "metadata": {},
   "source": [
    "# Haystack agents\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "cf9c082d-ec83-4c84-b4dc-d28acbae9aa5",
   "metadata": {},
   "outputs": [],
   "source": [
    "from dotenv import load_dotenv\n",
    "from IPython.display import Markdown, display\n",
    "import os\n",
    "\n",
    "           "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "371f9ccc-b591-4f97-91d9-dd8ef8df3356",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f0d601-c8a0-4c3d-8b63-24ed09dea8ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/anaconda3/envs/haystack2/lib/python3.12/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "from haystack import Pipeline, Document\n",
    "from haystack.utils import Secret\n",
    "from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "from haystack.components.retrievers.in_memory import InMemoryBM25Retriever\n",
    "from haystack.components.generators import OpenAIGenerator\n",
    "from haystack.components.builders.answer_builder import AnswerBuilder\n",
    "from haystack.components.builders.prompt_builder import PromptBuilder\n",
    "\n",
    "from haystack.components.converters import MarkdownToDocument\n",
    "from haystack.components.preprocessors import DocumentCleaner\n",
    "from haystack.components.preprocessors import DocumentSplitter\n",
    "from haystack.components.writers import DocumentWriter\n",
    "from haystack.components.retrievers.filter_retriever import FilterRetriever\n",
    "\n",
    "from haystack.components.converters.pypdf import PyPDFToDocument\n",
    "from haystack_integrations.document_stores.chroma import ChromaDocumentStore\n",
    "from haystack_integrations.components.retrievers.chroma import ChromaQueryTextRetriever\n",
    "from haystack import component\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "model = 'gpt-4o-mini'\n",
    "\n",
    "def print_results(results, reply_object='llm'):\n",
    "    return Markdown(result[reply_object]['replies'][0])   \n",
    "\n",
    "document_store = InMemoryDocumentStore()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "261dc060-f2d9-4f9b-ac90-f96640b5e59e",
   "metadata": {},
   "source": [
    "## In memory vector store : Report results\n",
    "Retrieving report results from an in memory vector store, using the FilterRetriever on the metadata\n",
    "\n",
    "### Markdown documents"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "b4be7b65-f0df-4cf0-a617-1f5dce0d960c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class InMemoryDocumentStore in module haystack.document_stores.in_memory.document_store:\n",
      "\n",
      "class InMemoryDocumentStore(builtins.object)\n",
      " |  InMemoryDocumentStore(bm25_tokenization_regex: str = '(?u)\\\\b\\\\w\\\\w+\\\\b', bm25_algorithm: Literal['BM25Okapi', 'BM25L', 'BM25Plus'] = 'BM25L', bm25_parameters: Optional[Dict] = None, embedding_similarity_function: Literal['dot_product', 'cosine'] = 'dot_product', index: Optional[str] = None)\n",
      " |\n",
      " |  Stores data in-memory. It's ephemeral and cannot be saved to disk.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, bm25_tokenization_regex: str = '(?u)\\\\b\\\\w\\\\w+\\\\b', bm25_algorithm: Literal['BM25Okapi', 'BM25L', 'BM25Plus'] = 'BM25L', bm25_parameters: Optional[Dict] = None, embedding_similarity_function: Literal['dot_product', 'cosine'] = 'dot_product', index: Optional[str] = None)\n",
      " |      Initializes the DocumentStore.\n",
      " |\n",
      " |      :param bm25_tokenization_regex: The regular expression used to tokenize the text for BM25 retrieval.\n",
      " |      :param bm25_algorithm: The BM25 algorithm to use. One of \"BM25Okapi\", \"BM25L\", or \"BM25Plus\".\n",
      " |      :param bm25_parameters: Parameters for BM25 implementation in a dictionary format.\n",
      " |          For example: {'k1':1.5, 'b':0.75, 'epsilon':0.25}\n",
      " |          You can learn more about these parameters by visiting https://github.com/dorianbrown/rank_bm25.\n",
      " |      :param embedding_similarity_function: The similarity function used to compare Documents embeddings.\n",
      " |          One of \"dot_product\" (default) or \"cosine\". To choose the most appropriate function, look for information\n",
      " |          about your embedding model.\n",
      " |      :param index: A specific index to store the documents. If not specified, a random UUID is used.\n",
      " |          Using the same index allows you to store documents across multiple InMemoryDocumentStore instances.\n",
      " |\n",
      " |  bm25_retrieval(self, query: str, filters: Optional[Dict[str, Any]] = None, top_k: int = 10, scale_score: bool = False) -> List[haystack.dataclasses.document.Document]\n",
      " |      Retrieves documents that are most relevant to the query using BM25 algorithm.\n",
      " |\n",
      " |      :param query: The query string.\n",
      " |      :param filters: A dictionary with filters to narrow down the search space.\n",
      " |      :param top_k: The number of top documents to retrieve. Default is 10.\n",
      " |      :param scale_score: Whether to scale the scores of the retrieved documents. Default is False.\n",
      " |      :returns: A list of the top_k documents most relevant to the query.\n",
      " |\n",
      " |  count_documents(self) -> int\n",
      " |      Returns the number of how many documents are present in the DocumentStore.\n",
      " |\n",
      " |  delete_documents(self, document_ids: List[str]) -> None\n",
      " |      Deletes all documents with matching document_ids from the DocumentStore.\n",
      " |\n",
      " |      :param document_ids: The object_ids to delete.\n",
      " |\n",
      " |  embedding_retrieval(self, query_embedding: List[float], filters: Optional[Dict[str, Any]] = None, top_k: int = 10, scale_score: bool = False, return_embedding: bool = False) -> List[haystack.dataclasses.document.Document]\n",
      " |      Retrieves documents that are most similar to the query embedding using a vector similarity metric.\n",
      " |\n",
      " |      :param query_embedding: Embedding of the query.\n",
      " |      :param filters: A dictionary with filters to narrow down the search space.\n",
      " |      :param top_k: The number of top documents to retrieve. Default is 10.\n",
      " |      :param scale_score: Whether to scale the scores of the retrieved Documents. Default is False.\n",
      " |      :param return_embedding: Whether to return the embedding of the retrieved Documents. Default is False.\n",
      " |      :returns: A list of the top_k documents most relevant to the query.\n",
      " |\n",
      " |  filter_documents(self, filters: Optional[Dict[str, Any]] = None) -> List[haystack.dataclasses.document.Document]\n",
      " |      Returns the documents that match the filters provided.\n",
      " |\n",
      " |      For a detailed specification of the filters, refer to the DocumentStore.filter_documents() protocol\n",
      " |      documentation.\n",
      " |\n",
      " |      :param filters: The filters to apply to the document list.\n",
      " |      :returns: A list of Documents that match the given filters.\n",
      " |\n",
      " |  save_to_disk(self, path: str) -> None\n",
      " |      Write the database and its' data to disk as a JSON file.\n",
      " |\n",
      " |      :param path: The path to the JSON file.\n",
      " |\n",
      " |  to_dict(self) -> Dict[str, Any]\n",
      " |      Serializes the component to a dictionary.\n",
      " |\n",
      " |      :returns:\n",
      " |          Dictionary with serialized data.\n",
      " |\n",
      " |  write_documents(self, documents: List[haystack.dataclasses.document.Document], policy: haystack.document_stores.types.policy.DuplicatePolicy = <DuplicatePolicy.NONE: 'none'>) -> int\n",
      " |      Refer to the DocumentStore.write_documents() protocol documentation.\n",
      " |\n",
      " |      If `policy` is set to `DuplicatePolicy.NONE` defaults to `DuplicatePolicy.FAIL`.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_dict(data: Dict[str, Any]) -> 'InMemoryDocumentStore'\n",
      " |      Deserializes the component from a dictionary.\n",
      " |\n",
      " |      :param data:\n",
      " |          The dictionary to deserialize from.\n",
      " |      :returns:\n",
      " |          The deserialized component.\n",
      " |\n",
      " |  load_from_disk(path: str) -> 'InMemoryDocumentStore'\n",
      " |      Load the database and its' data from disk as a JSON file.\n",
      " |\n",
      " |      :param path: The path to the JSON file.\n",
      " |      :returns: The loaded InMemoryDocumentStore.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Readonly properties defined here:\n",
      " |\n",
      " |  storage\n",
      " |      Utility property that returns the storage used by this instance of InMemoryDocumentStore.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(InMemoryDocumentStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "77a45264-3a55-4167-9e31-f2c5b18dc64d",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Converting markdown files to Documents: 100%|â–ˆâ–ˆâ–ˆâ–ˆâ–ˆ| 2/2 [00:00<00:00, 22.29it/s]\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fedfe0d6c00>\n",
       "ðŸš… Components\n",
       "  - retriever: FilterRetriever\n",
       "  - prompt_builder: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - retriever.documents -> prompt_builder.documents (List[Document])\n",
       "  - prompt_builder.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# from haystack import Pipeline\n",
    "# from haystack.document_stores.in_memory import InMemoryDocumentStore\n",
    "\n",
    "model = 'gpt-4o-mini'\n",
    "report_document_store = InMemoryDocumentStore()\n",
    "\n",
    "document_pipeline = Pipeline()\n",
    "document_pipeline.add_component(\"converter\", MarkdownToDocument())\n",
    "document_pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "document_pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"sentence\", split_length=10))\n",
    "document_pipeline.add_component(\"writer\", DocumentWriter(document_store=report_document_store))\n",
    "document_pipeline.connect(\"converter\", \"cleaner\")\n",
    "document_pipeline.connect(\"cleaner\", \"splitter\")\n",
    "document_pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "document_pipeline.run({\"converter\": {\"sources\": [\"vaud_report_results.md\", \"report_results.md\"], 'meta':[{'doc-id': 'Vaud 2015-11-15 2021-12-31', 'topic':'reports'}, {'doc-id': 'Bern 2015-11-15 2021-12-31', 'topic':'reports'}]}})\n",
    "\n",
    "prompt_template = \"\"\"\n",
    "    Given these documents, answer the question. Please provide the name of the reference(s) where the information came from\\nDocuments:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "report_pipeline = Pipeline()\n",
    "report_pipeline.add_component(name=\"retriever\", instance=FilterRetriever(document_store=report_document_store))\n",
    "report_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder\")\n",
    "report_pipeline.add_component(instance=OpenAIGenerator(model=model), name=\"llm\")\n",
    "report_pipeline.connect(\"retriever\", \"prompt_builder.documents\")\n",
    "report_pipeline.connect(\"prompt_builder\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "540999d7-377b-4db1-83b2-184f2944d85d",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# some_pdfs = [\n",
    "    \n",
    "#     {'topic': 'history of research, methods of research', 'source': 'resources/brief_history_marine_litter.pdf'},\n",
    "#     {'topic': 'threshold values, methods of calculation', 'source': 'resources/coastline_litter_threshold_value_report_14_9_2020_final.pdf'},\n",
    "#     {'topic': 'geospatial analysis, land use, feature evaluation', 'source': 'resources/revealing_the_role_of_landuse.pdf'}\n",
    "# ]\n",
    "\n",
    "# converter = PyPDFToDocument()\n",
    "\n",
    "# dox = []\n",
    "# for element in some_pdfs:\n",
    "#     results = converter.run(sources=[element['source']], meta={\"topic\":element['topic']})\n",
    "#     dox.append(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "c78f6e2b-f1eb-484c-8b14-536f3898ad5c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Help on class ChromaDocumentStore in module haystack_integrations.document_stores.chroma.document_store:\n",
      "\n",
      "class ChromaDocumentStore(builtins.object)\n",
      " |  ChromaDocumentStore(collection_name: str = 'documents', embedding_function: str = 'default', persist_path: Optional[str] = None, distance_function: Literal['l2', 'cosine', 'ip'] = 'l2', metadata: Optional[dict] = None, **embedding_function_params)\n",
      " |\n",
      " |  A document store using [Chroma](https://docs.trychroma.com/) as the backend.\n",
      " |\n",
      " |  We use the `collection.get` API to implement the document store protocol,\n",
      " |  the `collection.search` API will be used in the retriever instead.\n",
      " |\n",
      " |  Methods defined here:\n",
      " |\n",
      " |  __init__(self, collection_name: str = 'documents', embedding_function: str = 'default', persist_path: Optional[str] = None, distance_function: Literal['l2', 'cosine', 'ip'] = 'l2', metadata: Optional[dict] = None, **embedding_function_params)\n",
      " |      Initializes the store. The __init__ constructor is not part of the Store Protocol\n",
      " |      and the signature can be customized to your needs. For example, parameters needed\n",
      " |      to set up a database client would be passed to this method.\n",
      " |\n",
      " |      Note: for the component to be part of a serializable pipeline, the __init__\n",
      " |      parameters must be serializable, reason why we use a registry to configure the\n",
      " |      embedding function passing a string.\n",
      " |\n",
      " |      :param collection_name: the name of the collection to use in the database.\n",
      " |      :param embedding_function: the name of the embedding function to use to embed the query\n",
      " |      :param persist_path: where to store the database. If None, the database will be `in-memory`.\n",
      " |      :param distance_function: The distance metric for the embedding space.\n",
      " |          - `\"l2\"` computes the Euclidean (straight-line) distance between vectors,\n",
      " |          where smaller scores indicate more similarity.\n",
      " |          - `\"cosine\"` computes the cosine similarity between vectors,\n",
      " |          with higher scores indicating greater similarity.\n",
      " |          - `\"ip\"` stands for inner product, where higher scores indicate greater similarity between vectors.\n",
      " |          **Note**: `distance_function` can only be set during the creation of a collection.\n",
      " |          To change the distance metric of an existing collection, consider cloning the collection.\n",
      " |      :param metadata: a dictionary of chromadb collection parameters passed directly to chromadb's client\n",
      " |          method `create_collection`. If it contains the key `\"hnsw:space\"`, the value will take precedence over the\n",
      " |          `distance_function` parameter above.\n",
      " |\n",
      " |      :param embedding_function_params: additional parameters to pass to the embedding function.\n",
      " |\n",
      " |  count_documents(self) -> int\n",
      " |      Returns how many documents are present in the document store.\n",
      " |\n",
      " |      :returns: how many documents are present in the document store.\n",
      " |\n",
      " |  delete_documents(self, document_ids: List[str]) -> None\n",
      " |      Deletes all documents with a matching document_ids from the document store.\n",
      " |\n",
      " |      :param document_ids: the object_ids to delete\n",
      " |\n",
      " |  filter_documents(self, filters: Optional[Dict[str, Any]] = None) -> List[haystack.dataclasses.document.Document]\n",
      " |      Returns the documents that match the filters provided.\n",
      " |\n",
      " |      Filters are defined as nested dictionaries. The keys of the dictionaries can be a logical operator (`\"$and\"`,\n",
      " |      `\"$or\"`, `\"$not\"`), a comparison operator (`\"$eq\"`, `$ne`, `\"$in\"`, `$nin`, `\"$gt\"`, `\"$gte\"`, `\"$lt\"`,\n",
      " |      `\"$lte\"`) or a metadata field name.\n",
      " |\n",
      " |      Logical operator keys take a dictionary of metadata field names and/or logical operators as value. Metadata\n",
      " |      field names take a dictionary of comparison operators as value. Comparison operator keys take a single value or\n",
      " |      (in case of `\"$in\"`) a list of values as value. If no logical operator is provided, `\"$and\"` is used as default\n",
      " |      operation. If no comparison operator is provided, `\"$eq\"` (or `\"$in\"` if the comparison value is a list) is used\n",
      " |      as default operation.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      filters = {\n",
      " |          \"$and\": {\n",
      " |              \"type\": {\"$eq\": \"article\"},\n",
      " |              \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},\n",
      " |              \"rating\": {\"$gte\": 3},\n",
      " |              \"$or\": {\n",
      " |                  \"genre\": {\"$in\": [\"economy\", \"politics\"]},\n",
      " |                  \"publisher\": {\"$eq\": \"nytimes\"}\n",
      " |              }\n",
      " |          }\n",
      " |      }\n",
      " |      # or simpler using default operators\n",
      " |      filters = {\n",
      " |          \"type\": \"article\",\n",
      " |          \"date\": {\"$gte\": \"2015-01-01\", \"$lt\": \"2021-01-01\"},\n",
      " |          \"rating\": {\"$gte\": 3},\n",
      " |          \"$or\": {\n",
      " |              \"genre\": [\"economy\", \"politics\"],\n",
      " |              \"publisher\": \"nytimes\"\n",
      " |          }\n",
      " |      }\n",
      " |      ```\n",
      " |\n",
      " |      To use the same logical operator multiple times on the same level, logical operators can take a list of\n",
      " |      dictionaries as value.\n",
      " |\n",
      " |      Example:\n",
      " |\n",
      " |      ```python\n",
      " |      filters = {\n",
      " |          \"$or\": [\n",
      " |              {\n",
      " |                  \"$and\": {\n",
      " |                      \"Type\": \"News Paper\",\n",
      " |                      \"Date\": {\n",
      " |                          \"$lt\": \"2019-01-01\"\n",
      " |                      }\n",
      " |                  }\n",
      " |              },\n",
      " |              {\n",
      " |                  \"$and\": {\n",
      " |                      \"Type\": \"Blog Post\",\n",
      " |                      \"Date\": {\n",
      " |                          \"$gte\": \"2019-01-01\"\n",
      " |                      }\n",
      " |                  }\n",
      " |              }\n",
      " |          ]\n",
      " |      }\n",
      " |      ```\n",
      " |\n",
      " |      :param filters: the filters to apply to the document list.\n",
      " |      :returns: a list of Documents that match the given filters.\n",
      " |\n",
      " |  search(self, queries: List[str], top_k: int, filters: Optional[Dict[str, Any]] = None) -> List[List[haystack.dataclasses.document.Document]]\n",
      " |      Search the documents in the store using the provided text queries.\n",
      " |\n",
      " |      :param queries: the list of queries to search for.\n",
      " |      :param top_k: top_k documents to return for each query.\n",
      " |      :param filters: a dictionary of filters to apply to the search. Accepts filters in haystack format.\n",
      " |      :returns: matching documents for each query.\n",
      " |\n",
      " |  search_embeddings(self, query_embeddings: List[List[float]], top_k: int, filters: Optional[Dict[str, Any]] = None) -> List[List[haystack.dataclasses.document.Document]]\n",
      " |      Perform vector search on the stored document, pass the embeddings of the queries instead of their text.\n",
      " |\n",
      " |      :param query_embeddings: a list of embeddings to use as queries.\n",
      " |      :param top_k: the maximum number of documents to retrieve.\n",
      " |      :param filters: a dictionary of filters to apply to the search. Accepts filters in haystack format.\n",
      " |\n",
      " |      :returns: a list of lists of documents that match the given filters.\n",
      " |\n",
      " |  to_dict(self) -> Dict[str, Any]\n",
      " |      Serializes the component to a dictionary.\n",
      " |\n",
      " |      :returns:\n",
      " |          Dictionary with serialized data.\n",
      " |\n",
      " |  write_documents(self, documents: List[haystack.dataclasses.document.Document], policy: haystack.document_stores.types.policy.DuplicatePolicy = <DuplicatePolicy.FAIL: 'fail'>) -> int\n",
      " |      Writes (or overwrites) documents into the store.\n",
      " |\n",
      " |      :param documents:\n",
      " |          A list of documents to write into the document store.\n",
      " |      :param policy:\n",
      " |          Not supported at the moment.\n",
      " |\n",
      " |      :raises ValueError:\n",
      " |          When input is not valid.\n",
      " |\n",
      " |      :returns:\n",
      " |          The number of documents written\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Class methods defined here:\n",
      " |\n",
      " |  from_dict(data: Dict[str, Any]) -> 'ChromaDocumentStore'\n",
      " |      Deserializes the component from a dictionary.\n",
      " |\n",
      " |      :param data:\n",
      " |          Dictionary to deserialize from.\n",
      " |      :returns:\n",
      " |          Deserialized component.\n",
      " |\n",
      " |  ----------------------------------------------------------------------\n",
      " |  Data descriptors defined here:\n",
      " |\n",
      " |  __dict__\n",
      " |      dictionary for instance variables\n",
      " |\n",
      " |  __weakref__\n",
      " |      list of weak references to the object\n",
      "\n"
     ]
    }
   ],
   "source": [
    "help(ChromaDocumentStore)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "f06cd4f2-daaf-4d0a-9c8c-17607b6cf278",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'writer': {'documents_written': 347}}"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_names = ['resources/brief_history_marine_litter.pdf', 'resources/coastline_litter_threshold_value_report_14_9_2020_final.pdf', 'resources/revealing_the_role_of_landuse.pdf']\n",
    "metas = [{'topic': 'history of research, methods of research'},  {'topic': 'threshold values, methods of calculation'}, {'topic': 'geospatial analysis, land use, feature evaluation'}]\n",
    "\n",
    "\n",
    "context_document_store = ChromaDocumentStore(collection_name='context_docs')\n",
    "\n",
    "pipeline = Pipeline()\n",
    "pipeline.add_component(\"converter\", PyPDFToDocument())\n",
    "pipeline.add_component(\"cleaner\", DocumentCleaner())\n",
    "pipeline.add_component(\"splitter\", DocumentSplitter(split_by=\"page\", split_length=1))\n",
    "pipeline.add_component(\"writer\", DocumentWriter(document_store=context_document_store))\n",
    "pipeline.connect(\"converter\", \"cleaner\")\n",
    "pipeline.connect(\"cleaner\", \"splitter\")\n",
    "pipeline.connect(\"splitter\", \"writer\")\n",
    "\n",
    "pipeline.run({\"converter\": {\"sources\": file_names, \"meta\": metas}})\n",
    "# retriever = ChromaQueryTextRetriever(document_store=context_document_store)\n",
    "# retriever2 = FilterRetriever(document_store)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "720d9ef2-5ca1-44e7-aa9d-4c9949466247",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1 5\n"
     ]
    }
   ],
   "source": [
    "g  = context_document_store.search(['land use and buffer zones'], top_k=5)\n",
    "print(len(g), len(g[0]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "44813e25-9d6a-4ad4-839b-78555ea9099b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'geospatial analysis, land use, feature evaluation'"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "g[0][0].meta['topic']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd392a27-8fdd-437c-9bcb-150cc42acfda",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "f1a94aac-5acf-4881-9d1b-48c685a975ac",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from haystack.components.readers import ExtractiveReader\n",
    "\n",
    "# reader = ExtractiveReader()\n",
    "# reader.warm_up()\n",
    "\n",
    "\n",
    "# extractive_qa_pipeline = Pipeline()\n",
    "\n",
    "# context_pipeline = Pipeline()\n",
    "# context_pipeline.add_component(name=\"context_retriever\", instance=FilterRetriever(document_store=context_document_store))\n",
    "# extractive_qa_pipeline.add_component(instance=reader, name=\"reader\")\n",
    "\n",
    "# extractive_qa_pipeline.connect(\"embedder.embedding\", \"context_retriever.query_embedding\")\n",
    "# extractive_qa_pipeline.connect(\"retriever.documents\", \"reader.documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "f29b4e49-889f-47e0-9c0f-316842abfcbb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<haystack.core.pipeline.pipeline.Pipeline object at 0x7fed9bf69010>\n",
       "ðŸš… Components\n",
       "  - context_retriever: FilterRetriever\n",
       "  - prompt_builder_context: PromptBuilder\n",
       "  - llm: OpenAIGenerator\n",
       "ðŸ›¤ï¸ Connections\n",
       "  - context_retriever.documents -> prompt_builder_context.documents (List[Document])\n",
       "  - prompt_builder_context.prompt -> llm.prompt (str)"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given these documents, answer the question. Please provide the name of the reference(s) in your response\\nDocuments:\n",
    "    {% for doc in documents %}\n",
    "        {{ doc.content }}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "context_pipeline = Pipeline()\n",
    "context_pipeline.add_component(name=\"context_retriever\", instance=FilterRetriever(document_store=context_document_store))\n",
    "context_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder_context\")\n",
    "context_pipeline.add_component(instance=OpenAIGenerator( model=model), name=\"llm\")\n",
    "context_pipeline.connect(\"context_retriever\", \"prompt_builder_context.documents\")\n",
    "context_pipeline.connect(\"prompt_builder_context\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f2018552-680a-4186-bc4a-3a761febaf43",
   "metadata": {},
   "outputs": [],
   "source": [
    "def context_pipeline_func(query: str):\n",
    "    print(\"context called\")\n",
    "    if 'context_filter' in query:\n",
    "        result = context_pipeline.run({\"retriever\": {\"filters\": context_filters}, \"prompt_builder\": {\"question\": query}})\n",
    "\n",
    "        return {\"reply\": result[\"llm\"][\"replies\"][0].content}\n",
    "    else:\n",
    "        return \"There is no relevant background in my library\"\n",
    "\n",
    "def report_pipeline_func(query: str, filters: dict):\n",
    "    print(\"report called\")\n",
    "    if 'report_filters' in query:\n",
    "        result = context_pipeline.run({\"retriever\": {\"filters\": filters}, \"prompt_builder\": {\"question\": query}})\n",
    "        return {\"reply\": result[\"llm\"][\"replies\"][0].content}\n",
    "    else:\n",
    "        return \"There is no survey data for that date range and location, would you like a list of available cantons?\"\n",
    "\n",
    "def call_tools(query, a_report_filter, a_context_filter):\n",
    "\n",
    "    report = report_pipeline_func(qeury, a_report_filter)\n",
    "    context = context_pipeline_func(query, a_context_filter)\n",
    "\n",
    "    return (f'the requested survey results:\\n\\n {report}\\n\\nSome relevant background info {context}')\n",
    "\n",
    "\n",
    "# result[reply_object]['replies'][0]\n",
    "\n",
    "@component\n",
    "class AssembleResources:\n",
    "    def __init__(self, query: str, report_filters: dict, context_filters: dict, context_pipeline: Pipeline, report_pipeline: Pipeline, reply_object: str = 'llm'):\n",
    "        self.report_filter = report_filters\n",
    "        self.context_filter = context_filters\n",
    "        self.query = query\n",
    "        self.cont_pipeline = context_pipeline\n",
    "        self.rep_pipeline = report_pipeline\n",
    "        self.reply_object = reply_object\n",
    "        \n",
    "\n",
    "    @component.output_types(report_results=str, context_results=str)\n",
    "    def run(self):\n",
    "        r = self.rep_pipeline.run({'retriever': {'filters': self.report_filter}, \"prompt_builder\":{\"question\":self.query}})\n",
    "        c = self.cont_pipeline.run({'context_retriever': {'filters': self.context_filter}, \"prompt_builder_context\":{\"question\":self.query}})\n",
    "        return {'report_results': r[self.reply_object]['replies'][0], 'context_results':c[self.reply_object]['replies'][0]}\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "044cf7ac-91df-4b06-af04-9847225e8f22",
   "metadata": {},
   "outputs": [],
   "source": [
    "context_filters = {\n",
    "    'topic': {\"$in\":['threshold values, methods of calculation']},\n",
    "}\n",
    "\n",
    "v = ['Vaud 2015-11-15 2021-12-31']\n",
    "b = ['Bern 2015-11-15 2021-12-31']\n",
    "combined = [*v,*b]\n",
    "\n",
    "\n",
    "\n",
    "report_filters = {\n",
    "    'doc-id': {\"$in\":combined},\n",
    "}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f70d2129-00a6-4dfb-8b74-910d95d4ae1f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The average pcs/m (objects per meter) in Bern is approximately 0.71, as stated in the summary statistics of the survey results.\n",
       "\n",
       "Beach litter is measured or calculated by collecting samples from designated survey locations. The specific metrics used in the analysis typically include the average number of litter objects identified per meter, the standard deviation of litter density, the number of samples collected, and the percentile distribution (5th, 25th, 50th, 75th, 95th percentiles), as well as maximum observed litter density.\n",
       "\n",
       "Other metrics utilized in the analysis might include:\n",
       "- Material composition of the litter identified (e.g., percentage of plastic).\n",
       "- Inventory items that categorize the type of litter.\n",
       "- Stratification based on land-use profiles around the surveyed locations, examining how litter density may vary in relation to different environmental features (e.g., buildings, wetlands, forests, public services).\n",
       "- Results from cluster analysis to identify similarities in land use among sampled locations.\n",
       "- Regression analysis outcomes that assess the relationship between land use features and litter density.\n",
       "\n",
       "This information was derived from the \"Survey report Canton Bern 2015-11-15 2021-12-31\" document."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "f = AssembleResources('What was the average pcs/m in Bern? How is beach litter measured or calculated? What other metrics are used?', report_filters, context_filters, context_pipeline, report_pipeline).run()\n",
    "Markdown(f['report_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "62abbc5d-023e-4e3b-b26d-6890ab214a82",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "The documents provided do not specifically mention the average number of litter pieces per meter (pcs/m) in Bern. They present information regarding a European threshold value for macro litter on coastlines but do not provide city-specific data.\n",
       "\n",
       "Beach litter is typically measured or calculated using the following methodologies:\n",
       "\n",
       "1. **Survey Length**: Litter is surveyed along a set length of beach, commonly 100 meters. This standardization helps in achieving comparability across different monitoring sites.\n",
       "\n",
       "2. **Data Collection**: Surveys are conducted multiple times a year, ideally in each season, to account for seasonal variations in litter accumulation.\n",
       "\n",
       "3. **Data Clean-Up**: The collected data undergo cleaning to exclude fragments of litter that are not comparable (like smaller pieces) and to focus on macro-sized materials typically larger than 2.5 cm.\n",
       "\n",
       "4. **Abundance Calculation**: The total abundance of litter per survey is calculated by summing up the remaining litter types and normalizing to 100 m of beach.\n",
       "\n",
       "5. **Statistical Methods**: Various statistical methods are employed to calculate threshold values based on percentiles (e.g., the 10th percentile, 15th percentile), which allows for robust assessment values while accounting for uncertainties.\n",
       "\n",
       "Other metrics that are commonly used in beach litter assessments include:\n",
       "\n",
       "- **Median Values**: The median number of litter pieces is considered a robust indicator to assess beach litter levels as it is less influenced by extreme values compared to mean values.\n",
       "\n",
       "- **Mean Values**: While mean values can provide an overview, they are more susceptible to distortion from outliers.\n",
       "\n",
       "- **Confidence Intervals**: Calculated to assess the uncertainty around the median assessment values, ensuring statistical reliability.\n",
       "\n",
       "In summary, beach litter assessments utilize a combination of systematic survey methodologies, statistical calculations for threshold values, and the use of metrics like mean and median to evaluate the litter's impact on coastal environments."
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# f = AssembleResources('Which had the highest average pieces per meter Vaud or Bern?', report_filters, context_filters, context_pipeline, report_pipeline).run()\n",
    "Markdown(f['context_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d07bec9-ecc4-4eba-82ea-908c329b6364",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack import component\n",
    "\n",
    "@component\n",
    "class CombinedAnswer:\n",
    "  \"\"\"\n",
    "  A component generating personal welcome message and making it upper case\n",
    "  \"\"\"\n",
    "  @component.output_types(welcome_text=str, note=str)\n",
    "  def run(self, name:str):\n",
    "    return {\"welcome_text\": ('Hello {name}, welcome to Haystack!'.format(name=name)).upper(), \"note\": \"welcome message is ready\"}\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "126ee629-9002-4608-a7ec-88b14fb379dd",
   "metadata": {},
   "outputs": [],
   "source": [
    "prompt_template = \"\"\"\n",
    "    Given the results from the report and the results of the litterature review  documents, answer the question. Please provide the name of the reference(s) in your response\\nDocuments:\n",
    "    \\nReport results: {{report_results}}\n",
    "    \\n\n",
    "    \\nLitterature review: {{context_results}}\n",
    "    {% endfor %}\n",
    "\n",
    "    \\nQuestion: {{question}}\n",
    "    \\nAnswer:\n",
    "    \"\"\"\n",
    "\n",
    "context_pipeline = Pipeline()\n",
    "# context_pipeline.add_component(name=\"context_retriever\", instance=FilterRetriever(document_store=context_document_store))\n",
    "# context_pipeline.add_component(instance=PromptBuilder(template=prompt_template), name=\"prompt_builder_context\")\n",
    "# context_pipeline.add_component(instance=OpenAIGenerator( model=model), name=\"llm\")\n",
    "# context_pipeline.connect(\"context_retriever\", \"prompt_builder_context.documents\")\n",
    "# context_pipeline.connect(\"prompt_builder_context\", \"llm\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dff251ec-b22c-4655-8d5a-0e070c95a6d8",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "73a1b9b4-0b97-49e4-8d7b-c7098398cecb",
   "metadata": {},
   "outputs": [
    {
     "ename": "SyntaxError",
     "evalue": "'break' outside loop (668683560.py, line 1)",
     "output_type": "error",
     "traceback": [
      "\u001b[0;36m  Cell \u001b[0;32mIn[13], line 1\u001b[0;36m\u001b[0m\n\u001b[0;31m    break\u001b[0m\n\u001b[0m    ^\u001b[0m\n\u001b[0;31mSyntaxError\u001b[0m\u001b[0;31m:\u001b[0m 'break' outside loop\n"
     ]
    }
   ],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "375b86de-13d0-491d-bae0-1643cd6653b4",
   "metadata": {},
   "source": [
    "## Chroma vector store\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "317fb269-f5c2-418c-88aa-bfd33fc03a51",
   "metadata": {},
   "outputs": [],
   "source": [
    "break"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cba50ed-a7c7-47b8-8abc-2a7387204e05",
   "metadata": {},
   "outputs": [],
   "source": [
    "# tools = [\n",
    "#     {\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"context_pipeline_func\",\n",
    "#             \"description\": \"Retrieves background information about the query from matching meta-data\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"query\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"query\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "#     {\n",
    "#         \"type\": \"function\",\n",
    "#         \"function\": {\n",
    "#             \"name\": \"report_pipeline_func\",\n",
    "#             \"description\": \"Retrieves the results from survey report documents based on the matching meta-data\",\n",
    "#             \"parameters\": {\n",
    "#                 \"type\": \"object\",\n",
    "#                 \"properties\": {\n",
    "#                     \"query\": {\n",
    "#                         \"type\": \"string\",\n",
    "#                         \"description\": \"The query to use in the search. Infer this from the user's message. It should be a question or a statement\",\n",
    "#                     },\n",
    "#                 },\n",
    "#                 \"required\": [\"query\"],\n",
    "#             },\n",
    "#         },\n",
    "#     },\n",
    "# ]\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "26ccb598-975d-4ec5-9bbb-bd7cc9e58764",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.components.retrievers.in_memory import InMemoryEmbeddingRetriever\n",
    "from haystack.components.readers import ExtractiveReader\n",
    "from haystack.components.embedders import SentenceTransformersTextEmbedder\n",
    "\n",
    "\n",
    "retriever = InMemoryEmbeddingRetriever(document_store=document_store)\n",
    "reader = ExtractiveReader()\n",
    "reader.warm_up()\n",
    "\n",
    "extractive_qa_pipeline = Pipeline()\n",
    "\n",
    "extractive_qa_pipeline.add_component(instance=SentenceTransformersTextEmbedder(model=model), name=\"embedder\")\n",
    "extractive_qa_pipeline.add_component(instance=retriever, name=\"retriever\")\n",
    "extractive_qa_pipeline.add_component(instance=reader, name=\"reader\")\n",
    "\n",
    "extractive_qa_pipeline.connect(\"embedder.embedding\", \"retriever.query_embedding\")\n",
    "extractive_qa_pipeline.connect(\"retriever.documents\", \"reader.documents\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23d5a3fc-0c48-4f7a-9d58-c2a34d333ada",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c0baa1cf-28bf-4c7a-940d-e8dd9b8264b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Markdown(f['context_results'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "883c9587-e84e-4fef-89cd-6eaadb2b58c2",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "15a2c514-8547-4e76-a8e9-8232719be584",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "import gradio as gr\n",
    "import json\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"reply with at least a helpfull message\"\n",
    "    ),\n",
    "    ChatMessage.from_user(\"What were the regression results for Vaud ? What were the regression results for Bern ? Whats the difference?.? context_filter, report_filters\")\n",
    "]\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=model, streaming_callback=print_streaming_chunk)\n",
    "response = chat_generator.run(messages=messages generation_kwargs={\"tools\": tools})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "be15c740-ceb9-4ab7-a898-cd5435fe1bde",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46d8198d-2676-4a35-8172-08e898ee324b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "from haystack.components.generators.utils import print_streaming_chunk\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\", streaming_callback=print_streaming_chunk)\n",
    "response = chat_generator.run(messages=messages)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b4c497b-2248-4e2b-8c10-c2e387682e17",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"replies\"][0].meta[\"finish_reason\"] == \"tool_calls\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ef5b8a98-a551-401a-a938-1f4dbdb37572",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"replies\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0072de5b-9d85-4238-9e43-dc87e6b887dc",
   "metadata": {},
   "outputs": [],
   "source": [
    "from haystack.dataclasses import ChatMessage\n",
    "from haystack.components.generators.chat import OpenAIChatGenerator\n",
    "\n",
    "chat_generator = OpenAIChatGenerator(model=\"gpt-3.5-turbo\")\n",
    "response = None\n",
    "messages = [\n",
    "    ChatMessage.from_system(\n",
    "        \"Don't make assumptions about what values to plug into functions. Ask for clarification if a user request is ambiguous.\"\n",
    "    )\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e04a490-e456-4aea-a836-45ec84346824",
   "metadata": {},
   "outputs": [],
   "source": [
    "response[\"replies\"][0].content"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00e60810-8c10-4b4c-9192-36ff478ff041",
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "\n",
    "## Parse function calling information\n",
    "function_call = json.loads(response[\"replies\"][0].content)[0]\n",
    "function_name = function_call[\"function\"][\"name\"]\n",
    "function_args = json.loads(function_call[\"function\"][\"arguments\"])\n",
    "print(\"Function Name:\", function_name)\n",
    "print(\"Function Arguments:\", function_args)\n",
    "\n",
    "## Find the correspoding function and call it with the given arguments\n",
    "available_functions = {\"rag_pipeline_func\": rag_pipeline_func, \"get_current_weather\": get_current_weather}\n",
    "function_to_call = available_functions[function_name]\n",
    "function_response = function_to_call(**function_args)\n",
    "print(\"Function Response:\", function_response)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "daaccfc6-d7e4-4532-902a-bde936e1e4de",
   "metadata": {},
   "outputs": [],
   "source": [
    "response"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "91c06f63-d398-4608-a032-945047cb0d4f",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_filters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c47ae80-3507-4e62-9b21-05be54684d1a",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
