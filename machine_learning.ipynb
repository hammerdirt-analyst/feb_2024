{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c6bfd7-6b8a-4bec-82a4-2a34360f2ddb",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "import session_config\n",
    "from session_config import  collect_survey_data, feature_variables\n",
    "from reports import make_report_objects, reports_and_forecast\n",
    "from reports import admin_report, features_present, histograms_standard\n",
    "from reports import ecdf_plots_standard, scatter_plot_standard\n",
    "from reports import labels_for_display, make_standard_report, make_report_objects\n",
    "# import userdisplay\n",
    "# import geospatial\n",
    "import gridforecast as gfcast\n",
    "# import datetime as dt\n",
    "from IPython.display import Markdown\n",
    "\n",
    "from featureevaluator import FeatureEvaluation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression, LassoCV, TheilSenRegressor\n",
    "from sklearn.ensemble import RandomForestRegressor, GradientBoostingRegressor, BaggingRegressor, VotingRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "# from sklearn.decomposition import PCA\n",
    "# from scipy.spatial import ConvexHull\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "import openai\n",
    "from dotenv import load_dotenv\n",
    "import os\n",
    "from myst_nb import glue\n",
    "\n",
    "# import bs4\n",
    "# from langchain import hub\n",
    "# from langchain.chains import create_retrieval_chain\n",
    "# from langchain.chains.combine_documents import create_stuff_documents_chain\n",
    "# from langchain_openai import ChatOpenAI\n",
    "# from langchain_community.document_loaders import WebBaseLoader\n",
    "# from langchain_core.prompts import ChatPromptTemplate\n",
    "# from langchain_openai import OpenAIEmbeddings\n",
    "# from langchain_text_splitters import RecursiveCharacterTextSplitter\n",
    "# from langchain.chains import create_history_aware_retriever\n",
    "# from langchain_core.prompts import MessagesPlaceholder\n",
    "# from langchain_chroma import Chroma\n",
    "# from langchain_text_splitters import MarkdownHeaderTextSplitter\n",
    "# from langchain_core.messages import AIMessage, HumanMessage\n",
    "# from langchain_openai import OpenAI\n",
    "\n",
    "load_dotenv()\n",
    "api_key = os.getenv('OPENAI_API_KEY')\n",
    "\n",
    "datax = collect_survey_data()\n",
    "codes = pd.read_csv('data/end_process/codes.csv').set_index('code')\n",
    "\n",
    "# from use_cases example\n",
    "ooi = ['G10',  'G30', 'G31', 'G33', 'G34', 'G35', 'G8', 'G7', 'G6', 'G5', 'G4', 'G37', 'G2', 'G27', 'G25', 'G26', 'G11']\n",
    "# more refined search\n",
    "tobo_snacks = ['G27', 'G30', 'G35']\n",
    "# unidentified, plastic, different uses\n",
    "# udi = ['Gfrags', 'Gfoams']\n",
    "# industrial\n",
    "indus = ['G89', 'G67', 'G112', 'G93' , 'G66','G74', 'G72', 'G87', 'G65', 'G69', 'G68', 'G43', 'G41', 'G38', 'G36', 'G19', 'G17', 'Gfrags']\n",
    "\n",
    "# features\n",
    "land_covers = ['buildings', 'forest', 'undefined', 'public-services', 'recreation', 'streets']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4b5472bb-35f0-4642-b5da-fb16e9a46920",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "def evaluate_feature_importance(best_model, model_name, X_test, y_test, X_train, y_train):\n",
    "\n",
    "    # the permuation importance of the variables\n",
    "    if model_name in ['Random Forest Regression', 'Linear Regression', 'Gradient Boosting Regression',  'Theil-Sen Regressor']:\n",
    "        perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns,\n",
    "            'Importance': perm_importance.importances_mean\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    try:\n",
    "    # model feature importance\n",
    "        feature_importances_rf = best_model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns,\n",
    "            'Importance': feature_importances_rf\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        return feature_importance_df, perm_importance_df\n",
    "    except AttributeError:\n",
    "    # if feature importance not avaialable try the coefficients\n",
    "        try:\n",
    "            params = best_model.coef_\n",
    "            feature_importances_rf = params\n",
    "            feature_importance_df = pd.DataFrame({'feature':X_test.columns, 'Coeficient':feature_importances_rf})\n",
    "            return feature_importance_df, perm_importance_df\n",
    "        except AttributeError:\n",
    "            #return an empty DataFrame\n",
    "            return pd.DataFrame(), perm_importance_df\n",
    "\n",
    "def find_elbow_point(sse):\n",
    "    n_points = len(sse)\n",
    "    all_coords = np.vstack((range(n_points), sse)).T\n",
    "    first_point = all_coords[0]\n",
    "    last_point = all_coords[-1]\n",
    "\n",
    "    line_vec = last_point - first_point\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_product = np.sum(vec_from_first * line_vec_norm, axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line**2, axis=1))\n",
    "    elbow_point = np.argmax(dist_to_line)\n",
    "    \n",
    "    return elbow_point + 1\n",
    "\n",
    "def filter_features(data, threshold: float = 0.2, terms: [] = None ):\n",
    "   \n",
    "    filtered_columns = [col for col in terms if (data[col] > 0).mean() >= threshold]\n",
    "    return data[['pcs/m',  *filtered_columns]], filtered_columns\n",
    "    \n",
    "\n",
    "def determine_optimal_clusters(d):\n",
    "   \n",
    "    sse = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(d)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    optimal_k = find_elbow_point(sse)\n",
    "    return optimal_k, sse\n",
    "\n",
    "def kmeans_clustering(n_clusters, w_interactions: bool = False):\n",
    "    \n",
    "    kmeans = kmeans_plusplus(n_clusters=n_clusters, random_state=42)\n",
    "    \n",
    "        \n",
    "    d['clusters'] = kmeans.fit_predict(d)\n",
    "    some_features = [x for x in d.columns if x not in ['pcs/m','clusters', 'streets']]\n",
    "    \n",
    "    means = d.groupby(['clusters'])['pcs/m'].mean()\n",
    "    means_unscaled = self.unscale_target(means)\n",
    "    \n",
    "    counts = d.groupby(['clusters'])['pcs/m'].count()\n",
    "    \n",
    "    cluster_summary = d.groupby('clusters').agg({x:'mean' for x in some_features}).reset_index()\n",
    "    cluster_summary = self.unscale_values(cluster_summary, columns=some_features, w_interactions=w_interactions)\n",
    "    cluster_summary['pcs/m'] = means_unscaled\n",
    "    cluster_summary['samples'] = counts.values\n",
    "    cluster_summary = cluster_summary[['samples', 'pcs/m', *cluster_summary.columns[:-2]]]\n",
    "           \n",
    "    return cluster_summary, kmeans, d\n",
    "\n",
    "def unscale_target(means, ascaler):\n",
    "    means = means.values\n",
    "    means_shape = means.shape\n",
    "    if means.ndim == 1:\n",
    "        means = means.reshape(1, -1)\n",
    "\n",
    "    means_unscaled = ascaler.inverse_transform(means)\n",
    "        \n",
    "    means_unscaled.reshape(means_shape)\n",
    "    return means_unscaled[0]\n",
    "\n",
    "def perform_regression_analysis(d, features: [] = None, target_var: str = 'pcs/m'):\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"loss\": \"huber\",\n",
    "        \"alpha\": .9\n",
    "       \n",
    "        }\n",
    "    these_models = {\n",
    "        'Linear Regression': LinearRegression(),\n",
    "        'Random Forest Regression': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting Regression': GradientBoostingRegressor(**params),\n",
    "        'Theil-Sen Regressor': TheilSenRegressor(random_state=42)\n",
    "        }\n",
    "      \n",
    "    \n",
    "    X = d[features]\n",
    "    y = d[target_var].values\n",
    "       \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    regression_results = []\n",
    "    best_model = None\n",
    "    best_r2 = -np.inf\n",
    "    the_name = None\n",
    "    \n",
    "    # sklearn - linear models        \n",
    "    for model_name, model in these_models.items():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            regression_results.append({'Model': model_name, 'R²': r2, 'MSE': mse})\n",
    "            \n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model = model\n",
    "                the_name = model_name\n",
    "    # bagging\n",
    "    bag_estimator = these_models[the_name]\n",
    "    bag = BaggingRegressor(estimator=bag_estimator)\n",
    "    bag.fit(X_train, y_train)\n",
    "    y_pred = bag.predict(X_test)\n",
    "    predictions = {\n",
    "        the_name: best_model.predict(X_test),\n",
    "        'Bagging': y_pred\n",
    "    }\n",
    "\n",
    "    regression_results.append({'Model': f'Bagging:{the_name}', 'R²': bag.score(X_test, y_test), 'MSE':mean_squared_error(y_test, y_pred)})\n",
    "    # voting\n",
    "\n",
    "    lnr = these_models['Linear Regression']\n",
    "    rf = these_models['Random Forest Regression']\n",
    "    gbr = these_models['Gradient Boosting Regression']\n",
    "    voting = VotingRegressor([('lnr', lnr), ('rf', rf), ('gbr', gbr)])\n",
    "    voting.fit(X_train, y_train)\n",
    "    y_pred = voting.predict(X_test)\n",
    "    predictions.update({'voting': y_pred})\n",
    "    \n",
    "    regression_results.append({'Model': 'Voting', 'R²': voting.score(X_test, y_test), 'MSE':mean_squared_error(y_test, y_pred)})    \n",
    "    \n",
    "    return regression_results, best_model, the_name, predictions, X_test, y_test, X_train, y_train\n",
    "\n",
    "def create_interaction_terms(data, interaction_terms=None, target='pcs/m'):\n",
    "    if interaction_terms is None:\n",
    "        interaction_terms = ['streets', 'public-services', 'recreation']\n",
    "    \n",
    "    \n",
    "    d_cols = [x for x in data.columns if x not in [target, 'use']]\n",
    "    interaction_data = {}\n",
    "    interaction_columns = []\n",
    "    for col in d_cols:\n",
    "        if col not in interaction_terms:\n",
    "            feature_value = data[col].values\n",
    "            interaction_name = f'{col}'\n",
    "            for term in interaction_terms:\n",
    "                feature_value += data[col].values * data[term].values\n",
    "                interaction_name += f'_inter_{term}'\n",
    "                \n",
    "            interaction_data[interaction_name] = feature_value\n",
    "            interaction_columns.append(interaction_name)\n",
    "    \n",
    "    interaction_data = pd.DataFrame(interaction_data)\n",
    "    interaction_data[target] = data[target]\n",
    "    interaction_data['use'] = data['use']\n",
    "    return interaction_data, interaction_columns\n",
    "\n",
    "\n",
    "\n",
    "def clusters_by_use_case(cluster_data, scaled_cols: [] = None, columns_to_cluster: [] = None, scalers: {} = None):\n",
    "\n",
    "   \n",
    "       \n",
    "    cluster_p = cluster_data.copy()\n",
    "   \n",
    "    nclusters = determine_optimal_clusters(cluster_p[columns_to_cluster])\n",
    "    kmeans = KMeans(n_clusters=nclusters[0], random_state=42).fit(cluster_p[columns_to_cluster])\n",
    "    cluster_p['cluster'] = kmeans.labels_\n",
    "    cluster_p[scaled_cols] = scalers['feature_scaler'].inverse_transform(cluster_p[scaled_cols])\n",
    "    cluster_p['pcs/m'] = scalers['target_scaler'].inverse_transform(cluster_p['pcs/m'].values.reshape(-1,1))\n",
    "    cluster_p['streets'] = scalers['street_scaler'].inverse_transform(cluster_p['streets'].values.reshape(-1,1))\n",
    "    scaler = MinMaxScaler().fit(cluster_p['streets'].values.reshape(-1,1))\n",
    "    cluster_p['streets'] = scaler.transform(cluster_p['streets'].values.reshape(-1,1))\n",
    "    df = cluster_p.drop_duplicates('cluster').sort_values('cluster').set_index('cluster', drop=True)\n",
    "    pcs_m = cluster_p.groupby(['cluster'], as_index=False).agg({'pcs/m': 'mean'}).set_index('cluster', drop=True)\n",
    "    samps = cluster_p.groupby(['cluster'], as_index=False).agg({'pcs/m': 'count'}).rename(columns={'pcs/m':'nsamples'}).set_index('cluster', drop=True)\n",
    "    pcs_m['nsamps'] =samps.nsamples.values\n",
    "    df = pcs_m.merge(df[columns_to_cluster], left_index=True, right_index=True)\n",
    "\n",
    "    return cluster_p, df\n",
    "\n",
    "\n",
    "\n",
    "def append_to_markdown(filename, content):\n",
    "    with open(filename, 'a') as f:\n",
    "        f.write(content)\n",
    "\n",
    "def use_chat_completion(client, model: str = 'gpt-3.5-turbo-0125', messages: [{}] = None):\n",
    "    \n",
    "    \n",
    "    completed_chat = client.chat.completions.create(model=model, messages=messages)\n",
    "    return completed_chat\n",
    "\n",
    "def messages_for_chat_completion(system_prompt: str = None, user_prompt: str = None):\n",
    "    \n",
    "    messages=[\n",
    "    {\"role\": \"system\", \"content\": system_prompt},\n",
    "    {\"role\": \"user\", \"content\": user_prompt}]\n",
    "    \n",
    "    return messages\n",
    "\n",
    "def create_system_prompt(prompt, context=\"\") -> str:\n",
    "    return f\"{prompt}{context}\"\n",
    "\n",
    "\n",
    "\n",
    "o_dates = {'start':'2020-01-01', 'end':'2021-12-31'}\n",
    "prior_dates = {'start':'2015-11-15', 'end':'2019-12-31'}\n",
    "\n",
    "canton = 'Vaud'\n",
    "this_feature_type = 'l'\n",
    "\n",
    "d = datax.reset_index(drop=True)\n",
    "\n",
    "dc = d[d.canton.isin(['Genève', 'Valais', 'Vaud', 'Zürich', 'Bern'])].copy()\n",
    "dc['date'] = pd.to_datetime(dc['date'])\n",
    "dc_lakes = dc[dc.feature_type == 'l'].feature_name.unique()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "502e5d46-3a93-4739-9591-5a61e72612a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "def filter_dataframe(df, filters):\n",
    "    \"\"\"\n",
    "    Filters the DataFrame based on the provided dictionary of column-value pairs.\n",
    "    Special handling for 'start' and 'end' keys to filter date ranges.\n",
    "    \n",
    "    Args:\n",
    "    df (pd.DataFrame): The DataFrame to filter.\n",
    "    filters (dict): A dictionary where keys are column names and values are the values of interest.\n",
    "    \n",
    "    Returns:\n",
    "    pd.DataFrame: The filtered DataFrame.\n",
    "    \"\"\"\n",
    "    queries = []\n",
    "    date_column = None\n",
    "    \n",
    "    for col, value in filters.items():\n",
    "        if col == 'start':\n",
    "            date_column = 'date'  # Assuming 'date' is the column name for dates\n",
    "            queries.append(f\"({date_column} >= '{value}')\")\n",
    "        elif col == 'end':\n",
    "            date_column = 'date'  # Assuming 'date' is the column name for dates\n",
    "            queries.append(f\"({date_column} <= '{value}')\")\n",
    "        else:\n",
    "            if isinstance(value, str):\n",
    "                queries.append(f\"({col} == '{value}')\")\n",
    "            else:\n",
    "                queries.append(f\"({col} == {value})\")\n",
    "    \n",
    "    query = \" & \".join(queries)\n",
    "    return df.query(query)\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# data = filter_dataframe(dc.copy(), result_dates)\n",
    "# info_columns =  ['canton', 'city', 'feature_name']\n",
    "# first_report, first_land_use = make_report_objects(data, info_columns=info_columns)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "79793f07-7d8c-427e-8591-5a4fc57b265e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# from typing import Annotated, Literal, TypedDict\n",
    "# from typing import List, Optional, Dict\n",
    "\n",
    "# from langchain_core.tools import tool\n",
    "# from langgraph.checkpoint.memory import MemorySaver\n",
    "# from langgraph.graph import END, StateGraph, START, MessagesState\n",
    "# from langgraph.prebuilt import ToolNode\n",
    "# from langchain_openai import ChatOpenAI\n",
    "\n",
    "# from langchain_core.messages import (\n",
    "#     BaseMessage,\n",
    "#     HumanMessage,\n",
    "#     ToolMessage,\n",
    "# )\n",
    "# from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "507138ef-b6e9-498a-ac9b-1009975ef9e9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "o_dates = {'start':'2020-01-01', 'end':'2021-12-31'}\n",
    "prior_dates = {'start':'2015-11-15', 'end':'2019-12-31'}\n",
    "\n",
    "canton = 'Bern'\n",
    "this_feature_type = 'l'\n",
    "\n",
    "d = datax.reset_index(drop=True)\n",
    "\n",
    "d = d[d.canton.isin(['Genève', 'Valais', 'Vaud', 'Zürich', 'Bern'])]\n",
    "\n",
    "# make complete report\n",
    "params_l = {'canton':canton, 'date_range':o_dates, 'feature_type': this_feature_type}\n",
    "params_p = {'canton':canton, 'date_range':prior_dates, 'feature_type':this_feature_type}\n",
    "\n",
    "# set the parameters for the weighted prior\n",
    "# exclude records in the likelihood, set date range and feature type\n",
    "lu_catalogue = d[(d.canton != canton)&(d['date'] <= o_dates['end'])&(d.feature_type == 'l')].copy()\n",
    "catalog_surveys, catalog_features = make_report_objects(lu_catalogue)\n",
    "\n",
    "# this is the prior data: all data collected from\n",
    "# the same feature type. Lakes, rivers or parks\n",
    "prior_feature = catalog_features.df_cat\n",
    "prior_feature['feature_type'] = 'l'\n",
    "\n",
    "# the prior and likelihood data from the region of interest\n",
    "all_data_of_interest = d[(d['date'] >= prior_dates['start']) & (d['date'] <= o_dates['end'])&(d.feature_type == 'l')].copy()\n",
    "all_data_of_interest = all_data_of_interest[all_data_of_interest.canton == 'Bern'].copy()\n",
    "\n",
    "# create a variable for different code group totals\n",
    "all_data_of_interest = all_data_of_interest[all_data_of_interest.code.isin([*indus, *tobo_snacks])].copy()\n",
    "\n",
    "all_data_of_interest_i = all_data_of_interest[all_data_of_interest.code.isin(indus)].copy()\n",
    "all_data_of_interest_i['use'] = 'pro'\n",
    "\n",
    "all_data_of_interest_p = all_data_of_interest[all_data_of_interest.code.isin(tobo_snacks)].copy()\n",
    "all_data_of_interest_p['use'] = 'pers'\n",
    "\n",
    "all_data_of_interest = pd.concat([all_data_of_interest_i, all_data_of_interest_p])\n",
    "\n",
    "all_data_of_interest.reset_index(inplace=True, drop=True)\n",
    "\n",
    "land_covers = ['buildings', 'forest', 'undefined', 'public-services', 'streets', 'orchards', 'use', 'canton', 'city', 'feature_name']\n",
    "\n",
    "all_report, all_land_use = make_report_objects(all_data_of_interest, info_columns = ['use', 'canton', 'city', 'feature_name'])\n",
    "\n",
    "\n",
    "args = {\n",
    "    'likelihood': {'canton':canton, 'date_range':o_dates},\n",
    "    'prior' : {'canton':canton, 'date_range':prior_dates},\n",
    "    'data' : all_data_of_interest.copy(),\n",
    "    'land-use-inventory' : prior_feature.copy()\n",
    "}\n",
    "\n",
    "\n",
    "combined_results = reports_and_forecast(args['likelihood'], args['prior'], ldata=args['data'])\n",
    "standard_combined = make_standard_report(combined_results, args)\n",
    "\n",
    "\n",
    "lake_report = combined_results['this_report']\n",
    "lake_prior_report = combined_results['prior_report']\n",
    "lake_land_use = combined_results['this_land_use']\n",
    "\n",
    "scaled_cols = ['public-services', 'buildings', 'forest', 'undefined', 'vineyards', 'orchards', 'streets', 'recreation']\n",
    "system_prompt = (\n",
    "    \"Transcribe the values from tables and put them in paragraph form.\"\n",
    "    \"Being carefull that each value in the table is accounted for in the\"\n",
    "    \"paragraph. You are to do this in a narrative form. Answers must be concise.\"\n",
    "      \n",
    "    \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6563eefe-af62-4e33-b224-f572da3c190b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aa617e9b-78a3-440a-b883-dba31121bf3c",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "scrolled": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [
    {
     "data": {
      "application/papermill.record/text/plain": "\"\\n\\n## Litter surveys in Switzerland 2020-2021 - IQAASL\\n\\nIdentification, quantification and analysis of anthropogenic Swiss litter (IQAASL) is a project commissioned by the Swiss \\nFederal Office for the Environment to collect data concerning visible pollutants along Swiss lakes and rivers. All \\ndiscarded materials were collected and identified using litter survey techniques. in total there were 406 samples from 163 \\nlocations in 95 municipalities.\\n\\nThis report is a summary and analysis of the litter surveys conducted and the methods employed in Switzerland from March\\n2020 through August 2021. This sampling phase overlaps with the Swiss Litter Report (SLR) survey period, which ran\\nfrom April 2017 to March 2018. The SLR was the first project on a national level to use the standard protocol described in \\nthe Guide to monitoring beach litter or any other comparable method. This overlap allows the results of the \\npresent study to be compared with those of the SLR.\\n\\n## Lakes and rivers\\n\\nThe lakes and rivers were sampled from 2020-03 through 2021-05, a total of 54,744 objects were removed and classified over \\nthe course of 386 surveys. The survey locations were divided into survey areas for regional analysis and defined by the Aare, \\nRhône, Ticino and Linth/Limmat rivers. Surveys were conducted at 143 different locations, representing 77 municipalities. \\nThe total linear distance surveyed was 20 km with a surface area of 9 hectares and a total municipal population of 1.7 million.\\n\\nMost surveys were along lake shorelines (331 samples) as lakes offer more consistent and safe year-round access with \\nrespect to rivers. Additionally, lakes are large areas of reduced flow that receive input from multiple rivers, streams \\nand drainage systems providing ideal locations to assess the variety of objects in and around the water bodies.\\n\\nIn total 316 samples came from seven principal lakes in 3 major river basins. Twenty locations were selected to sample \\nmonthly for a twelve-month period with the exception of Lago Maggiore, which was sampled every three months. \\nSurveys were also conducted on Lago di Lugano, Lac des Quatre cantons, Brienzersee and Zugersee. In addition, there \\nwere 55 surveys on 16 rivers.\\n\\n### The sampling locations - type and description\\n\\nThe land use is reported as the percent of total area attributed to each land use category within a 1500m radius of the \\nsurvey location. The ratio of the number of samples completed at the different land use profiles is an indicator of the \\nenvironmental and economic conditions around the survey locations.\\n\\nThe land use around the survey locations had a higher attribution to buildings as opposed to agriculture and woods. For \\nexample, half of all the surveys had at least 37% of land use devoted to buildings as opposed to 19% for agriculture or \\n13% to woods. Land use devoted to recreation was at least 6% for half of all samples.\\n\\nThe length of the road network within the buffer zone differentiates between locations that have other wise similar land \\nuse characteristics. The length of road per buffer ranges from 13km to 212km, 50% of the surveys had less than 67km of road network.\\n\\nThe number of intersections ranges from zero to 23, 50% of the surveys had 3 or fewer intersections within 1500m of the \\nsurvey location. The size of the intersecting river or canal was not taken into consideration. Survey locations on rivers \\nhave zero intersections.\\n\\nThe population (not shown) is taken from statpop 2018 and represents the population of the municipality surrounding the \\nsurvey location. The smallest population was 442 and the maximum was 415,367, 50% of the surveys come from \\nmunicipalities with a population of at least 12,812.\\n\\nOverall, surveys at locations with more buildings and more recreation sites were more likely to facilitate the accumulation \\nof trash on the shoreline. When the most common objects are considered, only four of the twelve were found at higher rates \\nin the presence of more buildings. All of those objects are likely related to food or tobacco consumption near the location. \\nSuggesting that there are still gains to be made in prevention and attenuation efforts in areas of high traffic near the water.\\n\\nHowever, six of the twelve objects have no positive association to land use attributed to buildings but were found in at \\nleast 50% of all the surveys. These objects are generally associated with professional use or in the case of cotton swabs \\npersonal hygiene:\\n\\n* plastic construction waste\\n* fragmented plastics\\n* industrial sheeting\\n* expanded polystyrene\\n* cotton bud/swabs\\n* insulation, includes spray foams\\n\\nFurthermore, compared to products related to tobacco or food consumption these objects have fewer positive associations in \\ngeneral. Indicating that the appropriate land use feature is not currently accounted for and/or these objects are found \\nat similar quantities indifferent of the land use features. Suggesting that these objects are ubiquitous in the environment.\\n\\nFinally, two of the twelve most common objects were found in less than 50% of the surveys and have few positive associations:\\n\\n* industrial pellets\\n* expanded foams < 5mm\\n\\nThese objects are found in large quantities sporadically at specific locations. They have been found in all survey areas \\nand in all lakes. Industrial pellets have a very specific use and client base making it possible to find partners based \\non the density of the pellets found and the location of the nearest consumer or producer of pellets, see Shared responsibility.\\n\\n### Median survey total\\n\\nThe results are in units of pieces of litter per 100 meters (p/100m). The median survey result of all data was approximately\\n189 p/100m. The maximum recorded value was 6,617 p/100m (Rhône survey area) and the minimum recorded was 2p/100m (Aare survey area).\\nThe Rhône survey area had the highest median survey total of 442p/100m, this can in part be explained by the high number\\nof urban survey locations with respect to the other survey areas and the deposition of fragmented plastics and foamed \\nplastics at the Rhône River out flow in the upper lake region.\\n\\nA reference value was calculated excluding the results from samples that were less than 10m and objects less than 2.5cm. \\nThis method, described in EU Marine Beach Litter Baselines was used to calculate the reference and threshold \\nvalues for all European beaches in 2015 and 2016 resulting in a median value of 131 p/100m. The results from the European \\nbaseline value lie outside the 95% confidence interval (CI) of 147 - 213p/100m established using the data from IQAASL.\\n\\nSurveys in Switzerland were on average, smaller scale than in marine environments and in locations that would be \\nconsidered urban under most circumstances. To date monitoring of lakes and rivers upstream of coastal regions has \\nnot generalized on the European continent. However, there is a concerted effort by a group of associations in \\nSwitzerland and France to establish a common monitoring and data exchange protocol for the Rhône basin. Additionally, \\nthe Wageningen University & Research has begun analyzing data collected in the Meusse - Rhine delta using \\nprotocols like those in IQAASL.\\n\\n### The most common objects\\n\\nThe most common objects are defined as those objects identified in at least 50% of all surveys and/or are among the ten \\nmost abundant by quantity. As a group the most common objects represent 68% of all objects identified in the sampling period. \\nOf the most common items 27% are food, drink and tobacco related and 24% are infrastructure and agriculture related.\\n\\nObjects related to food, drink and tobacco are identified at higher rates at survey locations with a greater percentage \\nof land attributed to buildings or fixed infrastructure, the inverse is true of the locations with a higher percentage \\nof land attributed to woods or agriculture. However, infrastructure material and fragmented plastics, are found at similar \\nrates throughout all survey areas indifferent of land use surrounding the survey locations.\\n\\nThe most common objects identified in the surveys were:\\n\\n* cigarette ends: total 8'485, % of all objects 15.5%, fail-rate 87%, p/100m 20\\n* fragmented plastics: total 7'400, 13% of all objects, fail-rate 86%, p/100m 18\\n* expanded polystyrene: total 5'563, 10% of all objects, fail-rate 68%, p/100m ,\\n* snack wrappers: total 3'325, 6% of all objects, fail-rate 85%, p/100m 9\\n* industrial sheeting: total 2'534, 4% of all objects, fail-rate 69%, p/100m 5\\n* glass drink bottles, pieces: total 2'136, 3% of all objects, fail-rate 65%, p/100m 3\\n* industrial pellets: total 1'968, 3% of all objects, fail-rate 30%, p/100m 4\\n* insulation, includes spray foams: total 1'702, 3% of all objects, fail-rate 53%, p/100m 1\\n* cotton bud/swabs: total 1'406, 2% of all objects, fail-rate 50%, p/100m 1\\n* expanded foams < 5mm: total 1'209, 2% of all objects, fail-rate 25%, p/100m 0\\n* plastic construction waste: total 992, 1% of all objects, fail-rate 52%, p/100m 1\\n* metal bottle caps: total 700, 1% of all objects, fail-rate 52%, p/100m 1\\n\\n\\n\\nIndustrial pellets and expanded foams < 5mm both occurred in significant quantities but identified in less than 50% of \\nthe surveys (median of 0), indicating high counts at specific locations. While both are micro plastics, their use, \\norigin and rate of occurrence are different depending on the survey area region. Industrial pellets are raw materials \\nused in injection molding processes whereas foamed plastic beads are the result of fragmentation of expanded polystyrene.\\n\\n### Conclusions\\n\\nAt the national level, the IQAASL results are stable compared to the surveys that were carried out in 2017 as part of the \\nSLR study. However, there was a general decrease in the quantity of food, drink and tobacco objects. Infrastructure \\nobjects and fragmented plastics and foams did not decline and some locations may have experienced sharp increases. \\nPandemic restrictions limiting large outdoor gatherings may have had a beneficial effect on the reduction of food, drink \\nand tobacco items. The greatest increases in infrastructure related objects were in Valais, Vaud and Brienz, which are \\nlocations near the Rhône and Aare rivers discharge points.\\n\\nThe land use around a survey location has a measurable effect on the deposition of certain objects. The more buildings \\nand fixed infrastructure there are the more tobacco and food products are found. Objects like fragmented plastics and \\nindustrial sheeting do not have the same association and are identified at approximately equal rates indifferent of the \\nland use with increases near river/canal discharge points.\\n\\nCurrently three of the four survey areas in the IQAASL are actively monitored by research and governmental agencies \\ndownstream of Switzerland using similar methods presented in this report. Additionally, regional associations in \\nSwitzerland are actively pursuing a standardization of reporting and protocols with partner organizations in the EU.\\n\\nThe IQAASL is a citizen-science project that only uses open-source tools and shares data on GNU public license, \\nenabling collaboration with stakeholders. At the end of the mandate, December 31, 2021, Hammerdirt will assume the \\nresponsibility of maintaining the code and data repository which is hosted publicly on Github.\\n\\nThe associations that participated in the IQAASL are actively seeking ways to incorporate the data collection process \\nand/or the results into their own business model. However, there is a shortage of data scientists within many regional \\nassociations which may lengthen the process of integration and stifle the rate of innovation at the level where it is needed most.\\n\""
     },
     "metadata": {
      "scrapbook": {
       "mime_prefix": "application/papermill.record/",
       "name": "context-r"
      }
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "context_r = f\"\"\"\n",
    "\n",
    "## Litter surveys in Switzerland 2020-2021 - IQAASL\n",
    "\n",
    "Identification, quantification and analysis of anthropogenic Swiss litter (IQAASL) is a project commissioned by the Swiss \n",
    "Federal Office for the Environment to collect data concerning visible pollutants along Swiss lakes and rivers. All \n",
    "discarded materials were collected and identified using litter survey techniques. in total there were 406 samples from 163 \n",
    "locations in 95 municipalities.\n",
    "\n",
    "This report is a summary and analysis of the litter surveys conducted and the methods employed in Switzerland from March\n",
    "2020 through August 2021. This sampling phase overlaps with the Swiss Litter Report (SLR) survey period, which ran\n",
    "from April 2017 to March 2018. The SLR was the first project on a national level to use the standard protocol described in \n",
    "the Guide to monitoring beach litter or any other comparable method. This overlap allows the results of the \n",
    "present study to be compared with those of the SLR.\n",
    "\n",
    "## Lakes and rivers\n",
    "\n",
    "The lakes and rivers were sampled from 2020-03 through 2021-05, a total of 54,744 objects were removed and classified over \n",
    "the course of 386 surveys. The survey locations were divided into survey areas for regional analysis and defined by the Aare, \n",
    "Rhône, Ticino and Linth/Limmat rivers. Surveys were conducted at 143 different locations, representing 77 municipalities. \n",
    "The total linear distance surveyed was 20 km with a surface area of 9 hectares and a total municipal population of 1.7 million.\n",
    "\n",
    "Most surveys were along lake shorelines (331 samples) as lakes offer more consistent and safe year-round access with \n",
    "respect to rivers. Additionally, lakes are large areas of reduced flow that receive input from multiple rivers, streams \n",
    "and drainage systems providing ideal locations to assess the variety of objects in and around the water bodies.\n",
    "\n",
    "In total 316 samples came from seven principal lakes in 3 major river basins. Twenty locations were selected to sample \n",
    "monthly for a twelve-month period with the exception of Lago Maggiore, which was sampled every three months. \n",
    "Surveys were also conducted on Lago di Lugano, Lac des Quatre cantons, Brienzersee and Zugersee. In addition, there \n",
    "were 55 surveys on 16 rivers.\n",
    "\n",
    "### The sampling locations - type and description\n",
    "\n",
    "The land use is reported as the percent of total area attributed to each land use category within a 1500m radius of the \n",
    "survey location. The ratio of the number of samples completed at the different land use profiles is an indicator of the \n",
    "environmental and economic conditions around the survey locations.\n",
    "\n",
    "The land use around the survey locations had a higher attribution to buildings as opposed to agriculture and woods. For \n",
    "example, half of all the surveys had at least 37% of land use devoted to buildings as opposed to 19% for agriculture or \n",
    "13% to woods. Land use devoted to recreation was at least 6% for half of all samples.\n",
    "\n",
    "The length of the road network within the buffer zone differentiates between locations that have other wise similar land \n",
    "use characteristics. The length of road per buffer ranges from 13km to 212km, 50% of the surveys had less than 67km of road network.\n",
    "\n",
    "The number of intersections ranges from zero to 23, 50% of the surveys had 3 or fewer intersections within 1500m of the \n",
    "survey location. The size of the intersecting river or canal was not taken into consideration. Survey locations on rivers \n",
    "have zero intersections.\n",
    "\n",
    "The population (not shown) is taken from statpop 2018 and represents the population of the municipality surrounding the \n",
    "survey location. The smallest population was 442 and the maximum was 415,367, 50% of the surveys come from \n",
    "municipalities with a population of at least 12,812.\n",
    "\n",
    "Overall, surveys at locations with more buildings and more recreation sites were more likely to facilitate the accumulation \n",
    "of trash on the shoreline. When the most common objects are considered, only four of the twelve were found at higher rates \n",
    "in the presence of more buildings. All of those objects are likely related to food or tobacco consumption near the location. \n",
    "Suggesting that there are still gains to be made in prevention and attenuation efforts in areas of high traffic near the water.\n",
    "\n",
    "However, six of the twelve objects have no positive association to land use attributed to buildings but were found in at \n",
    "least 50% of all the surveys. These objects are generally associated with professional use or in the case of cotton swabs \n",
    "personal hygiene:\n",
    "\n",
    "* plastic construction waste\n",
    "* fragmented plastics\n",
    "* industrial sheeting\n",
    "* expanded polystyrene\n",
    "* cotton bud/swabs\n",
    "* insulation, includes spray foams\n",
    "\n",
    "Furthermore, compared to products related to tobacco or food consumption these objects have fewer positive associations in \n",
    "general. Indicating that the appropriate land use feature is not currently accounted for and/or these objects are found \n",
    "at similar quantities indifferent of the land use features. Suggesting that these objects are ubiquitous in the environment.\n",
    "\n",
    "Finally, two of the twelve most common objects were found in less than 50% of the surveys and have few positive associations:\n",
    "\n",
    "* industrial pellets\n",
    "* expanded foams < 5mm\n",
    "\n",
    "These objects are found in large quantities sporadically at specific locations. They have been found in all survey areas \n",
    "and in all lakes. Industrial pellets have a very specific use and client base making it possible to find partners based \n",
    "on the density of the pellets found and the location of the nearest consumer or producer of pellets, see Shared responsibility.\n",
    "\n",
    "### Median survey total\n",
    "\n",
    "The results are in units of pieces of litter per 100 meters (p/100m). The median survey result of all data was approximately\n",
    "189 p/100m. The maximum recorded value was 6,617 p/100m (Rhône survey area) and the minimum recorded was 2p/100m (Aare survey area).\n",
    "The Rhône survey area had the highest median survey total of 442p/100m, this can in part be explained by the high number\n",
    "of urban survey locations with respect to the other survey areas and the deposition of fragmented plastics and foamed \n",
    "plastics at the Rhône River out flow in the upper lake region.\n",
    "\n",
    "A reference value was calculated excluding the results from samples that were less than 10m and objects less than 2.5cm. \n",
    "This method, described in EU Marine Beach Litter Baselines was used to calculate the reference and threshold \n",
    "values for all European beaches in 2015 and 2016 resulting in a median value of 131 p/100m. The results from the European \n",
    "baseline value lie outside the 95% confidence interval (CI) of 147 - 213p/100m established using the data from IQAASL.\n",
    "\n",
    "Surveys in Switzerland were on average, smaller scale than in marine environments and in locations that would be \n",
    "considered urban under most circumstances. To date monitoring of lakes and rivers upstream of coastal regions has \n",
    "not generalized on the European continent. However, there is a concerted effort by a group of associations in \n",
    "Switzerland and France to establish a common monitoring and data exchange protocol for the Rhône basin. Additionally, \n",
    "the Wageningen University & Research has begun analyzing data collected in the Meusse - Rhine delta using \n",
    "protocols like those in IQAASL.\n",
    "\n",
    "### The most common objects\n",
    "\n",
    "The most common objects are defined as those objects identified in at least 50% of all surveys and/or are among the ten \n",
    "most abundant by quantity. As a group the most common objects represent 68% of all objects identified in the sampling period. \n",
    "Of the most common items 27% are food, drink and tobacco related and 24% are infrastructure and agriculture related.\n",
    "\n",
    "Objects related to food, drink and tobacco are identified at higher rates at survey locations with a greater percentage \n",
    "of land attributed to buildings or fixed infrastructure, the inverse is true of the locations with a higher percentage \n",
    "of land attributed to woods or agriculture. However, infrastructure material and fragmented plastics, are found at similar \n",
    "rates throughout all survey areas indifferent of land use surrounding the survey locations.\n",
    "\n",
    "The most common objects identified in the surveys were:\n",
    "\n",
    "* cigarette ends: total 8'485, % of all objects 15.5%, fail-rate 87%, p/100m 20\n",
    "* fragmented plastics: total 7'400, 13% of all objects, fail-rate 86%, p/100m 18\n",
    "* expanded polystyrene: total 5'563, 10% of all objects, fail-rate 68%, p/100m ,\n",
    "* snack wrappers: total 3'325, 6% of all objects, fail-rate 85%, p/100m 9\n",
    "* industrial sheeting: total 2'534, 4% of all objects, fail-rate 69%, p/100m 5\n",
    "* glass drink bottles, pieces: total 2'136, 3% of all objects, fail-rate 65%, p/100m 3\n",
    "* industrial pellets: total 1'968, 3% of all objects, fail-rate 30%, p/100m 4\n",
    "* insulation, includes spray foams: total 1'702, 3% of all objects, fail-rate 53%, p/100m 1\n",
    "* cotton bud/swabs: total 1'406, 2% of all objects, fail-rate 50%, p/100m 1\n",
    "* expanded foams < 5mm: total 1'209, 2% of all objects, fail-rate 25%, p/100m 0\n",
    "* plastic construction waste: total 992, 1% of all objects, fail-rate 52%, p/100m 1\n",
    "* metal bottle caps: total 700, 1% of all objects, fail-rate 52%, p/100m 1\n",
    "\n",
    "\n",
    "\n",
    "Industrial pellets and expanded foams < 5mm both occurred in significant quantities but identified in less than 50% of \n",
    "the surveys (median of 0), indicating high counts at specific locations. While both are micro plastics, their use, \n",
    "origin and rate of occurrence are different depending on the survey area region. Industrial pellets are raw materials \n",
    "used in injection molding processes whereas foamed plastic beads are the result of fragmentation of expanded polystyrene.\n",
    "\n",
    "### Conclusions\n",
    "\n",
    "At the national level, the IQAASL results are stable compared to the surveys that were carried out in 2017 as part of the \n",
    "SLR study. However, there was a general decrease in the quantity of food, drink and tobacco objects. Infrastructure \n",
    "objects and fragmented plastics and foams did not decline and some locations may have experienced sharp increases. \n",
    "Pandemic restrictions limiting large outdoor gatherings may have had a beneficial effect on the reduction of food, drink \n",
    "and tobacco items. The greatest increases in infrastructure related objects were in Valais, Vaud and Brienz, which are \n",
    "locations near the Rhône and Aare rivers discharge points.\n",
    "\n",
    "The land use around a survey location has a measurable effect on the deposition of certain objects. The more buildings \n",
    "and fixed infrastructure there are the more tobacco and food products are found. Objects like fragmented plastics and \n",
    "industrial sheeting do not have the same association and are identified at approximately equal rates indifferent of the \n",
    "land use with increases near river/canal discharge points.\n",
    "\n",
    "Currently three of the four survey areas in the IQAASL are actively monitored by research and governmental agencies \n",
    "downstream of Switzerland using similar methods presented in this report. Additionally, regional associations in \n",
    "Switzerland are actively pursuing a standardization of reporting and protocols with partner organizations in the EU.\n",
    "\n",
    "The IQAASL is a citizen-science project that only uses open-source tools and shares data on GNU public license, \n",
    "enabling collaboration with stakeholders. At the end of the mandate, December 31, 2021, Hammerdirt will assume the \n",
    "responsibility of maintaining the code and data repository which is hosted publicly on Github.\n",
    "\n",
    "The associations that participated in the IQAASL are actively seeking ways to incorporate the data collection process \n",
    "and/or the results into their own business model. However, there is a shortage of data scientists within many regional \n",
    "associations which may lengthen the process of integration and stifle the rate of innovation at the level where it is needed most.\n",
    "\"\"\"\n",
    "\n",
    "glue('context-r', context_r, display=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d40e4eb4-86ee-4228-9b39-e38a1e576db3",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "## Context: Litter surveys in Switzerland 2020-2021 - IQAASL\n",
    "\n",
    "We provide one document for the context here. It is indexed with \n",
    "\n",
    "Identification, quantification and analysis of anthropogenic Swiss litter (IQAASL) is a project commissioned by the Swiss \n",
    "Federal Office for the Environment to collect data concerning visible pollutants along Swiss lakes and rivers. All \n",
    "discarded materials were collected and identified using litter survey techniques. in total there were 406 samples from 163 \n",
    "locations in 95 municipalities.\n",
    "\n",
    "This report is a summary and analysis of the litter surveys conducted and the methods employed in Switzerland from March\n",
    "2020 through August 2021. This sampling phase overlaps with the Swiss Litter Report (SLR) survey period, which ran\n",
    "from April 2017 to March 2018. The SLR was the first project on a national level to use the standard protocol described in \n",
    "the Guide to monitoring beach litter or any other comparable method. This overlap allows the results of the \n",
    "present study to be compared with those of the SLR.\n",
    "\n",
    ":::{dropdown} See the rest of the context document\n",
    "\n",
    "```{glue:md} contex-r\n",
    ":format: myst\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "83b1b15f-583c-40fc-a8c3-08ad51d1f276",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "outputs": [],
   "source": [
    "indus_code_defs = codes.loc[indus, 'en']\n",
    "pro_codes = (', ').join(indus_code_defs.values)\n",
    "\n",
    "rec_code_defs = codes.loc[tobo_snacks, 'en']\n",
    "rec_codes = (', ').join(rec_code_defs.values)\n",
    "\n",
    "use_groups = {'personal':rec_codes, 'professional':pro_codes}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f5049215-c6d7-4809-b130-cb8c38a8cfaf",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "file saved as report_results.md\n"
     ]
    }
   ],
   "source": [
    "def a_model_feature_importance_prompt(table):\n",
    "    feature_importance_prompt = f\"\"\"\n",
    "The following table details the model feature importance. \n",
    "\n",
    "Table has the following format:\n",
    "\n",
    "1. Feature: the name of the land-use feature\n",
    "2. importance: The model feature importance\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each row without any comments or analysis:\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return feature_importance_prompt\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def a_permutation_feature_importance_prompt(table):\n",
    "    feature_importance_prompt = f\"\"\"\n",
    "The following table details the permutation feature importance. \n",
    "\n",
    "Table has the following format:\n",
    "\n",
    "1. Feature: the name of the land-use feature\n",
    "2. importance: The model feature importance\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each row without any comments or analysis:\n",
    "   \n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return feature_importance_prompt\n",
    "\n",
    "def a_forecast_prompt(table):\n",
    "    forecast_prompt = f\"\"\"\n",
    "The following contains the expected distribution of survey results.\n",
    "\n",
    "The table has the following format:\n",
    "\n",
    "1. average: the expected average sample total\n",
    "2. hdi min: the minimum of the 90% Highest Density Interval\n",
    "3. hdi max: the maximum of the 90% of the Highest Density Interval\n",
    "4. 5th, 25th, 50th, 75th, 95th : the percentile rankings based on the expected distribution\n",
    "5. max predicted: the maximum value predicted by the model\n",
    "\n",
    "Generate a narrative summary based on the following table. Include all values. Reply in paragraph format, do not comment do not embelish. Use the following style guide:\n",
    "    \n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return forecast_prompt\n",
    "\n",
    "def admin_prompt(table, place_names):\n",
    "    prompt  = f\"\"\"\n",
    "The following table details the number of survey locations, cities, cantons and survey areas present in the data under analysis. \n",
    "\n",
    "Please provide a concise narrative of the contents of the following table. In your narrative be sure to include the list of cities, \n",
    "and the names of the canton and survey areas.\n",
    "\n",
    " {table}\n",
    "\n",
    "The following is the names of the cities, cantons, and survey areas.\n",
    "\n",
    "{place_names}    \n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def feature_count_prompt(table, place_names):\n",
    "    prompt = f\"\"\"\n",
    "The following table details the number and the name of the lakes, rivers and parks in the survey data under analysis. \n",
    "\n",
    "Please provide a concise narrative of the contents of the following table. In your narrative be sure to the name of each park, lake or river\n",
    "that is mentioned.\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "The following is the names of the lakes, rivers and parks included in the data.\n",
    "\n",
    "{place_names}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "def survey_result_summary_prompt(table):\n",
    "    \n",
    "    combined_summary_prompt  = f\"\"\"\n",
    "These are the survey totals for the data we are studying. We are analyzing count data from beach-litter surveys. The table has the following format:\n",
    "\n",
    "1. total (quantity) = the total number of objects identified\n",
    "2. nsamples = the numner of samples collected\n",
    "3. average = average objects per meter\\n\n",
    "4. 5th, 25th, 50th,\t75th, 95th = the objects per meter percentile ranking\n",
    "5. std = standard deviation in objects per meter\n",
    "6. max = the maximum recorded objects per meter\n",
    "7. start = the date of the first sample\n",
    "8. end = the date of the las sample\n",
    "\n",
    "Generate a narrative summary based on the following table.\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return combined_summary_prompt\n",
    "\n",
    "def inventory_prompt(table):    \n",
    "    inventory_prompt  = f\"\"\"\n",
    "This is the list of all objects found at the beach. The table has the following format:   \n",
    "\n",
    "1. code: object identifier\n",
    "1. quantity: the total number found\n",
    "2. pcs/m = average objects per meter\n",
    "3. % of total = the proportion of the total for for this object\n",
    "4. sample_id = the number of samples\n",
    "5. fails = the number of times at least one of the object was found at a survey\n",
    "6. rate =  fails/the number of samples\n",
    "7. object: the plain english name of the object type\n",
    "\n",
    "Generate a narrative summary based on the following table. You need to list all the codes starting from the top and working down that make up at least 50% of the total.\n",
    "Provide the code quantity and % of total.\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return inventory_prompt\n",
    "\n",
    "def landuse_profile_prompt(table):\n",
    "    \n",
    "    profile_prompt  = f\"\"\"\n",
    "\n",
    "The following table describes the distribution of samples according to the proportion of the buffer area (index) attributed to a topographical feature (columns) for example if\n",
    "if the index is 1 (0-20%) and the forest column is .1 that means that 10% of the samples took place in locations where 0-20% of the buffer was attributed to forest.\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each column along with their respective index values without any comments or analysis:\n",
    "\n",
    "The table has the following format:\n",
    "\n",
    "1. Index = proportion of buffer occupied by feature: (1 - 5) or (0-20%, 20-40%, 40-60%, 60-80%, 80-100%)\n",
    "2. Columns = The named topographical feature that maybe in the buffer\n",
    "3. Values = The proportion of all the samples that were conducted at the magnitude and feature.\n",
    "\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return profile_prompt\n",
    "\n",
    "def landuse_rates_prompt(table):    \n",
    "\n",
    "    rates_prompt  = f\"\"\"    \n",
    "The table has the following format:\n",
    "\n",
    "1. Index = proportion of buffer occupied by feature: (1 - 5) or (0-20%, 20-40%, 40-60%, 60-80%, 80-100%)\n",
    "2. Columns = The named topographical feature that maybe in the buffer\n",
    "3. Values = the average pieces of trash per meter that was observed at the magnitude and feature\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each column along with their respective index values without any comments or analysis:\n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return rates_prompt\n",
    "\n",
    "def cluster_composition_prompt(table):    \n",
    "\n",
    "    cluster_prompt = f\"\"\"    \n",
    "The following are the summary results of a cluster analysis. The columns are the features that were used to make the clusters. The optimal number of clusters was\n",
    "determined using the elbow method (you can check the docs for this: https://hammerdirt-analyst.github.io/feb_2024/titlepage.html). The table displays the average magnitude\n",
    "of each feature in the cluster. For example if the value for forest, cluster 1 = .45 then that means that in cluster 1, the average sample was taken from a location that was\n",
    "45% dedicated to forest.\n",
    "\n",
    "Table has the following format:\n",
    "\n",
    "1. the columns are the measured land use features\n",
    "2. the index is the cluster number\n",
    "3. the value is the proportion of the cluster that is attributed to that column. For example if buildings in cluster 1 = .17 it means that the average magnitude of\n",
    "the buildings variable was 0.17 in cluster 1.\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each column along with their cluster number values without any comments or analysis:\n",
    "   \n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return cluster_prompt\n",
    "\n",
    "def cluster_rates_prompt(table):\n",
    "    \n",
    "    cluster_rates = f\"\"\"    \n",
    "The following are the observed sample average per cluster. The units is objects per meter of beach. The columns are the use case of the objects: personal or professional. The index is\n",
    "the cluster number.\n",
    "\n",
    "Table has the following format:\n",
    "\n",
    "1. the columns are the object use case\n",
    "2. the index is the cluster number\n",
    "3. the value is the objects found per meter of beach\n",
    "\n",
    "Convert the following table into a paragraph, reporting the values for each column along with their respective cluster values without any comments or analysis:\n",
    "The narrative needs to be in paragraph format.\n",
    "       \n",
    "{table}   \n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return cluster_rates\n",
    "\n",
    "def regression_results_prompt(table):\n",
    "    \n",
    "    prompt = f\"\"\"\n",
    "    \n",
    "The following table details the results from different regression analysis of our data. \n",
    "\n",
    "Table has the following format:\n",
    "\n",
    "1. Model: the type of regression model used\n",
    "2. R²: The coefficient of determination\n",
    "3. MSE: the mean squared error\n",
    "\n",
    "Generate a narrative summary based on the following table. You need to include all the models and the R² and MSE result.\n",
    "The narrative needs to be in paragraph format.\n",
    "   \n",
    "\n",
    "{table}\n",
    "\n",
    "\n",
    "\"\"\"\n",
    "    return prompt\n",
    "\n",
    "system_prompt = (\n",
    "    \"Transcribe the values from tables and put them in paragraph form.\"\n",
    "    \"Being carefull that each value in the table is accounted for in the\"\n",
    "    \"paragraph. You are to do this in a narrative form. Answers must be concise.\"\n",
    "     \"\\n\\n\"\n",
    "    \"{context}\"\n",
    ")\n",
    "\n",
    "\n",
    "model = 'gpt-4o-mini'\n",
    "class ReportTexts:\n",
    "    def __init__(self, name: str, start: str, end: str, groups: {}, standard_report: {}, report: {}, landuse_report: {}, client: callable = None):\n",
    "        self.name = name\n",
    "        self.start = start\n",
    "        self.end = end\n",
    "        self.groups = groups\n",
    "        self.report = report\n",
    "        self.landuse_report = landuse_report\n",
    "        self.client = client\n",
    "        self.standard_report = standard_report\n",
    "        self.chat = False\n",
    "        self.cluster_d = None\n",
    "\n",
    "    def  the_admin_boundaries(self, system_prompt: str = None, user_prompt: str = None):\n",
    "        d = self.report.administrative_boundaries()[0]\n",
    "        d.loc['survey areas', 'count'] = d.loc['parent_boundary', 'count']\n",
    "        d.drop('parent_boundary', inplace=True)\n",
    "\n",
    "        d_names = self.report.administrative_boundaries()[1]\n",
    "        d_names['survey_area'] = d_names['parent_boundary']\n",
    "        d_names.pop('parent_boundary')\n",
    "        report_label = f\"\\n## Administrative boundaries {self.name} {self.start} {self.end}\\n\\n__{self.name}: Political boundaries and survey locations__\\n\\n\"\n",
    "\n",
    "        if self.chat is True:\n",
    "            user_prompt = admin_prompt(d.to_markdown(), d_names)\n",
    "            messages = messages_for_chat_completion(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = admin_prompt(d.to_markdown(), d_names)\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def the_named_features(self):\n",
    "        d = self.report.feature_inventory()[0]\n",
    "        d_names = self.report.feature_inventory()[1]\n",
    "        report_label = f\"\\n## Named features {self.name} {self.start} {self.end}\\n\\n__{self.name}: The number and place names of lakes, rivers and parks__\\n\\n\"\n",
    "\n",
    "        if self.chat is True:\n",
    "            user_prompt = feature_count_prompt(d.to_markdown(), d_names)\n",
    "            messages = messages_for_chat_completion(system_prompt=system_prompt, user_prompt=user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = feature_count_prompt(d.to_markdown(), d_names)\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def summary_statistics(self):\n",
    "        d = self.report.sampling_results_summary.T\n",
    "        report_label = f\"\\n## Summary statistics {self.name} {self.start} {self.end}\\n\\n__{self.name}: The distribution of the sample totals__\\n\\n\"\n",
    "\n",
    "        if self.chat is True:\n",
    "            user_prompt = survey_result_summary_prompt(d.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = survey_result_summary_prompt(d.to_markdown())\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def inventory(self):\n",
    "        d = self.report.object_summary()\n",
    "        d['object'] = d.index.map(lambda x: codes.loc[x, 'en'])\n",
    "        report_label = f\"\\n## Inventory items {self.name} {self.start} {self.end}\\n\\n__{self.name}: The quantity, average density, % of total and fail rate per object__\\n\\n\"\n",
    "\n",
    "        if self.chat is True:\n",
    "            user_prompt = inventory_prompt(d.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = inventory_prompt(d.to_markdown())\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def landuse_profile(self):\n",
    "        d = self.landuse_report.n_samples_per_feature()/self.report.number_of_samples\n",
    "        d.sort_index(inplace=True)\n",
    "        report_label = f\"\\n## Land use profile {self.name} {self.start} {self.end}\\n\\n__{self.name}: The landuse profile of the surveys.__\\n\\n\"\n",
    "\n",
    "        if self.chat is True:\n",
    "            user_prompt = landuse_profile_prompt(d.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = landuse_profile_prompt(d.to_markdown())\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def landuse_rates(self):\n",
    "        d = self.landuse_report.rate_per_feature()\n",
    "        report_label = f\"\\n## Land use and trash density {self.name} {self.start} {self.end}\\n\\n{self.name}: The density of trash by feature and proportion of buffer.__\\n\\n\"\n",
    "        \n",
    "        if self.chat is True:\n",
    "            user_prompt = landuse_rates_prompt(d.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = land_use_rates_prompt(d.to_markdown())\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def cluster_analysis(self, scaled_cols: [] = None):\n",
    "        \n",
    "        report_label_cluster_features = f\"\\n__{self.name}: Cluster composition__\"\n",
    "        report_label_cluster_averages = f\"\\n__{self.name}: Average density per cluster__\"         \n",
    "                                            \n",
    "        cluster_d, filtered_columns = filter_features(self.landuse_report.df_cont.copy(), terms=scaled_cols)        \n",
    "                            \n",
    "        self.target_scaler = StandardScaler()\n",
    "        self.feature_scaler = StandardScaler()\n",
    "        self.street_scaler = StandardScaler()\n",
    "                      \n",
    "        cluster_d['pcs/m'] = self.target_scaler.fit_transform(cluster_d[['pcs/m']])\n",
    "        cluster_d['streets'] = self.street_scaler.fit_transform(cluster_d[['streets']])\n",
    "        cluster_d[filtered_columns] = self.feature_scaler.fit_transform(cluster_d[filtered_columns])\n",
    "        self.cluster_d = cluster_d\n",
    "        self.filtered_columns = filtered_columns        \n",
    "\n",
    "        args = {\n",
    "            'cluster_data': cluster_d,\n",
    "            'columns_to_cluster':self.filtered_columns,\n",
    "            'scaled_cols': self.filtered_columns,\n",
    "            'scalers':{'street_scaler':self.street_scaler, 'target_scaler': self.target_scaler, 'feature_scaler':self.feature_scaler}\n",
    "        }           \n",
    "\n",
    "        cluster_pro, summary_pro = clusters_by_use_case(**args)              \n",
    "\n",
    "        # if use is None:\n",
    "        summary_pro.drop(['nsamps'], inplace=True, axis=1)\n",
    "        cols = [x for x in summary_pro.columns if x not in ['pcs/m']]\n",
    "        cluster_features = summary_pro[cols].drop_duplicates()\n",
    "        cluster_results = summary_pro[['pcs/m']].copy()            \n",
    "        \n",
    "        \n",
    "        if self.chat:\n",
    "            # cluster composition\n",
    "            user_prompt = cluster_composition_prompt(cluster_features.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_comp = use_chat_completion(client, model, messages)\n",
    "    \n",
    "            # average rate per cluster\n",
    "            user_prompt = cluster_rates_prompt(cluster_results.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_rate = use_chat_completion(client, model, messages)\n",
    "            self.cluster_comp = completed_chat_comp\n",
    "            self.cluster_results = completed_chat_rate\n",
    "            return report_label_cluster_features, completed_chat_comp, cluster_features, report_label_cluster_averages, completed_chat_rate\n",
    "        else:\n",
    "            user_prompt_f = cluster_composition_prompt(cluster_features.to_markdown())\n",
    "            user_prompt_r = cluster_rates_prompt(cluster_results.to_markdown())\n",
    "            \n",
    "            return f\"\\n## Cluster analysis {self.name} {self.start} {self.end}\\n\\n{report_label_cluster_features}\\n{user_prompt_f}\\n\\n{report_label_cluster_averages}\\n{user_prompt_r }\"\n",
    "        \n",
    "    \n",
    "    def linear_and_ensemble_regression(self):\n",
    "        \n",
    "        d, best_model, the_name, predictions, X_test, y_test, X_train, y_train = perform_regression_analysis(self.cluster_d, features=self.filtered_columns)\n",
    "        d = pd.DataFrame(d)\n",
    "        report_label = f\"\\n## Summary of regression results {self.name} {self.start} {self.end}\\n\\n{self.name}: The density of trash by feature and proportion of buffer.__\\n\\n\"\n",
    "        \n",
    "        \n",
    "        self.best_model = best_model\n",
    "        self.best_model_name = the_name\n",
    "        self.predictions = predictions\n",
    "        self.x_train = X_train\n",
    "        self.x_test = X_test\n",
    "        self.y_train = y_train\n",
    "        self.y_test =  y_test\n",
    "        if self.chat:\n",
    "            user_prompt = regression_results_prompt(d.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat = use_chat_completion(client, model, messages)\n",
    "            return d, completed_chat, report_label\n",
    "        else:\n",
    "            user_prompt = regression_results_prompt(d.to_markdown())\n",
    "            return f'{report_label}\\n\\n{user_prompt}'\n",
    "\n",
    "    def feature_importance(self):\n",
    "        \n",
    "\n",
    "        d1, d2 = evaluate_feature_importance(self.best_model, self.best_model_name, self.x_test, self.y_test, self.x_train, self.y_train)\n",
    "        report_label_model_f = f\"\\n__Model feature importance__\"\n",
    "        report_label_model_p = f\"\\n__Permutation feature importance__\"\n",
    "       \n",
    "        if self.chat:\n",
    "            user_prompt = a_model_feature_importance_prompt(d1.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_model_features = use_chat_completion(client, model, messages)\n",
    "\n",
    "            user_prompt = a_model_feature_importance_prompt(d2.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_model_permutation = use_chat_completion(client, model, messages)\n",
    "            return report_label_model_f,  completed_chat_model_features, d1, report_label_model_p, completed_chat_model_permutation, d2\n",
    "        else:\n",
    "            user_prompt_f = a_model_feature_importance_prompt(d1.to_markdown())\n",
    "            user_prompt_p = a_model_feature_importance_prompt(d2.to_markdown())\n",
    "            \n",
    "            return f\"{report_label_model_f}\\n{user_prompt_f}\\n\\n{report_label_model_p}\\n{user_prompt_p}\"\n",
    "\n",
    "       \n",
    "    def grid_approximation(self):\n",
    "        \n",
    "        d1 = self.standard_report['weighted-forecast'].copy()\n",
    "        d2 = self.standard_report['observed-99-forecast'].copy()\n",
    "        report_label_model_f = f\"\\n__{self.name}: Weighted prior forecast__\"\n",
    "        report_label_model_p = f\"\\n__{self.name}: Observed 99th percentile forecast__\"\n",
    "       \n",
    "        if self.chat:\n",
    "            user_prompt = a_forecast_prompt(d1)\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_weighted = use_chat_completion(client, model, messages)\n",
    "\n",
    "            user_prompt = a_forecast_prompt(d2.to_markdown())\n",
    "            messages = messages_for_chat_completion(system_prompt, user_prompt)\n",
    "            completed_chat_99 = use_chat_completion(client, model, messages)\n",
    "            return report_label_model_f, completed_chat_weighted, d1, report_label_model_p, completed_chat_99, d2\n",
    "        else:\n",
    "            user_prompt_f = a_forecast_prompt(d1.to_markdown())\n",
    "            user_prompt_p = a_forecast_prompt(d2.to_markdown())\n",
    "            return f\"\\n## Forecasts__\\n\\n{report_label_model_f}__\\n{user_prompt_f}\\n\\n__{report_label_model_p}__\\n{user_prompt_p}\"\n",
    "    \n",
    "    \n",
    "    \n",
    "    def chat_rep(self, scaled_cols, file_name):        \n",
    "    \n",
    "        title = f\"\\n# Survey report {self.name} {self.start} {self.end}\\n\\n\"\n",
    "        objects = f\"\\n__Objects in data__\\n\\n{', '.join([x for x in self.groups.values()])}\\n\\n\"\n",
    "        with open(file_name, 'w') as file:\n",
    "            file.write(title)\n",
    "\n",
    "        append_to_markdown(file_name, objects)\n",
    "        \n",
    "        a, b, c = self.the_admin_boundaries(system_prompt=system_prompt)\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "        \n",
    "        a, b, c = self.the_named_features()\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "    \n",
    "        a, b, c = self.summary_statistics()\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "    \n",
    "        a, b, c = self.inventory()\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "\n",
    "        a, b, c = self.landuse_profile()\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "\n",
    "        a, b, c = self.landuse_rates()\n",
    "        entry = f'{c}{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "        \n",
    "        a, b, c, d, e = self.cluster_analysis(scaled_cols=scaled_cols)\n",
    "        section = f\"\\n## Cluster analysis {self.name} {self.start} {self.end}\\n\\n\"\n",
    "        entry = f'{section}{a}\\n\\n{b.choices[0].message.content}\\n\\n{c.to_markdown()}\\n\\n{d}\\n\\n{e.choices[0].message.content}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "    \n",
    "        a, b, c = self.linear_and_ensemble_regression()\n",
    "        entry = f'{c}\\n\\n{b.choices[0].message.content}\\n\\n{a.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "    \n",
    "        a, b, c, d, e, f = self.feature_importance()\n",
    "        section = f\"\\n## Feature and permutation importance {self.name} {self.start} {self.end}\\n\\n\"\n",
    "        entry = f'{section}{a}\\n\\n{b.choices[0].message.content}\\n\\n{c.to_markdown()}\\n\\n{d}\\n\\n{e.choices[0].message.content}\\n\\n{f.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "    \n",
    "        a, b, c, d, e, f = self.grid_approximation()\n",
    "        section = f\"\\n## Forecasts {self.name} {self.start} {self.end}\\n\\n\"\n",
    "        entry = f'{section}{a}\\n\\n{b.choices[0].message.content}\\n\\n{c.to_markdown()}\\n\\n{d}\\n\\n{e.choices[0].message.content}\\n\\n{f.to_markdown()}'\n",
    "        append_to_markdown(file_name, entry + '\\n\\n')\n",
    "        \n",
    "        return print(f\"file saved as {file_name}\") \n",
    "\n",
    "    def string_rep(self, scaled_cols: [] = None):\n",
    "        title = f\"\\n# Survey report {self.name} {self.start} {self.end}\\n\\n\"\n",
    "        objects = f\"\\n__Objects in data__\\n\\n{', '.join([x for x in self.groups.values()])}\\n\\n\"\n",
    "        admin_boundaries = self.the_admin_boundaries()\n",
    "        feature_names = self.the_named_features()\n",
    "        summary_statistics = self.summary_statistics()\n",
    "        inventory = self.inventory()\n",
    "        clusteranalysis= self.cluster_analysis(scaled_cols=scaled_cols)\n",
    "        linear_ensemble = self.linear_and_ensemble_regression()\n",
    "        forecasts = self.grid_approximation()\n",
    "        astring = f\"\"\"\n",
    "        {title}\n",
    "        {objects}\n",
    "        {admin_boundaries}\n",
    "        {feature_names}\n",
    "        {summary_statistics}\n",
    "        {inventory}\n",
    "        {clusteranalysis}\n",
    "        {linear_ensemble}\n",
    "        {forecasts}\n",
    "        \"\"\"\n",
    "        return astring\n",
    "\n",
    "    def __repr__(self):\n",
    "        return self.string_rep(scaled_cols=scaled_cols)\n",
    "        \n",
    "client = openai.OpenAI()\n",
    "args = {\n",
    "    'report':lake_report,\n",
    "    'landuse_report': lake_land_use,\n",
    "    'client': client,\n",
    "    'start': prior_dates['start'],\n",
    "    'end': o_dates['end'],\n",
    "    'groups': use_groups,\n",
    "    'standard_report': standard_combined,\n",
    "    'name': canton\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "achatter = ReportTexts(**args)\n",
    "# title = f\"\\n# Survey report {achatter.name} {achatter.start} {achatter.end}\\n\\n\"\n",
    "# objects = f\"\\n__Objects in data__\\n\\n{', '.join([x for x in achatter.groups.values()])}\\n\\n\"\n",
    "file_name = 'report_results.md'\n",
    "achatter.chat = True\n",
    "achatter.chat_rep(scaled_cols, file_name)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23b0e527-81f0-436c-8c97-7caedf1291eb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff82f9d2-a024-4de6-a357-a5a0ef361eb2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
