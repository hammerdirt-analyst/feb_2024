{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "b0c6bfd7-6b8a-4bec-82a4-2a34360f2ddb",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/roger/anaconda3/envs/cantonal_report/lib/python3.9/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "# import matplotlib as mpl\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import seaborn as sns\n",
    "\n",
    "import matplotlib.patches as mpatches\n",
    "\n",
    "from session_config import  collect_survey_data, feature_variables\n",
    "from reports import make_report_objects, reports_and_forecast\n",
    "from reports import admin_report, features_present, histograms_standard\n",
    "from reports import ecdf_plots_standard, scatter_plot_standard\n",
    "from reports import labels_for_display, make_standard_report, make_report_objects\n",
    "# import userdisplay\n",
    "# import geospatial\n",
    "import gridforecast as gfcast\n",
    "import datetime as dt\n",
    "from IPython.display import Markdown\n",
    "\n",
    "import shap\n",
    "\n",
    "from featureevaluator import FeatureEvaluation\n",
    "from sklearn.cluster import KMeans\n",
    "from sklearn.preprocessing import StandardScaler, MinMaxScaler\n",
    "from scipy.stats import spearmanr\n",
    "from sklearn.linear_model import LinearRegression, LassoCV\n",
    "from sklearn.ensemble import RandomForestRegressor\n",
    "from sklearn.linear_model import TheilSenRegressor\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import r2_score, mean_squared_error\n",
    "from sklearn.inspection import permutation_importance\n",
    "from sklearn.decomposition import PCA\n",
    "from scipy.spatial import ConvexHull\n",
    "from sklearn.exceptions import ConvergenceWarning\n",
    "import warnings\n",
    "\n",
    "datax = collect_survey_data()\n",
    "\n",
    "# from use_cases example\n",
    "ooi = ['G10',  'G30', 'G31', 'G33', 'G34', 'G35', 'G8', 'G7', 'G6', 'G5', 'G4', 'G37', 'G2', 'G27', 'G25', 'G26', 'G11']\n",
    "# more refined search\n",
    "tobo_snacks = ['G27', 'G30', 'G35']\n",
    "# unidentified, plastic, different uses\n",
    "# udi = ['Gfrags', 'Gfoams']\n",
    "# industrial\n",
    "indus = ['G89', 'G67', 'G112', 'G93' , 'G66','G74', 'G72', 'G87', 'G65', 'G69', 'G68', 'G43', 'G41', 'G38', 'G36', 'G19', 'G17', 'Gfrags']\n",
    "\n",
    "# features\n",
    "land_covers = ['buildings', 'forest', 'undefined', 'public services', 'streets', 'orchards']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "507138ef-b6e9-498a-ac9b-1009975ef9e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "o_dates = {'start':'2020-01-01', 'end':'2021-12-31'}\n",
    "prior_dates = {'start':'2015-11-15', 'end':'2019-12-31'}\n",
    "\n",
    "# 'Neuchâtel', 'Zürich', \n",
    "\n",
    "\n",
    "canton = 'Bern'\n",
    "this_feature_type = 'l'\n",
    "\n",
    "d= datax.reset_index(drop=True)\n",
    "\n",
    "\n",
    "# make complete report\n",
    "params_l = {'canton':canton, 'date_range':o_dates, 'feature_type': this_feature_type}\n",
    "params_p = {'canton':canton, 'date_range':prior_dates, 'feature_type':this_feature_type}\n",
    "\n",
    "# set the parameters for the weighted prior\n",
    "# exclude records in the likelihood, set date range and feature type\n",
    "# make the land-use-inventory, exclude any likelihood values\n",
    "lu_catalogue = d[(d.canton != canton)&(d['date'] <= o_dates['end'])&(d.feature_type == 'l')].copy()\n",
    "catalog_surveys, catalog_features = make_report_objects(lu_catalogue)\n",
    "prior_feature = catalog_features.df_cat\n",
    "prior_feature['feature_type'] = 'l'\n",
    "# combined_land_use_catalog = catalog_features.df_cat.copy()\n",
    "# lake_land_use_catalog['feature_type'] = 'l' \n",
    "\n",
    "\n",
    "all_data_of_interest = d[(d['date'] >= prior_dates['start']) & (d['date'] <= o_dates['end'])&(d.feature_type == 'l')].copy()\n",
    "all_data_of_interest = all_data_of_interest[all_data_of_interest.canton == 'Bern'].copy()\n",
    "# create a variable for different code group totals\n",
    "\n",
    "m = all_data_of_interest.code.isin([*indus, *tobo_snacks])\n",
    "\n",
    "# o = all_data_of_interest.code.isin([indus)\n",
    "all_data_of_interest['use'] = all_data_of_interest.code.where(m, 'other')\n",
    "n = (all_data_of_interest.code.isin([*tobo_snacks, *all_data_of_interest[all_data_of_interest.use =='other'].code.unique()]))\n",
    "all_data_of_interest['use'] = all_data_of_interest.code.where(n, 'pro')\n",
    "o = all_data_of_interest.use.isin(['other', 'pro'])\n",
    "all_data_of_interest['use'] = all_data_of_interest.use.where(o, 'rec')\n",
    "all_data_of_interest.reset_index(inplace=True, drop=True)\n",
    "\n",
    "land_covers = ['buildings', 'forest', 'undefined', 'public services', 'streets', 'orchards', 'use', 'canton', 'city', 'feature_name']\n",
    "\n",
    "all_report, all_land_use = make_report_objects(all_data_of_interest, info_columns = ['use', 'canton', 'city', 'feature_name'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "94f69969-b5f9-45d0-a58e-eeba87db9139",
   "metadata": {},
   "outputs": [],
   "source": [
    "report_d = all_report.sample_results(info_columns=['use', 'canton', 'city', 'feature_name']).head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "55598d25-bff5-4b59-abe6-404232d987f1",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196 entries, 0 to 195\n",
      "Data columns (total 16 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   sample_id        196 non-null    object \n",
      " 1   location         196 non-null    object \n",
      " 2   date             196 non-null    object \n",
      " 3   use              196 non-null    object \n",
      " 4   canton           196 non-null    object \n",
      " 5   city             196 non-null    object \n",
      " 6   feature_name     196 non-null    object \n",
      " 7   quantity         196 non-null    int64  \n",
      " 8   pcs/m            196 non-null    float64\n",
      " 9   public services  196 non-null    float64\n",
      " 10  streets          196 non-null    float64\n",
      " 11  orchards         196 non-null    float64\n",
      " 12  vineyards        196 non-null    float64\n",
      " 13  buildings        196 non-null    float64\n",
      " 14  forest           196 non-null    float64\n",
      " 15  undefined        196 non-null    float64\n",
      "dtypes: float64(8), int64(1), object(7)\n",
      "memory usage: 24.6+ KB\n"
     ]
    }
   ],
   "source": [
    "all_land_use.df_cont.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "4b5472bb-35f0-4642-b5da-fb16e9a46920",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import KMeans, kmeans_plusplus\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.ensemble import GradientBoostingRegressor, BaggingRegressor, VotingRegressor\n",
    "\n",
    "def evaluate_feature_importance(best_model, model_name, X_test, y_test, X_train, y_train):\n",
    "\n",
    "    # the permuation importance of the variables\n",
    "    if model_name in ['Random Forest Regression', 'Linear Regression']:\n",
    "        perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "        perm_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns,\n",
    "            'Importance': perm_importance.importances_mean\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "    try:\n",
    "    # model feature importance\n",
    "        feature_importances_rf = best_model.feature_importances_\n",
    "        feature_importance_df = pd.DataFrame({\n",
    "            'Feature': X_test.columns,\n",
    "            'Importance': feature_importances_rf\n",
    "        }).sort_values(by='Importance', ascending=False)\n",
    "        return feature_importance_df, perm_importance_df\n",
    "    except AttributeError:\n",
    "    # if feature importance not avaialable try the coefficients\n",
    "        try:\n",
    "            params = best_model.coef_\n",
    "            feature_importances_rf = params\n",
    "            feature_importance_df = pd.DataFrame({'feature':X_test.columns, 'Coeficient':feature_importances_rf})\n",
    "            return feature_importance_df, perm_importance_df\n",
    "        except AttributeError:\n",
    "            #return an empty DataFrame\n",
    "            return pd.DataFrame(), perm_importance_df\n",
    "\n",
    "def find_elbow_point(sse):\n",
    "    n_points = len(sse)\n",
    "    all_coords = np.vstack((range(n_points), sse)).T\n",
    "    first_point = all_coords[0]\n",
    "    last_point = all_coords[-1]\n",
    "\n",
    "    line_vec = last_point - first_point\n",
    "    line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "\n",
    "    vec_from_first = all_coords - first_point\n",
    "    scalar_product = np.sum(vec_from_first * line_vec_norm, axis=1)\n",
    "    vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "    vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "\n",
    "    dist_to_line = np.sqrt(np.sum(vec_to_line**2, axis=1))\n",
    "    elbow_point = np.argmax(dist_to_line)\n",
    "    \n",
    "    return elbow_point + 1\n",
    "\n",
    "def filter_features(data, threshold: float = 0.2, terms: [] = None ):\n",
    "\n",
    "    filtered_columns = [col for col in terms if (data[col] > 0).mean() >= threshold]\n",
    "    return data[['pcs/m', 'canton', 'use', *filtered_columns]], filtered_columns\n",
    "    \n",
    "\n",
    "def determine_optimal_clusters(d):\n",
    "\n",
    "  \n",
    "    sse = []\n",
    "    k_range = range(1, 11)\n",
    "    for k in k_range:\n",
    "        kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "        kmeans.fit(d)\n",
    "        sse.append(kmeans.inertia_)\n",
    "    \n",
    "    optimal_k = find_elbow_point(sse)\n",
    "    return optimal_k, sse\n",
    "\n",
    "def kmeans_clustering(n_clusters, w_interactions: bool = False):\n",
    "    \n",
    "    kmeans = kmeans_plusplus(n_clusters=n_clusters, random_state=42)\n",
    "    \n",
    "        \n",
    "    d['clusters'] = kmeans.fit_predict(d)\n",
    "    some_features = [x for x in d.columns if x not in ['pcs/m','clusters', 'streets']]\n",
    "    \n",
    "    means = d.groupby(['clusters'])['pcs/m'].mean()\n",
    "    means_unscaled = self.unscale_target(means)\n",
    "    \n",
    "    counts = d.groupby(['clusters'])['pcs/m'].count()\n",
    "    \n",
    "    cluster_summary = d.groupby('clusters').agg({x:'mean' for x in some_features}).reset_index()\n",
    "    cluster_summary = self.unscale_values(cluster_summary, columns=some_features, w_interactions=w_interactions)\n",
    "    cluster_summary['pcs/m'] = means_unscaled\n",
    "    cluster_summary['samples'] = counts.values\n",
    "    cluster_summary = cluster_summary[['samples', 'pcs/m', *cluster_summary.columns[:-2]]]\n",
    "           \n",
    "    return cluster_summary, kmeans, d\n",
    "\n",
    "def unscale_target(means, ascaler):\n",
    "    means = means.values\n",
    "    means_shape = means.shape\n",
    "    if means.ndim == 1:\n",
    "        means = means.reshape(1, -1)\n",
    "\n",
    "    means_unscaled = ascaler.inverse_transform(means)\n",
    "        \n",
    "    means_unscaled.reshape(means_shape)\n",
    "    return means_unscaled[0]\n",
    "\n",
    "def perform_regression_analysis(d, features: [] = None, target_var: str = 'pcs/m'):\n",
    "    params = {\n",
    "        \"n_estimators\": 100,\n",
    "        \"max_depth\": 4,\n",
    "        \"min_samples_split\": 5,\n",
    "        \"learning_rate\": 0.01,\n",
    "        \"loss\": \"huber\",\n",
    "        \"alpha\": .9\n",
    "       \n",
    "        }\n",
    "    these_models = {\n",
    "        'Linear Regression': LinearRegression(random_state=42),\n",
    "        'Random Forest Regression': RandomForestRegressor(n_estimators=100, random_state=42),\n",
    "        'Gradient Boosting Regression': GradientBoostingRegressor(**params),\n",
    "        'Theil-Sen Regressor': TheilSenRegressor(random_state=42)\n",
    "        }\n",
    "      \n",
    "    \n",
    "    X = d[features]\n",
    "    # X = feature_scaler.transform(X)\n",
    "    y = d[target_var].values\n",
    "    # y = target_scaler.transform(y)\n",
    "    \n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "    \n",
    "    regression_results = []\n",
    "    best_model = None\n",
    "    best_r2 = -np.inf\n",
    "    the_name = None\n",
    "    \n",
    "    # sklearn - linear models        \n",
    "    for model_name, model in these_models.items():\n",
    "        with warnings.catch_warnings():\n",
    "            warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "            model.fit(X_train, y_train)\n",
    "            y_pred = model.predict(X_test)\n",
    "            r2 = r2_score(y_test, y_pred)\n",
    "            mse = mean_squared_error(y_test, y_pred)\n",
    "            regression_results.append({'Model': model_name, 'R²': r2, 'MSE': mse})\n",
    "            \n",
    "            if r2 > best_r2:\n",
    "                best_r2 = r2\n",
    "                best_model = model\n",
    "                the_name = model_name\n",
    "    # bagging\n",
    "    bag_estimator = these_models[the_name]\n",
    "    bag = BaggingRegressor(estimator=bag_estimator)\n",
    "    bag.fit(X_train, y_train)\n",
    "    y_pred = bag.predict(X_test)\n",
    "\n",
    "    \n",
    "    regression_results.append({'Model': f'Bagging:{the_name}', 'R²': bag.score(X_test, y_test), 'MSE':mean_squared_error(y_test, y_pred)})\n",
    "    # voting\n",
    "\n",
    "    lnr = these_models['Linear Regression']\n",
    "    rf = these_models['Random Forest Regression']\n",
    "    gbr = these_models['Gradient Boosting Regression']\n",
    "    voting = VotingRegressor([('lnr', lnr), ('rf', rf), ('gbr', gbr)])\n",
    "    voting.fit(X_train, y_train)\n",
    "    y_pred = voting.predict(X_test)\n",
    "    \n",
    "    regression_results.append({'Model': 'Voting', 'R²': voting.score(X_test, y_test), 'MSE':mean_squared_error(y_test, y_pred)})\n",
    "    \n",
    "    \n",
    "    \n",
    "    \n",
    "    return regression_results, best_model, the_name, X_test, y_test, X_train, y_train\n",
    "\n",
    "d = all_land_use.df_cont.copy()\n",
    "fcols = ['public services', 'buildings', 'forest', 'undefined', 'vineyards', 'orchards', 'streets']\n",
    "cluster_d, filtered_columns = filter_features(d.copy(), terms=fcols)\n",
    "\n",
    "target_scaler = StandardScaler()\n",
    "feature_scaler = StandardScaler()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "78a57d07-86e4-471b-8f1b-ded269b60a94",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['public services', 'buildings', 'forest', 'undefined', 'vineyards', 'streets']"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "filtered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "a69d00f5-057f-4e3b-b68c-e7665186a743",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196 entries, 0 to 195\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   pcs/m            196 non-null    float64\n",
      " 1   canton           196 non-null    object \n",
      " 2   use              196 non-null    object \n",
      " 3   public services  196 non-null    float64\n",
      " 4   buildings        196 non-null    float64\n",
      " 5   forest           196 non-null    float64\n",
      " 6   undefined        196 non-null    float64\n",
      " 7   vineyards        196 non-null    float64\n",
      " 8   streets          196 non-null    float64\n",
      "dtypes: float64(7), object(2)\n",
      "memory usage: 13.9+ KB\n"
     ]
    }
   ],
   "source": [
    "cluster_d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "8ff7b373-fd90-4eab-bb36-3225747cb57c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 196 entries, 0 to 195\n",
      "Data columns (total 9 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   pcs/m            196 non-null    float64\n",
      " 1   canton           196 non-null    object \n",
      " 2   use              196 non-null    object \n",
      " 3   public services  196 non-null    float64\n",
      " 4   buildings        196 non-null    float64\n",
      " 5   forest           196 non-null    float64\n",
      " 6   undefined        196 non-null    float64\n",
      " 7   vineyards        196 non-null    float64\n",
      " 8   streets          196 non-null    float64\n",
      "dtypes: float64(7), object(2)\n",
      "memory usage: 13.9+ KB\n"
     ]
    }
   ],
   "source": [
    "cluster_d['pcs/m'] = target_scaler.fit_transform(d[['pcs/m']])\n",
    "scaled_cols = [x for x in filtered_columns if x != 'streets']\n",
    "cluster_d[scaled_cols] = feature_scaler.fit_transform(cluster_d[scaled_cols])\n",
    "cluster_d.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5026f84f-2817-43ed-b923-af6bb62d4cb3",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.decomposition import PCA\n",
    "# initializing the PCA transformer and\n",
    "# logistic regression estimator:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "540aa0ce-8b1b-4b44-854e-ebd5b5afb8cf",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>use</th>\n",
       "      <th>pcs/m</th>\n",
       "      <th>nsamps</th>\n",
       "      <th>streets</th>\n",
       "      <th>public services</th>\n",
       "      <th>buildings</th>\n",
       "      <th>forest</th>\n",
       "      <th>undefined</th>\n",
       "      <th>vineyards</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>cluster</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>rec</td>\n",
       "      <td>0.802857</td>\n",
       "      <td>7</td>\n",
       "      <td>0.135132</td>\n",
       "      <td>0.000638</td>\n",
       "      <td>0.167</td>\n",
       "      <td>0.556</td>\n",
       "      <td>0.138</td>\n",
       "      <td>0.000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>rec</td>\n",
       "      <td>1.411286</td>\n",
       "      <td>70</td>\n",
       "      <td>0.251038</td>\n",
       "      <td>0.015443</td>\n",
       "      <td>0.309</td>\n",
       "      <td>0.106</td>\n",
       "      <td>0.565</td>\n",
       "      <td>0.020</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>rec</td>\n",
       "      <td>3.357619</td>\n",
       "      <td>21</td>\n",
       "      <td>0.179974</td>\n",
       "      <td>0.016754</td>\n",
       "      <td>0.680</td>\n",
       "      <td>0.153</td>\n",
       "      <td>0.152</td>\n",
       "      <td>0.015</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "         use     pcs/m  nsamps   streets  public services  buildings  forest  \\\n",
       "cluster                                                                        \n",
       "0        rec  0.802857       7  0.135132         0.000638      0.167   0.556   \n",
       "1        rec  1.411286      70  0.251038         0.015443      0.309   0.106   \n",
       "2        rec  3.357619      21  0.179974         0.016754      0.680   0.153   \n",
       "\n",
       "         undefined  vineyards  \n",
       "cluster                        \n",
       "0            0.138      0.000  \n",
       "1            0.565      0.020  \n",
       "2            0.152      0.015  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def clusters_by_use_case(cluster_data, use: str = 'pro', scaled_cols=scaled_cols, columns_to_cluster=['streets', *scaled_cols]):\n",
    "    \n",
    "    \n",
    "\n",
    "    cluster_p = cluster_data[cluster_data.use == use].copy()\n",
    "    nclusters = determine_optimal_clusters(cluster_p[columns_to_cluster])\n",
    "    kmeans = KMeans(n_clusters=nclusters[0], random_state=42).fit(cluster_p[columns_to_cluster])\n",
    "    cluster_p['cluster'] = kmeans.labels_\n",
    "    cluster_p[scaled_cols] = feature_scaler.inverse_transform(cluster_p[scaled_cols])\n",
    "    cluster_p['pcs/m'] = unscale_target(cluster_p['pcs/m'], target_scaler)\n",
    "   \n",
    "    df = cluster_p.drop_duplicates('cluster').sort_values('cluster').set_index('cluster', drop=True)\n",
    "    pcs_m = cluster_p.groupby(['use', 'cluster'], as_index=False).agg({'pcs/m': 'mean'}).set_index('cluster', drop=True)\n",
    "    samps = cluster_p.groupby(['use', 'cluster'], as_index=False).agg({'pcs/m': 'count'}).rename(columns={'pcs/m':'nsamples'}).set_index('cluster', drop=True)\n",
    "    pcs_m['nsamps'] =samps.nsamples.values\n",
    "    df = pcs_m.merge(df[columns_to_cluster], left_index=True, right_index=True)\n",
    "\n",
    "    return cluster_p, df\n",
    "\n",
    "cluster_pro, summary_pro = clusters_by_use_case(cluster_d)\n",
    "cluster_rec, summary_rec = clusters_by_use_case(cluster_d, use='rec')\n",
    "summary_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "aaba4432-0f02-47f4-8cf4-1777d2343c23",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 98 entries, 1 to 195\n",
      "Data columns (total 10 columns):\n",
      " #   Column           Non-Null Count  Dtype  \n",
      "---  ------           --------------  -----  \n",
      " 0   pcs/m            98 non-null     float64\n",
      " 1   canton           98 non-null     object \n",
      " 2   use              98 non-null     object \n",
      " 3   public services  98 non-null     float64\n",
      " 4   buildings        98 non-null     float64\n",
      " 5   forest           98 non-null     float64\n",
      " 6   undefined        98 non-null     float64\n",
      " 7   vineyards        98 non-null     float64\n",
      " 8   streets          98 non-null     float64\n",
      " 9   cluster          98 non-null     int32  \n",
      "dtypes: float64(7), int32(1), object(2)\n",
      "memory usage: 8.0+ KB\n"
     ]
    }
   ],
   "source": [
    "cluster_rec.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "e24b02c4-f1f1-4125-a9dc-973b864cda67",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pcs/m</th>\n",
       "      <th>public services</th>\n",
       "      <th>buildings</th>\n",
       "      <th>forest</th>\n",
       "      <th>undefined</th>\n",
       "      <th>vineyards</th>\n",
       "      <th>streets</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>pcs/m</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.594305</td>\n",
       "      <td>0.312254</td>\n",
       "      <td>0.217679</td>\n",
       "      <td>-0.171863</td>\n",
       "      <td>0.187353</td>\n",
       "      <td>0.560896</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>public services</th>\n",
       "      <td>0.594305</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.729031</td>\n",
       "      <td>-0.081645</td>\n",
       "      <td>-0.288387</td>\n",
       "      <td>0.269734</td>\n",
       "      <td>0.760403</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>buildings</th>\n",
       "      <td>0.312254</td>\n",
       "      <td>0.729031</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.281545</td>\n",
       "      <td>-0.484495</td>\n",
       "      <td>0.100292</td>\n",
       "      <td>0.753278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>forest</th>\n",
       "      <td>0.217679</td>\n",
       "      <td>-0.081645</td>\n",
       "      <td>-0.281545</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>-0.506422</td>\n",
       "      <td>-0.520583</td>\n",
       "      <td>0.025603</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>undefined</th>\n",
       "      <td>-0.171863</td>\n",
       "      <td>-0.288387</td>\n",
       "      <td>-0.484495</td>\n",
       "      <td>-0.506422</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.357783</td>\n",
       "      <td>-0.537401</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>vineyards</th>\n",
       "      <td>0.187353</td>\n",
       "      <td>0.269734</td>\n",
       "      <td>0.100292</td>\n",
       "      <td>-0.520583</td>\n",
       "      <td>0.357783</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>0.241662</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>streets</th>\n",
       "      <td>0.560896</td>\n",
       "      <td>0.760403</td>\n",
       "      <td>0.753278</td>\n",
       "      <td>0.025603</td>\n",
       "      <td>-0.537401</td>\n",
       "      <td>0.241662</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                    pcs/m  public services  buildings    forest  undefined  \\\n",
       "pcs/m            1.000000         0.594305   0.312254  0.217679  -0.171863   \n",
       "public services  0.594305         1.000000   0.729031 -0.081645  -0.288387   \n",
       "buildings        0.312254         0.729031   1.000000 -0.281545  -0.484495   \n",
       "forest           0.217679        -0.081645  -0.281545  1.000000  -0.506422   \n",
       "undefined       -0.171863        -0.288387  -0.484495 -0.506422   1.000000   \n",
       "vineyards        0.187353         0.269734   0.100292 -0.520583   0.357783   \n",
       "streets          0.560896         0.760403   0.753278  0.025603  -0.537401   \n",
       "\n",
       "                 vineyards   streets  \n",
       "pcs/m             0.187353  0.560896  \n",
       "public services   0.269734  0.760403  \n",
       "buildings         0.100292  0.753278  \n",
       "forest           -0.520583  0.025603  \n",
       "undefined         0.357783 -0.537401  \n",
       "vineyards         1.000000  0.241662  \n",
       "streets           0.241662  1.000000  "
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cluster_rec[['pcs/m', *filtered_columns]].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "599f37c7-7270-4076-a9aa-633c4866d869",
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "__init__() got an unexpected keyword argument 'random_state'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[13], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m regression_results, best_model, the_name, X_test, y_test, X_train, y_train \u001b[38;5;241m=\u001b[39m  \u001b[43mperform_regression_analysis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mcluster_d\u001b[49m\u001b[43m[\u001b[49m\u001b[43mcluster_d\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43muse\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m==\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mrec\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfeatures\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mfiltered_columns\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m      2\u001b[0m pd\u001b[38;5;241m.\u001b[39mDataFrame(regression_results)\n",
      "Cell \u001b[0;32mIn[5], line 115\u001b[0m, in \u001b[0;36mperform_regression_analysis\u001b[0;34m(d, features, target_var)\u001b[0m\n\u001b[1;32m    104\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mperform_regression_analysis\u001b[39m(d, features: [] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m, target_var: \u001b[38;5;28mstr\u001b[39m \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mpcs/m\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    105\u001b[0m     params \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    106\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mn_estimators\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m100\u001b[39m,\n\u001b[1;32m    107\u001b[0m         \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mmax_depth\u001b[39m\u001b[38;5;124m\"\u001b[39m: \u001b[38;5;241m4\u001b[39m,\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    112\u001b[0m        \n\u001b[1;32m    113\u001b[0m         }\n\u001b[1;32m    114\u001b[0m     these_models \u001b[38;5;241m=\u001b[39m {\n\u001b[0;32m--> 115\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mLinear Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: \u001b[43mLinearRegression\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrandom_state\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m42\u001b[39;49m\u001b[43m)\u001b[49m,\n\u001b[1;32m    116\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mRandom Forest Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: RandomForestRegressor(n_estimators\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m100\u001b[39m, random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m),\n\u001b[1;32m    117\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mGradient Boosting Regression\u001b[39m\u001b[38;5;124m'\u001b[39m: GradientBoostingRegressor(\u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mparams),\n\u001b[1;32m    118\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mTheil-Sen Regressor\u001b[39m\u001b[38;5;124m'\u001b[39m: TheilSenRegressor(random_state\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m42\u001b[39m)\n\u001b[1;32m    119\u001b[0m         }\n\u001b[1;32m    122\u001b[0m     X \u001b[38;5;241m=\u001b[39m d[features]\n\u001b[1;32m    123\u001b[0m     \u001b[38;5;66;03m# X = feature_scaler.transform(X)\u001b[39;00m\n",
      "\u001b[0;31mTypeError\u001b[0m: __init__() got an unexpected keyword argument 'random_state'"
     ]
    }
   ],
   "source": [
    "regression_results, best_model, the_name, X_test, y_test, X_train, y_train =  perform_regression_analysis(cluster_d[cluster_d.use == 'rec'].copy(), features=filtered_columns)\n",
    "pd.DataFrame(regression_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ca2aa96-0c0a-4c50-a917-91a9278e2be9",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_importance, permutation_feature_importance = evaluate_feature_importance(best_model, the_name, X_test, y_test, X_train, y_train)\n",
    "model_feature_importance.to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b46e6a4d-34b7-489e-bc66-797834dd527a",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "33ea570e-c9f1-4ed9-a3ee-86dab8538b1c",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.linear_model import LinearRegression\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import mean_squared_error, r2_score\n",
    "\n",
    "# Sample DataFrame structure (replace with your actual data)\n",
    "# data = pd.DataFrame({\n",
    "#     'sample_id': [('augustmutzenbergstrandweg', '2020-10-09'), ('augustmutzenbergstrandweg', '2020-10-09'), ('bielersee_vinelz_fankhausers', '2017-04-22'), ('bielersee_vinelz_fankhausers', '2017-04-22'), ('bielersee_vinelz_fankhausers', '2017-05-27')],\n",
    "#     'location': ['augustmutzenbergstrandweg', 'augustmutzenbergstrandweg', 'bielersee_vinelz_fankhausers', 'bielersee_vinelz_fankhausers', 'bielersee_vinelz_fankhausers'],\n",
    "#     'date': ['2020-10-09', '2020-10-09', '2017-04-22', '2017-04-22', '2017-05-27'],\n",
    "#     'use': ['pro', 'rec', 'pro', 'rec', 'pro'],\n",
    "#     'canton': ['Bern', 'Bern', 'Bern', 'Bern', 'Bern'],\n",
    "#     'city': ['Spiez', 'Spiez', 'Vinelz', 'Vinelz', 'Vinelz'],\n",
    "#     'feature_name': ['thunersee', 'thunersee', 'bielersee', 'bielersee', 'bielersee'],\n",
    "#     'quantity': [0, 7, 57, 48, 78],\n",
    "#     'pcs/m': [0.0, 0.77, 1.62, 1.4, 2.23],\n",
    "#     'public services': [1, 1, 1, 1, 1],\n",
    "#     'streets': [1, 1, 2, 2, 2],\n",
    "#     'orchards': [1, 1, 1, 1, 1],\n",
    "#     'vineyards': [1, 1, 1, 1, 1],\n",
    "#     'buildings': [4, 4, 2, 2, 2],\n",
    "#     'forest': [1, 1, 1, 1, 1],\n",
    "#     'undefined': [1, 1, 3, 3, 3]\n",
    "# })\n",
    "\n",
    "# # We will use only the necessary columns for PCA and regression\n",
    "# features = ['public services', 'streets', 'orchards', 'vineyards', 'buildings', 'forest', 'undefined']\n",
    "data = cluster_d[cluster_d.use == 'rec'].copy()\n",
    "data.reset_index(inplace=True, drop=True)\n",
    "X = data[filtered_columns]\n",
    "y = data['pcs/m']\n",
    "\n",
    "# Standardize the features\n",
    "# scaler = StandardScaler()\n",
    "# X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "# Apply PCA\n",
    "pca = PCA(n_components=4)  # You can choose the number of components\n",
    "X_pca = pca.fit_transform(X)\n",
    "\n",
    "# Split the data into training and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(X_pca, y, test_size=0.2, random_state=42)\n",
    "\n",
    "# Train a regression model\n",
    "regressor = RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "regressor.fit(X_train, y_train)\n",
    "\n",
    "# Predict on the test set\n",
    "y_pred = regressor.predict(X_test)\n",
    "\n",
    "# Evaluate the model\n",
    "mse = mean_squared_error(y_test, y_pred)\n",
    "r2 = r2_score(y_test, y_pred)\n",
    "\n",
    "print(\"Mean Squared Error:\", mse)\n",
    "print(\"R² Score:\", r2)\n",
    "\n",
    "# Print the explained variance ratio\n",
    "print(\"Explained Variance Ratio:\", pca.explained_variance_ratio_)\n",
    "print(\"Cumulative Explained Variance:\", np.cumsum(pca.explained_variance_ratio_))\n",
    "\n",
    "# # Function to predict the density given a profile\n",
    "# def predict_density(profile, scaler, pca, regressor):\n",
    "#     profile_scaled = scaler.transform([profile])\n",
    "#     profile_pca = pca.transform(profile_scaled)\n",
    "#     prediction = regressor.predict(profile_pca)\n",
    "#     return prediction[0]\n",
    "\n",
    "# # Example profile for prediction\n",
    "# profile = [1, 1, 1, 1, 4, 1, 1]\n",
    "# predicted_density = predict_density(profile, scaler, pca, regressor)\n",
    "# print(f\"Predicted density for profile {profile}: {predicted_density}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fd14c7f-9fbc-44ed-b7f7-9bf4d922b3d0",
   "metadata": {},
   "outputs": [],
   "source": [
    "X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "467103d4-e51a-4457-927a-da32361edfab",
   "metadata": {},
   "outputs": [],
   "source": [
    "filtered_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6b3a4bb7-6ae7-4fd9-8877-43f81b95a66d",
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.DataFrame(pca.components_, columns=filtered_columns).to_markdown()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eedb3e0a-3af7-421b-840a-5fcfeff1bcf5",
   "metadata": {},
   "source": [
    "## prompt\n",
    "\n",
    "this is my output from pca.components_, I also included a cluster analysis I would like to know how these help in analyzing the results from the data. That is what does the PCA, Cluster Analysis, Feature Importance, Permutation importance and regression results tell us about our data specifically?\n",
    "\n",
    "Please compare the data in these tables. Tell me what I can infer about the data.\n",
    "\n",
    " \n",
    "| pca_cluster   |   public services |   buildings |      forest |   undefined |   vineyards |    streets | |---:|------------------:|------------:|------------:|------------:|------------:|-----------:|\n",
    "|  0 |         0.576462  |  0.584898   |  0.0501843  |   -0.551022 |  -0.135209  |  0.0340636 |\n",
    "|  1 |        -0.15808   | -0.281025   |  0.868234   |   -0.374328 |  -0.0444754 | -0.0102967 |\n",
    "|  2 |         0.0684222 |  0.00692365 | -0.00462544 |   -0.162147 |   0.984258  |  0.0139045 |\n",
    "|  3 |         0.79313   | -0.437969   |  0.168645   |    0.381859 |   0.010671  |  0.068973  |\n",
    "\n",
    "\n",
    "This is the summary of pca:\n",
    "\n",
    "Explained Variance Ratio: \\[0.50039666 0.22192225 0.16947933 0.08456349\\]\n",
    "Cumulative Explained Variance: \\[0.50039666 0.72231891 0.89179824 0.97636173\\]\n",
    "\n",
    "I also did a cluster analysis\n",
    "\n",
    "|   cluster | use   |    pcs/m |   nsamps |   streets |   public services |   buildings |   forest |   undefined |   vineyards |\\n|----------:|:------|---------:|---------:|----------:|------------------:|------------:|---------:|------------:|------------:|\\n|         0 | pro   | 0.422857 |        7 |  0.135132 |       0.000638057 |       0.167 |    0.556 |       0.138 |       0     |\\n|         1 | pro   | 1.06686  |       70 |  0.251038 |       0.0154434   |       0.309 |    0.106 |       0.565 |       0.02  |\\n|         2 | pro   | 1.10381  |       21 |  0.179974 |       0.0167539   |       0.68  |    0.153 |       0.152 |       0.015 |\n",
    "\n",
    "here are the regression results before the pca\n",
    "\n",
    "Random Forest Regression \tr² = 0.43, MSE=0.85\n",
    "here is the feature importances:\n",
    "\n",
    "|    | Feature         |   Importance |\\n|---:|:----------------|-------------:|\\n|  0 | public services |    0.395367  |\\n|  4 | vineyards       |    0.352367  |\\n|  2 | forest          |    0.0988824 |\\n|  3 | undefined       |    0.0794109 |\\n|  1 | buildings       |    0.0739721 |\n",
    "\n",
    "here is the permutation feature importance\n",
    "\n",
    "|    | Feature         |   Importance |\\n|---:|:----------------|-------------:|\\n|  0 | public services |   0.370762   |\\n|  4 | vineyards       |   0.130635   |\\n|  2 | forest          |   0.029211   |\\n|  1 | buildings       |   0.00597635 |\\n|  3 | undefined       |  -0.0135929  |\n",
    "\n",
    "here are the regresionn results after the pca\n",
    "Random Forest regression MSE: 0.8906075059724426,  R² Score: 0.40510057101753845"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92238003-0145-4163-9619-64f7d8b6b0a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbc3063b-6720-4c5e-bc4b-116c843b21f3",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca = PCA(n_components=3)\n",
    "lr = LogisticRegression(multi_class='ovr',\n",
    "                        random_state=1,\n",
    "                        solver='lbfgs')\n",
    "# dimensionality reduction:\n",
    "X_train_pca = pca.fit_transform(X_train)\n",
    "X_test_pca = pca.transform(X_test)\n",
    "# fitting the logistic regression model on the reduced dataset:\n",
    "lr.fit(X_train_pca, y_train)\n",
    "# plot_decision_regions(X_train_pca, y_train, classifier=lr)\n",
    "# plt.xlabel('PC 1')\n",
    "# plt.ylabel('PC 2')\n",
    "# plt.legend(loc='lower left')\n",
    "# plt.tight_layout()\n",
    "# plt.show()\n",
    "pca.components_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a3548ddf-0da8-48bc-a2fc-40cce792fc99",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.get_feature_names_out()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7e260a0e-9ad3-4ec0-b464-808516532f29",
   "metadata": {},
   "outputs": [],
   "source": [
    "t =y_train.reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03f074ba-265c-4d38-97cc-b91fb970ad56",
   "metadata": {},
   "outputs": [],
   "source": [
    "t"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "461742d4-9dc7-4831-9cbc-dd933dc9266f",
   "metadata": {},
   "outputs": [],
   "source": [
    "dta = pd.DataFrame(X_train_pca, columns=pca.get_feature_names_out())\n",
    "dta['Y'] = t\n",
    "dta"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04b70da0-06c1-4663-ab97-8e7865793716",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "afbd8015-4d6d-4e1c-9b09-0088b22bd8fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "pca.explained_variance_ratio_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0318231f-4113-4c7c-8f26-50b8e934eaec",
   "metadata": {},
   "outputs": [],
   "source": [
    "summary_rec"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7b169620-ef78-4dad-a5e9-c7876b280bba",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_rec[['pcs/m', *fcols]].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "94e5e80a-d773-466d-a084-108455c95d8b",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results, best_model, the_name, X_test, y_test, X_train, y_train =  perform_regression_analysis(cluster_d[cluster_d.use == 'rec'].copy(), features=filtered_columns)\n",
    "pd.DataFrame(regression_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6e4f850-0eb7-4c6f-9d16-dcf7757299f1",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_importance, permutation_feature_importance = evaluate_feature_importance(best_model, the_name, X_test, y_test, X_train, y_train)\n",
    "model_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d9193b64-e833-45b9-979a-81b36f27f3f6",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b6e714c4-188d-4893-9d7f-87a2321ad8ec",
   "metadata": {},
   "outputs": [],
   "source": [
    "amin, amax = cluster_pro[cluster_pro.cluster == 2].buildings.min(), cluster_pro[cluster_pro.cluster == 2].buildings.max()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8dc47cc6-10de-4018-b7b7-758ae88a0ad1",
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_interaction_terms(some_data: pd.DataFrame = None):\n",
    "    terms = ['streets']\n",
    "    label_terms = 'streets'\n",
    "    interaction_terms = []\n",
    "    cols = [x for x in fcols if x not in terms]\n",
    "        \n",
    "    for column in cols:\n",
    "        interaction_term = f'{column}_{label_terms}'\n",
    "        some_data[interaction_term] = d[column] * d[terms[0]]\n",
    "        interaction_terms.append(interaction_term)\n",
    "    \n",
    "    return interaction_terms, some_data\n",
    "\n",
    "cluster_d['streets'] = d['streets']\n",
    "\n",
    "some_terms, cluster_di = create_interaction_terms(cluster_d)\n",
    "# interaction_scaler = MinMaxScaler()\n",
    "# cluster_di[some_terms] = interaction_scaler.fit_transform(cluster_di[some_terms])\n",
    "cluster_di = cluster_di[['pcs/m', 'use', *some_terms]]\n",
    "cluster_di[cluster_di.use == 'pro'][['pcs/m', *some_terms]].corr('spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e87b37c-314d-4d33-b080-92be5a399f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results, best_model, the_name, X_test, y_test, X_train, y_train =  perform_regression_analysis(cluster_d[cluster_di.use == 'pro'].copy(), features=some_terms)\n",
    "pd.DataFrame(regression_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c25e1d70-61a3-43cc-b24c-21fa37cd548e",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_importance, permutation_feature_importance = evaluate_feature_importance(best_model, the_name, X_test, y_test, X_train, y_train)\n",
    "model_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "973f1b4d-8b5c-4f09-861e-5e8d76548c28",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78868425-ce88-4519-a570-24c27abc7870",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_di[cluster_di.use == 'rec'][['pcs/m', *some_terms]].corr(method='spearman')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9ec4b4fc-87da-4495-9812-d288a14b813f",
   "metadata": {},
   "outputs": [],
   "source": [
    "regression_results, best_model, the_name, X_test, y_test, X_train, y_train =  perform_regression_analysis(cluster_d[cluster_di.use == 'rec'].copy(), features=some_terms)\n",
    "pd.DataFrame(regression_results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "115f6952-2680-441a-9bbb-6754dcf1813b",
   "metadata": {},
   "outputs": [],
   "source": [
    "model_feature_importance, permutation_feature_importance = evaluate_feature_importance(best_model, the_name, X_test, y_test, X_train, y_train)\n",
    "model_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "753ca2c6-6cdb-4de1-b8b2-432283177aac",
   "metadata": {},
   "outputs": [],
   "source": [
    "permutation_feature_importance"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "acdeea16-a2b6-4d9d-a1d1-52d1af489c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "cluster_di.head().to_markdown()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "76c7bedf-c376-4c04-ab31-841962ecf266",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.gaussian_process import GaussianProcessRegressor\n",
    "from sklearn.gaussian_process.kernels import RBF, ConstantKernel as C, RationalQuadratic, ExpSineSquared, Matern\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "\n",
    "\n",
    "# Define function to prepare data, train, and predict using Gaussian Process Regressor\n",
    "def train_and_predict(data):\n",
    "    # Extract features and target variable\n",
    "    X = data[['public services', 'streets', 'orchards', 'vineyards', 'buildings', 'forest', 'undefined']]\n",
    "    y = data['pcs/m']\n",
    "    \n",
    "    # Split the data into training and test sets\n",
    "    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)\n",
    "    \n",
    "    # Standardize the features\n",
    "    scaler = StandardScaler()\n",
    "    X_train_scaled = scaler.fit_transform(X_train)\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "    \n",
    "    # Define the kernel\n",
    "    kernel = C(1.0, (1e-3, 1e3)) * RBF(length_scale=1.0, length_scale_bounds=(1e-2, 1e2)) + \\\n",
    "             RationalQuadratic(length_scale=1.0, alpha=0.1) + \\\n",
    "             ExpSineSquared(length_scale=1.0, periodicity=3.0) + \\\n",
    "             Matern(length_scale=1.0, nu=1.5)\n",
    "    \n",
    "    # Train the Gaussian Process Regressor\n",
    "    gpr = GaussianProcessRegressor(kernel=kernel, n_restarts_optimizer=10, random_state=42)\n",
    "    gpr.fit(X_train_scaled, y_train)\n",
    "    \n",
    "    # Predict on the test set\n",
    "    y_pred, y_std = gpr.predict(X_test_scaled, return_std=True)\n",
    "    \n",
    "    # Evaluate the model\n",
    "    print(\"Predictions:\", y_pred)\n",
    "    print(\"Uncertainties (std dev):\", y_std)\n",
    "    \n",
    "    # Function to predict the density given a profile\n",
    "    def predict_density(profile):\n",
    "        profile_scaled = scaler.transform([profile])\n",
    "        prediction, std_dev = gpr.predict(profile_scaled, return_std=True)\n",
    "        return prediction[0], std_dev[0]\n",
    "    \n",
    "    return predict_density\n",
    "\n",
    "# Train and predict for 'pro' use\n",
    "predict_density_pro = train_and_predict(data_pro)\n",
    "\n",
    "# Train and predict for 'rec' use\n",
    "predict_density_rec = train_and_predict(data_rec)\n",
    "\n",
    "# Example profile for prediction\n",
    "profile = [1, 1, 1, 1, 4, 1, 1]\n",
    "predicted_density_pro, uncertainty_pro = predict_density_pro(profile)\n",
    "print(f\"Predicted density for profile {profile} (pro): {predicted_density_pro} ± {uncertainty_pro}\")\n",
    "\n",
    "predicted_density_rec, uncertainty_rec = predict_density_rec(profile)\n",
    "print(f\"Predicted density for profile {profile} (rec): {predicted_density_rec} ± {uncertainty_rec}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41c5d0a4-ea0c-45a1-b48b-e97d99c6ee5a",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.cluster import DBSCAN\n",
    "\n",
    "\n",
    "# Perform DBSCAN clustering\n",
    "db = DBSCAN(eps=0.01, min_samples=7).fit(cluster_d[fcols])\n",
    "db_pro = cluster_di.copy()\n",
    "# Get cluster labels\n",
    "db_pro['cluster'] = db.labels_\n",
    "\n",
    "means = db_pro.groupby(['cluster'], as_index=False).agg({'pcs/m': 'mean'})\n",
    "means['samples'] = db_pro.groupby(['cluster'])['pcs/m'].count()\n",
    "# meanss = pd.melt(means, id_vars=['cluster'], value_vars=['rec', 'pro'])\n",
    "   \n",
    "db_pro[db_pro.use == 'pro'].groupby(['cluster']).agg({x:'mean' for x in some_terms})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1006363-8efb-4740-9956-e30db6f84cfe",
   "metadata": {},
   "outputs": [],
   "source": [
    "means"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d7564be4-3fe2-4197-991f-e15d484c6335",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ef41d80-4805-44d6-92a3-9a8683f506fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "def plot_feature_importances(self, feature_importance_df, title):\n",
    "    custom_palette = sns.color_palette(\"gist_rainbow\", n_colors=len(feature_importance_df))\n",
    "    # feature_importance_df.plot(kind='bar', x='Feature', y='Importance', figsize=(10, 6))\n",
    "    sns.barplot(data=feature_importance_df, x='Feature', y='Importance', hue='Feature', palette=custom_palette)\n",
    "    plt.xlabel('Cluster')\n",
    "    plt.ylabel('Importance')\n",
    "    plt.title('Feature importance')\n",
    "    plt.legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "    plt.grid(True)\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ec5dadb4-54ef-4525-b63a-7c3d6cc74e43",
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "class FeatureEvaluation:\n",
    "    def __init__(self, data, feature_vars=None, target_var='pcs/m', regression_models=None):\n",
    "        self.data = data\n",
    "        self.feature_vars = feature_vars or ['orchards', 'vineyards', 'buildings', 'forest', 'undefined', 'public services', 'streets', 'use', 'canton', 'feature_name']\n",
    "        self.target_var = target_var\n",
    "        self.target_scaler = MinMaxScaler()\n",
    "        self.feature_scaler = MinMaxScaler()\n",
    "        self.feature_int_scaler = MinMaxScaler()\n",
    "        self.regression_models = regression_models or {\n",
    "            'Linear Regression': LinearRegression(),\n",
    "            'Random Forest Regression': RandomForestRegressor(n_estimators=100, random_state=42)\n",
    "        }\n",
    "        self.interactions = self.preprocess_data()\n",
    "\n",
    "    def unscale_values(self, scaled_values, columns=None, w_interactions: bool = False):\n",
    "        \"\"\"\n",
    "        Unscale the given values.\n",
    "        \n",
    "        :param scaled_values: Array-like, scaled values to unscale\n",
    "        :param columns: List of column names or indices to unscale. If None, unscale all columns.\n",
    "        :return: Unscaled values\n",
    "        \"\"\"\n",
    "        if isinstance(scaled_values, pd.DataFrame):\n",
    "            if columns is None:\n",
    "                columns = scaled_values.columns\n",
    "            scaled_array = scaled_values[columns].values\n",
    "        else:\n",
    "            scaled_array = np.array(scaled_values)\n",
    "            if columns is None:\n",
    "                columns = range(scaled_array.shape[1])\n",
    "            elif isinstance(columns[0], str):\n",
    "                raise ValueError(\"Column names provided but input is not a DataFrame\")\n",
    "            scaled_array = scaled_array[:, columns]\n",
    "\n",
    "        original_shape = scaled_array.shape\n",
    "\n",
    "        # Reshape to 2D if necessary\n",
    "        if scaled_array.ndim == 1:\n",
    "            scaled_array = scaled_array.reshape(1, -1)\n",
    "        if w_interactions is True:\n",
    "            unscaled_array = self.feature_int_scaler.inverse_transform(scaled_array)\n",
    "        else:\n",
    "            unscaled_array = self.feature_scaler.inverse_transform(scaled_array)\n",
    "\n",
    "        # Reshape back to original shape if it was 1D\n",
    "        if len(original_shape) == 1:\n",
    "            unscaled_array = unscaled_array.flatten()\n",
    "\n",
    "        if isinstance(scaled_values, pd.DataFrame):\n",
    "            unscaled_df = scaled_values.copy()\n",
    "            unscaled_df[columns] = unscaled_array\n",
    "            return unscaled_df\n",
    "        else:\n",
    "            return unscaled_array\n",
    "    \n",
    "    def unscale_target(self, means):\n",
    "        means = means.values\n",
    "        means_shape = means.shape\n",
    "        if means.ndim == 1:\n",
    "            means = means.reshape(1, -1)\n",
    "    \n",
    "        means_unscaled = self.target_scaler.inverse_transform(means)\n",
    "            \n",
    "        means_unscaled.reshape(means_shape)\n",
    "        return means_unscaled[0]\n",
    "\n",
    "    def preprocess_data(self, remove_features_by_threshold=0.5, scale_these: [] = None):\n",
    "\n",
    "        \n",
    "        if scale_these is None:\n",
    "            dfi = filter_features(self.data.copy(), threshold=remove_features_by_threshold)\n",
    "            dfi.reset_index(drop=True, inplace=True)\n",
    "            df = dfi.copy()\n",
    "            Y = df[['pcs/m']].copy()\n",
    "            these_features = [x for x in df.columns if x not in ['pcs/m', 'streets']]\n",
    "            self.feature_vars = [*these_features, 'streets']\n",
    "            df[these_features] = self.feature_scaler.fit_transform(df[these_features])\n",
    "            df['pcs/m'] = self.target_scaler.fit_transform(Y)\n",
    "            df['streets'] = dfi.streets\n",
    "                        \n",
    "            self.data = df\n",
    "            \n",
    "            new_df = create_interaction_terms(dfi)\n",
    "            these_features = [x for x in new_df.columns if x not in ['streets', 'pcs/m', 'public services']]\n",
    "            \n",
    "\n",
    "            new_df[these_features] = self.feature_int_scaler.fit_transform(new_df[these_features])\n",
    "            new_df['pcs/m'] = self.target_scaler.fit_transform(new_df['pcs/m'].values.reshape(-1, 1))\n",
    "            new_df['streets'] = dfi.streets\n",
    "            self.interactions = new_df\n",
    "            self.interaction_terms = these_features\n",
    "            return new_df\n",
    "            \n",
    "        else:\n",
    "            return scaler.fit_transform(df[scale_these])\n",
    "            \n",
    "            \n",
    "    \n",
    "    def find_elbow_point(self, sse):\n",
    "        n_points = len(sse)\n",
    "        all_coords = np.vstack((range(n_points), sse)).T\n",
    "        first_point = all_coords[0]\n",
    "        last_point = all_coords[-1]\n",
    "\n",
    "        line_vec = last_point - first_point\n",
    "        line_vec_norm = line_vec / np.sqrt(np.sum(line_vec**2))\n",
    "\n",
    "        vec_from_first = all_coords - first_point\n",
    "        scalar_product = np.sum(vec_from_first * line_vec_norm, axis=1)\n",
    "        vec_from_first_parallel = np.outer(scalar_product, line_vec_norm)\n",
    "        vec_to_line = vec_from_first - vec_from_first_parallel\n",
    "\n",
    "        dist_to_line = np.sqrt(np.sum(vec_to_line**2, axis=1))\n",
    "        elbow_point = np.argmax(dist_to_line)\n",
    "        \n",
    "        return elbow_point + 1\n",
    "\n",
    "    def determine_optimal_clusters(self, w_interactions: bool = False):\n",
    "\n",
    "        if w_interactions is True:\n",
    "            d = self.interactions\n",
    "        else:\n",
    "            d = self.data\n",
    "        \n",
    "        sse = []\n",
    "        k_range = range(1, 11)\n",
    "        for k in k_range:\n",
    "            kmeans = KMeans(n_clusters=k, random_state=42)\n",
    "            kmeans.fit(d)\n",
    "            sse.append(kmeans.inertia_)\n",
    "        \n",
    "        optimal_k = self.find_elbow_point(sse)\n",
    "        return optimal_k\n",
    "    \n",
    "    def kmeans_clustering(self, n_clusters, w_interactions: bool = False):\n",
    "        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n",
    "\n",
    "        if w_interactions is True:\n",
    "            d = self.interactions\n",
    "        else:\n",
    "            d = self.data\n",
    "        d['clusters'] = kmeans.fit_predict(d)\n",
    "        some_features = [x for x in d.columns if x not in ['pcs/m','clusters', 'streets']]\n",
    "\n",
    "        means = d.groupby(['clusters'])['pcs/m'].mean()\n",
    "        means_unscaled = self.unscale_target(means)\n",
    "\n",
    "        counts = d.groupby(['clusters'])['pcs/m'].count()\n",
    "        \n",
    "        cluster_summary = d.groupby('clusters').agg({x:'mean' for x in some_features}).reset_index()\n",
    "        cluster_summary = self.unscale_values(cluster_summary, columns=some_features, w_interactions=w_interactions)\n",
    "        cluster_summary['pcs/m'] = means_unscaled\n",
    "        cluster_summary['samples'] = counts.values\n",
    "        cluster_summary = cluster_summary[['samples', 'pcs/m', *cluster_summary.columns[:-2]]]\n",
    "               \n",
    "        return cluster_summary, kmeans, d\n",
    "\n",
    "    def create_interaction_terms(self, some_data: pd.DataFrame = None):\n",
    "        if some_data is None:\n",
    "            d = self.data.copy()\n",
    "            these_features = self.feature_vars\n",
    "            \n",
    "        else:\n",
    "            d = some_data.copy()\n",
    "            these_features = [x for x in d.columns if x != 'pcs/m']\n",
    "        \n",
    "        \n",
    "        correlation_matrix = d[these_features].corr()\n",
    "        self.corr_matrix = correlation_matrix\n",
    "        interaction_terms = []\n",
    "        \n",
    "        for (feature1, feature2) in correlation_matrix[abs(correlation_matrix) > 0.5].stack().index.tolist():\n",
    "            if feature1 != feature2 and (feature2, feature1) not in interaction_terms:\n",
    "                interaction_term = feature1 + \"_\" + feature2\n",
    "                d[interaction_term] = d[feature1] * d[feature2]\n",
    "                interaction_terms.append(interaction_term)\n",
    "        \n",
    "        return interaction_terms, d\n",
    "        \n",
    "    def perform_regression_analysis(self, w_interactions: bool = False):\n",
    "\n",
    "        if w_interactions is True:\n",
    "            d = self.interactions\n",
    "            features = [x for x in self.interaction_terms if x not in ['pcs/m', 'clusters', 'streets']]\n",
    "        else:\n",
    "            d = self.data\n",
    "            features = [x for x in d.columns if x not in ['pcs/m', 'clusters']]       \n",
    "        \n",
    "        X = d[features]\n",
    "        y = d[self.target_var]\n",
    "        \n",
    "        X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
    "        \n",
    "        regression_results = []\n",
    "        best_model = None\n",
    "        best_r2 = -np.inf\n",
    "        the_name = None\n",
    "\n",
    "        # sklearn - linear models        \n",
    "        for model_name, model in self.regression_models.items():\n",
    "            with warnings.catch_warnings():\n",
    "                warnings.simplefilter(\"ignore\", ConvergenceWarning)\n",
    "                model.fit(X_train, y_train)\n",
    "                y_pred = model.predict(X_test)\n",
    "                r2 = r2_score(y_test, y_pred)\n",
    "                mse = mean_squared_error(y_test, y_pred)\n",
    "                regression_results.append({'Model': model_name, 'R²': r2, 'MSE': mse})\n",
    "                \n",
    "                if r2 > best_r2:\n",
    "                    best_r2 = r2\n",
    "                    best_model = model\n",
    "                    the_name = model_name\n",
    "        \n",
    "\n",
    "        return regression_results, best_model, the_name, X_test, y_test, X_train, y_train\n",
    "\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "    def evaluate_feature_importance(self, best_model, model_name, X_test, y_test, X_train, y_train):\n",
    "\n",
    "        # the_model = self.regression_models[model]\n",
    "        \n",
    "\n",
    "        if model_name != 'Negative Binomial':\n",
    "            perm_importance = permutation_importance(best_model, X_test, y_test, n_repeats=30, random_state=42)\n",
    "            perm_importance_df = pd.DataFrame({\n",
    "                'Feature': X_test.columns,\n",
    "                'Importance': perm_importance.importances_mean\n",
    "                }).sort_values(by='Importance', ascending=False)\n",
    "\n",
    "        try:\n",
    "        # Attempt to get feature importances\n",
    "            feature_importances_rf = best_model.feature_importances_\n",
    "            feature_importance_df = pd.DataFrame({\n",
    "                'Feature': X_test.columns,\n",
    "                'Importance': feature_importances_rf\n",
    "            }).sort_values(by='Importance', ascending=False)\n",
    "            return feature_importance_df, perm_importance_df\n",
    "        except AttributeError:\n",
    "        # If feature importances are not available, try to get parameters\n",
    "            try:\n",
    "                params = best_model.coef_\n",
    "                feature_importances_rf = params\n",
    "                feature_importance_df = pd.DataFrame({'feature':X_test.columns, 'Coeficient':feature_importances_rf})\n",
    "                return feature_importance_df, perm_importance_df\n",
    "            except AttributeError:\n",
    "                # If neither are available, return an empty DataFrame\n",
    "                return pd.DataFrame(), perm_importance_df\n",
    "\n",
    "    def plot_cluster_barchart(self, cluster_summary, title, w_interactions: bool = False):\n",
    "\n",
    "        if w_interactions is True:\n",
    "            d = self.interactions.copy()\n",
    "            features = self.interaction_terms\n",
    "        else:\n",
    "            d = self.data.copy()\n",
    "            features = self.feature_vars\n",
    "      \n",
    "        if 'clusters' in d:\n",
    "            \n",
    "            cluster_means =d[['clusters' ,*features]].groupby('clusters', as_index=False).mean()\n",
    "            cluster_means = pd.melt(cluster_means, id_vars=['clusters'], value_vars=[x for x in cluster_means.columns if x != 'clusters'])\n",
    "        else:\n",
    "            n = self.determine_optimal_clusters()\n",
    "            cluster_summary, kmeans, d = self.kmeans_clustering(n, w_interactions=w_interactions)\n",
    "            cluster_means = d[['clusters' , *features]].copy()\n",
    "            \n",
    "            cluster_means =cluster_means.groupby('clusters', as_index=False).mean()\n",
    "            cluster_means = pd.melt(cluster_means, id_vars=['clusters'], value_vars=cluster_means.columns[0:])\n",
    "            \n",
    "\n",
    "        # Custom color palette\n",
    "        custom_palette = sns.color_palette(\"gist_rainbow\", n_colors=len(cluster_means['variable'].unique()))\n",
    "        fig, ax = plt.subplots()\n",
    "\n",
    "        ax = sns.barplot(data=cluster_means, x=cluster_means.clusters, y=cluster_means.value, hue=cluster_means.variable, palette=custom_palette)\n",
    "\n",
    "        \n",
    "        ax2 = ax.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "        \n",
    "        ax2.set_ylabel('pcs/m', color='black')  # we already handled the x-label with ax1\n",
    "        sns.scatterplot(data=cluster_summary, x=cluster_summary.index, y='pcs/m' , color='black', ax=ax2)\n",
    "        ax2.tick_params(axis='y', labelcolor='black')\n",
    "        ax.legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        \n",
    "        fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "        ax.set_xlabel('Cluster')\n",
    "        ax.set_ylabel('Average Proportion')\n",
    "        ax.set_title('Average Feature Values and pcs/m by Cluster')\n",
    "        # \n",
    "        plt.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "    def plot_feature_importances(self, feature_importance_df, title):\n",
    "        custom_palette = sns.color_palette(\"gist_rainbow\", n_colors=len(feature_importance_df))\n",
    "        # feature_importance_df.plot(kind='bar', x='Feature', y='Importance', figsize=(10, 6))\n",
    "        sns.barplot(data=feature_importance_df, x='Feature', y='Importance', hue='Feature', palette=custom_palette)\n",
    "        plt.xlabel('Cluster')\n",
    "        plt.ylabel('Importance')\n",
    "        plt.title('Feature importance')\n",
    "        plt.legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "        plt.grid(True)\n",
    "        plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "062fd209-f839-40d1-9564-bcded17c21a8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# custom_palette = sns.color_palette(\"gist_rainbow\", n_colors=len(cluster_means['variable'].unique()))\n",
    "\n",
    "# ax = sns.barplot(data=cluster_means, x=cluster_means.cluster, y=cluster_means.value, hue=cluster_means.variable, palette=custom_palette)\n",
    "\n",
    "\n",
    "# ax2 = ax.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "# ax2.set_ylabel('pcs/m', color='black') \n",
    "# sns.scatterplot(data=meanss, x='cluster', y='value' , hue='variable', ax=ax2)\n",
    "# ax2.tick_params(axis='y', labelcolor='black')\n",
    "# ax.legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# ax.set_xlabel('Cluster')\n",
    "# ax.set_ylabel('Average Proportion')\n",
    "# ax.set_title('Average Feature Values and pcs/m by Cluster')\n",
    "# # \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "34e7a5cd-d0dc-4b78-b0ad-4388715f41e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# fig, ax = plt.subplots()\n",
    "# custom_palette = sns.color_palette(\"gist_rainbow\", n_colors=len(cluster_means['variable'].unique()))\n",
    "\n",
    "# ax = sns.barplot(data=cluster_means, x=cluster_means.cluster, y=cluster_means.value, hue=cluster_means.variable, palette=custom_palette)\n",
    "\n",
    "\n",
    "# ax2 = ax.twinx()  # instantiate a second Axes that shares the same x-axis\n",
    "\n",
    "# ax2.set_ylabel('pcs/m', color='black') \n",
    "# sns.scatterplot(data=means, x='cluster', y='rec', ax=ax2)\n",
    "# ax2.tick_params(axis='y', labelcolor='black')\n",
    "# ax.legend(title='Feature', bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "# fig.tight_layout()  # otherwise the right y-label is slightly clipped\n",
    "# ax.set_xlabel('Cluster')\n",
    "# ax.set_ylabel('Average Proportion')\n",
    "# ax.set_title('Average Feature Values and pcs/m by Cluster')\n",
    "# # \n",
    "# plt.tight_layout()\n",
    "# plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b800580c-a171-457c-a9b7-f4e38c5e6646",
   "metadata": {},
   "source": [
    "# Report on Litter Analysis in the Canton of Bern, Switzerland\n",
    "\n",
    "## Abstract\n",
    "\n",
    "This report investigates the distribution and determinants of litter, specifically cigarette ends, snack wrappers, drink straws, and coffee stirrers, along the shores of lakes in the Canton of Bern, Switzerland. The study aims to identify areas that would benefit most from investment in reducing plastic waste, using topographical features as key determinants. Using regression analysis, cluster analysis, and Bayesian methods, we determine the areas with the greatest need and the factors most correlated with litter presence.\n",
    "\n",
    "## Introduction\n",
    "\n",
    "The accumulation of litter in natural environments poses significant ecological and aesthetic challenges. This study focuses on the litter found on the shores of lakes in the Canton of Bern, Switzerland. Our objective is to identify the areas most in need of intervention to reduce plastic and other trash. We examine the relationships between various land use features and the density of litter to prioritize areas for investment. \n",
    "\n",
    "## Methods\n",
    "\n",
    "Data collection followed the protocol outlined in \"A guide for monitoring litter on European seas\" and involved volunteers sampling pieces of trash per meter (pcs/m). The dataset includes land use features within a 1,500-meter buffer zone around the lakes. We conducted cluster and regression analyses, both with and without interaction terms, and utilized a Bayesian approach to predict future trends.\n",
    "\n",
    "## Results\n",
    "\n",
    "### Summary of Sample Totals\n",
    "\n",
    "The dataset recorded a total of 2,299 pieces of trash over 98 samples, with an average of 0.587 pcs/m and a standard deviation of 0.832 pcs/m. The maximum recorded pcs/m was 4.04. The distribution quantiles are as follows: [0.0085, 0.12, 0.25, 0.675, 2.466].\n",
    "\n",
    "### Cluster Analysis\n",
    "\n",
    "Cluster analysis identified three distinct clusters based on the pcs/m and the land use features. Cluster 0, consisting of 7 samples, has an average of 0.235 pcs/m and is characterized by higher forest coverage. Cluster 1, which includes 70 samples, shows an average of 0.377 pcs/m and predominantly covers undefined areas with some buildings and streets. Cluster 2, with 21 samples, has the highest average pcs/m of 1.405 and features high building density along with some forest and undefined areas.\n",
    "\n",
    "| clusters | samples | pcs/m | buildings | forest | undefined | public services |\n",
    "|----------|---------|-------|-----------|--------|-----------|-----------------|\n",
    "| 0        | 7       | 0.235 | 0.162     | 0.553  | 0.186     | 0.002           |\n",
    "| 1        | 70      | 0.377 | 0.248     | 0.199  | 0.536     | 0.008           |\n",
    "| 2        | 21      | 1.405 | 0.586     | 0.292  | 0.119     | 0.039           |\n",
    "\n",
    "### Feature Magnitude and Litter Density\n",
    "\n",
    "The summary of pcs/m by feature magnitude reveals significant variation. Orchards, vineyards, and public services consistently show lower pcs/m values. Buildings show a variable but often higher pcs/m, peaking at 1.83188 pcs/m. Forest areas have moderate pcs/m, with some peaks, while undefined areas exhibit the highest pcs/m values, particularly peaking at 1.20714 pcs/m. Streets are associated with moderate to high pcs/m values.\n",
    "\n",
    "|                 |        1 |        2 |        3 |        4 |    5 |\n",
    "|-----------------|---------:|---------:|---------:|---------:|-----:|\n",
    "| orchards        | 0.587449 | 0        | 0        | 0        | 0    |\n",
    "| vineyards       | 0.587449 | 0        | 0        | 0        | 0    |\n",
    "| buildings       | 0.64125  | 0.199216 | 1.83188  | 0.414    | 0.32 |\n",
    "| forest          | 0.275172 | 0.773226 | 0.235714 | 0        | 0    |\n",
    "| undefined       | 1.14731  | 1.20714  | 0.206415 | 0.695833 | 0    |\n",
    "| public services | 0.587449 | 0        | 0        | 0        | 0    |\n",
    "| streets         | 0.376531 | 0.798367 | 0        | 0        | 0    |\n",
    "\n",
    "### Feature Importance from Regression Analysis\n",
    "\n",
    "Regression analysis without interaction terms showed an R² of 0.63 with a standard error of 0.02. The feature importance ranked streets highest at 31.87%, followed by forest at 24.08%, buildings at 18.37%, public services at 14.31%, and undefined areas at 11.36%. Including interaction terms did not significantly change the importance ranking.\n",
    "\n",
    "| Feature         | Importance |\n",
    "|-----------------|------------|\n",
    "| streets         | 0.318717   |\n",
    "| forest          | 0.24078    |\n",
    "| buildings       | 0.183706   |\n",
    "| public services | 0.143164   |\n",
    "| undefined       | 0.113633   |\n",
    "\n",
    "### Correlation Analysis\n",
    "\n",
    "The correlation matrix reveals significant positive correlations between buildings and public services (0.826) and between streets and public services (0.748). Negative correlations are noted between undefined areas and buildings (-0.701) and between undefined areas and public services (-0.662). These correlations support the regression analysis, where streets and buildings show high importance, aligning with their positive correlation with public services. Undefined areas, negatively correlated with both buildings and public services, also show a lower importance in regression.\n",
    "\n",
    "|                 |   orchards |   vineyards |   buildings |     forest |   undefined |   public services |    streets |\n",
    "|:----------------|-----------:|------------:|------------:|-----------:|------------:|------------------:|-----------:|\n",
    "| orchards        |  1         |  -0.0806167 |   -0.191287 |  0.576411  |  -0.332721  |        -0.215838  | -0.189411  |\n",
    "| vineyards       | -0.0806167 |   1         |   -0.174966 | -0.0712951 |   0.0521685 |        -0.11741   |  0.0314758 |\n",
    "| buildings       | -0.191287  |  -0.174966  |    1        | -0.260137  |  -0.701151  |         0.826351  |  0.556692  |\n",
    "| forest          |  0.576411  |  -0.0712951 |   -0.260137 |  1         |  -0.478371  |        -0.0795163 | -0.0597917 |\n",
    "| undefined       | -0.332721  |   0.0521685 |   -0.701151 | -0.478371  |   1         |        -0.662271  | -0.454093  |\n",
    "| public services | -0.215838  |  -0.11741   |    0.826351 | -0.0795163 |  -0.662271  |         1         |  0.748266  |\n",
    "| streets         | -0.189411  |   0.0314758 |    0.556692 | -0.0597917 |  -0.454093  |         0.748266  |  1         |\n",
    "\n",
    "### Regression Results\n",
    "\n",
    "Without interaction terms, the random forest regression model achieved an R² of 0.63 with a mean squared error (MSE) of 0.02. The predicted sample mean was 0.64, with a 90% interval of [0.126, 0.216, 0.895, 1.892]. When including interaction terms, the R² remained at 0.63, and the MSE was also 0.02, indicating that interaction terms did not significantly improve the model's explanatory power.\n",
    "\n",
    "### Comparison of Predictions\n",
    "\n",
    "The linear regression predicted an average pcs/m of 0.64, while the Bayesian grid approximation predicted an average of 0.85 pcs/m. The 90% intervals for both methods overlap, suggesting reasonable agreement, with Bayesian predictions indicating potentially higher values.\n",
    "\n",
    "## Discussion\n",
    "\n",
    "### Feature Analysis\n",
    "\n",
    "Undefined areas showed high pcs/m values, possibly due to less controlled environments leading to more litter accumulation. Buildings were often associated with higher pcs/m, likely due to higher human activity and resulting litter. Forests had moderate litter levels, potentially due to lower human traffic compared to urban areas. Streets were strongly correlated with high pcs/m, supporting the hypothesis that streets lead to more litter, as confirmed by their importance in the regression model. Public services, although showing lower direct impact, were positively correlated with buildings and streets, indicating higher litter in areas with more infrastructure.\n",
    "\n",
    "### Hypothesis on Streets and Litter\n",
    "\n",
    "The analysis supports the hypothesis that more streets lead to higher pcs/m. Streets have the highest feature importance in the regression model, indicating a strong influence on litter density.\n",
    "\n",
    "### Future Predictions\n",
    "\n",
    "Both linear regression and Bayesian methods suggest that pcs/m will likely remain significant, with potential increases. Future values are expected to be between 0.64 and 0.85 pcs/m, with possible peaks up to 3.02 pcs/m.\n",
    "\n",
    "### Suggestions for Improving Inferences\n",
    "\n",
    "\n",
    "\n",
    "#### Sampling Methods\n",
    "\n",
    "Increased sampling frequency would provide a clearer picture of temporal variations and identify peak littering periods. Stratified sampling, ensuring that samples cover all topographical features proportionally, would improve the representativeness of the data.\n",
    "\n",
    "#### Analytical Methods\n",
    "\n",
    "Utilizing more sophisticated models, such as generalized additive models (GAMs) or machine learning approaches, could capture non-linear relationships more effectively. Exploring more interaction effects and potential non-linearities between features could enhance the understanding of how different land uses influence litter distribution.\n",
    "\n",
    "## Conclusion\n",
    "\n",
    "This study highlights the significant impact of land use features on litter density along the lakeshores of the Canton of Bern. Streets and buildings are the primary contributors to litter, suggesting targeted interventions in urban areas and along roadways could be most effective. The alignment between regression and Bayesian predictions supports the robustness of the findings, providing a solid basis for future environmental management and policy-making."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c34210bd-1bbd-4b23-afe9-d152a040bd24",
   "metadata": {},
   "source": [
    "The dataset records various types of trash (cigarette ends, snack wrappers, drink straws, and coffee stirrers) found on lake shores in the Canton of Bern, Switzerland. The data includes topographical features within a 1,500-meter buffer zone around the lakes, such as the proportion of land occupied by buildings, streets, public services, orchards, vineyards, forests, and undefined areas. The goal is to identify areas that would most benefit from investments aimed at reducing trash levels, defined by the highest pieces of trash per meter (pcs/m).\n",
    "\n",
    "A summary of the trash data reveals a total of 2,299 pieces of trash over 98 samples, averaging 0.587 pcs/m with a standard deviation of 0.832 pcs/m and a maximum of 4.04 pcs/m. Cluster analysis using the elbow point method identified three clusters: Cluster 0 (low pcs/m, higher forest proportion), Cluster 1 (moderate pcs/m, balanced proportions of buildings, forest, and undefined areas), and Cluster 2 (high pcs/m, higher proportions of buildings and public services). Comparing these clusters with the pcs/m by feature magnitude table, it is evident that areas with higher proportions of buildings and undefined areas tend to have higher trash levels, particularly in Cluster 2.\n",
    "\n",
    "Regression analysis without interaction terms revealed a random forest model with an \\( R^2 \\) of 0.63 and a standard error of 0.02. The feature importance showed streets as the most significant predictor of pcs/m, followed by forests, buildings, public services, and undefined areas. This analysis indicates that urban features (streets, buildings) and less natural areas (undefined) have a substantial impact on litter levels. Thus, areas with these features might be prioritized for investment to reduce trash.\n",
    "\n",
    "To test the hypothesis that the presence of streets increases pcs/m when combined with other land uses, a regression analysis with interaction terms was conducted. This model also yielded an \\( R^2 \\) of 0.63 and a mean squared error (MSE) of 0.02. The feature importance rankings remained consistent: streets were the most significant predictor, followed by forests, buildings, public services, and undefined areas.\n",
    "\n",
    "Comparing the results of the regression analysis with and without interaction terms, we observe that the inclusion of interaction terms did not significantly change the model's performance metrics (both had \\( R^2 \\) of 0.63 and similar feature importance rankings). This consistency suggests that while streets are a significant predictor of pcs/m, their effect when combined with other land uses is adequately captured without explicitly modeling interactions.\n",
    "\n",
    "In summary, the combined analysis supports the conclusion that urban features, particularly streets and buildings, along with undefined areas, significantly impact trash levels on lake shores. The cluster analysis and regression results align, indicating that areas with higher proportions of these features experience higher pcs/m. Therefore, targeted investments in such areas, focusing on reducing trash levels, could be most beneficial in addressing environmental pollution. The hypothesis that streets increase pcs/m when combined with other land uses is consistent with these findings, reinforcing the need for comprehensive management strategies in urbanized and mixed-use areas. Undefined usage areas may have higher values for the target variable due to their ambiguous nature and potential for overlapping human activities that are not specifically managed or regulated. These areas might attract more litter due to lack of clear ownership, maintenance, or public awareness, resulting in higher trash accumulation.\n",
    "\n",
    "To improve the quality of data and the validity of the conclusions, sampling strategies should include a more comprehensive and stratified approach. Ensuring that samples are evenly distributed across different types of land use and varying levels of human activity will help in obtaining a more representative dataset. Additionally, increasing the sample size and frequency of data collection can provide a better temporal understanding of trash accumulation patterns. Employing a mixed-method approach that includes both random and systematic sampling can also help in covering areas that might be overlooked by one method alone.\n",
    "\n",
    "Beyond the current analysis, other analytical methods and metrics can be considered. Spatial analysis techniques, such as Geographic Information System (GIS) mapping, can provide visual insights into the distribution of trash and its correlation with land use features. Incorporating temporal trends analysis can help in understanding seasonal variations and long-term patterns in trash accumulation. Additionally, machine learning algorithms beyond random forests, such as gradient boosting machines or neural networks, might capture more complex interactions between features. Introducing metrics like human foot traffic or proximity to recreational areas can also enrich the analysis, providing a more nuanced understanding of the factors contributing to higher pcs/m levels."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d047891-29d4-4a8b-a8d3-9dbb13e584c1",
   "metadata": {},
   "source": [
    "### Explanation of SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "SHAP (SHapley Additive exPlanations) is a unified framework for interpreting the predictions of machine learning models. It leverages concepts from cooperative game theory, particularly the Shapley value, to attribute the contribution of each feature to the model's prediction. SHAP values quantify the impact of each feature on the prediction by considering all possible combinations of features, ensuring a fair distribution of contribution among features. This approach not only identifies the most important features but also explains how changes in feature values influence the model's output, making it a powerful tool for model interpretability and feature importance analysis.\n",
    "\n",
    "### Explanation of Shapley Value\n",
    "\n",
    "In the context of game theory, the Shapley value is a solution concept used to distribute the total gains (or costs) generated by a coalition of players in a way that fairly reflects each player's contribution to the coalition. Developed by Lloyd Shapley in 1953, the Shapley value ensures that the distribution is both fair and equitable by considering every possible permutation of players joining the coalition. For each player, the Shapley value is the average marginal contribution of that player across all permutations. This means it calculates how much value a player adds to each possible coalition they could be a part of, and then averages these values to ensure a fair share.\n",
    "\n",
    "When applied to machine learning and data, Shapley values provide a way to interpret complex model predictions by attributing the contribution of each feature to the final prediction. Each feature is treated as a \"player\" in a coalition, and the Shapley value represents the average contribution of that feature to the model’s prediction over all possible feature combinations. This method allows for a detailed understanding of how each feature influences the outcome, making it a robust tool for model interpretability. By considering all possible interactions among features, Shapley values ensure that the contribution of each feature is fairly assessed, regardless of the presence or absence of other features in the model.\n",
    "\n",
    "### References\n",
    "\n",
    "1. **Shapley, L. S. (1953). A Value for n-Person Games. Contributions to the Theory of Games, vol. 2, 307–317.** - This seminal paper by Lloyd Shapley introduces the Shapley value concept in game theory.\n",
    "   \n",
    "2. **Strumbelj, E., & Kononenko, I. (2014). Explaining Prediction Models and Individual Predictions with Feature Contributions. Knowledge and Information Systems, 41(3), 647-665.** - This paper discusses how Shapley values can be used to interpret predictions in machine learning.\n",
    "\n",
    "3. **Lundberg, S. M., & Lee, S.-I. (2017). A Unified Approach to Interpreting Model Predictions. Advances in Neural Information Processing Systems 30 (NIPS 2017), 4765-4774.** - This paper introduces SHAP (SHapley Additive exPlanations) and demonstrates its application to model interpretability.\n",
    "\n",
    "4. **Molnar, C. (2019). Interpretable Machine Learning. A Guide for Making Black Box Models Explainable.** - This book provides an in-depth look at various methods for interpreting machine learning models, including SHAP.\n",
    "\n",
    "5. **Lipovetsky, S., & Conklin, M. (2001). Analysis of Regression in Game Theory Approach. Applied Stochastic Models in Business and Industry, 17(4), 319-330.** - This paper explores the use of game theory, particularly Shapley values, in the context of regression analysis.\n",
    "\n",
    "### Comparison of SHAP and Partial Dependence Plots (PDP)\n",
    "\n",
    "### SHAP (SHapley Additive exPlanations)\n",
    "\n",
    "#### Purpose:\n",
    "SHAP provides a unified approach to interpreting model predictions by attributing the contribution of each feature to the final prediction. It quantifies how much each feature contributes to the prediction by considering all possible combinations of feature values, ensuring a fair distribution of contributions among features.\n",
    "\n",
    "#### How It Works:\n",
    "- **Game Theory Basis**: SHAP values are based on the Shapley value concept from cooperative game theory, which ensures a fair allocation of the total gain (or prediction) among features.\n",
    "- **Local Interpretability**: SHAP values explain individual predictions by showing the impact of each feature on that specific prediction.\n",
    "- **Global Interpretability**: Aggregating SHAP values across many predictions provides insights into overall feature importance and interactions.\n",
    "- **Fairness**: By considering all possible subsets of features, SHAP values ensure that each feature's contribution is fairly assessed in the presence of other features.\n",
    "\n",
    "#### Strengths:\n",
    "- **Model-Agnostic**: SHAP can be applied to any machine learning model.\n",
    "- **Detailed Explanations**: Provides both local and global interpretability.\n",
    "- **Interaction Effects**: Captures interactions between features.\n",
    "\n",
    "#### Limitations:\n",
    "- **Computationally Intensive**: Calculating SHAP values can be time-consuming, especially for large datasets and complex models.\n",
    "\n",
    "### Partial Dependence Plots (PDP)\n",
    "\n",
    "#### Purpose:\n",
    "PDPs are used to visualize the effect of one or two features on the predicted outcome of a machine learning model. They show the average predicted outcome as a function of the feature(s) of interest, while averaging out the effects of other features.\n",
    "\n",
    "#### How It Works:\n",
    "- **Marginal Effect**: PDPs plot the marginal effect of a feature by keeping other features constant or averaging their effects.\n",
    "- **Global Interpretability**: They provide a global view of how the feature(s) influence predictions across the entire dataset.\n",
    "- **Visualization**: PDPs are typically visualized as line or contour plots (for one or two features).\n",
    "\n",
    "#### Strengths:\n",
    "- **Simplicity**: Easy to interpret and visualize.\n",
    "- **Computationally Efficient**: Less computationally intensive compared to SHAP.\n",
    "- **Clear Effect**: Provides a clear view of the main effect of a feature on the prediction.\n",
    "\n",
    "#### Limitations:\n",
    "- **Assumes Independence**: Assumes that the feature of interest is independent of other features, which might not hold in practice.\n",
    "- **Limited Interaction Insight**: While PDPs can show interaction effects (with two features), they are less comprehensive than SHAP in capturing complex interactions.\n",
    "\n",
    "### Key Differences\n",
    "\n",
    "1. **Interpretability Scope**:\n",
    "   - **SHAP**: Provides both local (individual prediction) and global (overall model behavior) interpretability.\n",
    "   - **PDP**: Primarily provides global interpretability, showing the average effect of a feature across the dataset.\n",
    "\n",
    "2. **Feature Interactions**:\n",
    "   - **SHAP**: Captures complex interactions between features.\n",
    "   - **PDP**: Can capture interactions between two features, but generally assumes feature independence for single-feature plots.\n",
    "\n",
    "3. **Computational Complexity**:\n",
    "   - **SHAP**: More computationally intensive due to the need to consider all possible feature combinations.\n",
    "   - **PDP**: Less computationally intensive, averaging out effects of other features.\n",
    "\n",
    "4. **Model Dependency**:\n",
    "   - **SHAP**: Model-agnostic and can be applied to any machine learning model.\n",
    "   - **PDP**: Typically implemented within specific frameworks like scikit-learn.\n",
    "\n",
    "### Example Use Cases\n",
    "\n",
    "- **SHAP**: Best used when detailed explanations of individual predictions are needed, or when understanding complex feature interactions is crucial.\n",
    "- **PDP**: Useful for gaining a general understanding of how specific features influence predictions on average, especially in simpler models or as an initial step in model interpretation.\n",
    "\n",
    "### Conclusion\n",
    "\n",
    "Both SHAP and PDP are valuable tools for interpreting machine learning models, but they serve different purposes and have different strengths and limitations. SHAP provides a more detailed and comprehensive view, especially useful for understanding individual predictions and feature interactions, while PDP offers a simpler and computationally efficient way to visualize the average effect of features on model predictions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e889ff49-dc97-4754-a6e9-3bd654d459f5",
   "metadata": {},
   "source": [
    "### Relevant Results and Citations from Articles\n",
    "\n",
    "#### 1. **Beach litter distribution and abundance on the coast of Ghana**\n",
    "   - **Results**: \n",
    "     - The study found a significant correlation between higher population densities and increased litter quantities. Industrial activities were also strongly associated with higher litter counts.\n",
    "     - Litter distribution varied along the coast, with hotspots near urban and industrial areas.\n",
    "   - **Citation**: \n",
    "     - Adu-Boahene, S., et al. \"Beach litter distribution and abundance on the coast of Ghana.\" *Marine Pollution Bulletin*, vol. 91, no. 1, 2015, pp. 222-227. doi:10.1016/j.marpolbul.2014.11.017.\n",
    "\n",
    "#### 2. **Assessment of plastic debris on beaches: Implications for marine pollution**\n",
    "   - **Results**: \n",
    "     - Plastic debris was found to be the most prevalent type of litter, with significant quantities near urban areas.\n",
    "     - Regression analysis indicated a strong relationship between proximity to urban areas and higher litter densities.\n",
    "     - Spatial analysis revealed that beaches closer to urban centers had more litter, suggesting urban runoff as a major source.\n",
    "   - **Citation**: \n",
    "     - Barnes, D.K.A., Galgani, F., Thompson, R.C., & Barlaz, M. \"Assessment of plastic debris on beaches: Implications for marine pollution.\" *Philosophical Transactions of the Royal Society B: Biological Sciences*, vol. 364, no. 1526, 2009, pp. 1985-1998. doi:10.1098/rstb.2008.0305.\n",
    "\n",
    "#### 3. **Quantification and source identification of marine litter on beaches along the southeastern coast of South Korea**\n",
    "   - **Results**: \n",
    "     - Urban areas and tourism activities were identified as significant contributors to marine litter.\n",
    "     - Correlation analysis showed a positive relationship between the number of visitors to beaches and the quantity of litter.\n",
    "     - Regression models indicated that proximity to rivers and urban areas significantly increased litter quantities.\n",
    "   - **Citation**: \n",
    "     - Lee, J., Hong, S., & Lee, J. \"Quantification and source identification of marine litter on beaches along the southeastern coast of South Korea.\" *Marine Pollution Bulletin*, vol. 62, no. 6, 2011, pp. 1308-1316. doi:10.1016/j.marpolbul.2011.04.021.\n",
    "\n",
    "These summaries and citations should provide you with the relevant information to quote in your analysis. If you need further details or assistance with anything else, feel free to ask!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03850811-9532-4cc2-a133-4e991f689d41",
   "metadata": {},
   "source": [
    "### Improved Approach: Incorporating Interaction Terms from the Beginning\n",
    "\n",
    "Given that we know interactions between variables are crucial, we should account for them early in the analysis process. Here’s an improved step-by-step approach:\n",
    "\n",
    "1. **Exploratory Data Analysis (EDA)**\n",
    "   - **Descriptive Statistics**: Summarize the data to understand the basic properties of each feature.\n",
    "   - **Correlation Matrix**: Calculate correlations between features to identify potential interactions and multicollinearity.\n",
    "   - **Visualizations**: Use scatter plots, pair plots, and heatmaps to visualize relationships and potential interactions between variables.\n",
    "\n",
    "2. **Feature Scaling**\n",
    "   - **Normalization**: Scale all features to a range of 0 to 1 using Min-Max scaling. This ensures that each feature contributes proportionately to the analysis.\n",
    "   - **Separate Scaling for Streets**: Since \"streets\" is already scaled between 0 and 1, ensure it remains unchanged.\n",
    "\n",
    "3. **Identifying and Creating Interaction Terms**\n",
    "   - **Correlation Matrix Analysis**: Use the correlation matrix to identify pairs of features with high correlation that may interact.\n",
    "   - **Domain Knowledge**: Use domain knowledge to hypothesize interactions (e.g., interaction between streets and buildings).\n",
    "   - **Create Interaction Terms**: Explicitly create interaction terms (e.g., `streets * buildings`) in the dataset before fitting the model.\n",
    "\n",
    "4. **Initial Model Fitting**\n",
    "   - **Model Selection**: Start with models suitable for count data, such as Poisson Regression and Negative Binomial Regression.\n",
    "   - **Include Interaction Terms**: Incorporate identified interaction terms in the initial models.\n",
    "   - **Assess Multicollinearity**: Use VIF to check for multicollinearity after adding interaction terms.\n",
    "\n",
    "5. **Model Comparison and Evaluation**\n",
    "   - **Goodness of Fit**: Compare models using AIC, BIC, and deviance.\n",
    "   - **Cross-Validation**: Perform cross-validation to ensure the models generalize well to unseen data.\n",
    "   - **Residual Analysis**: Examine residuals to verify model assumptions and identify any remaining patterns.\n",
    "\n",
    "6. **Refinement and Regularization**\n",
    "   - **Regularization Techniques**: Apply Ridge or Lasso regression if multicollinearity is an issue.\n",
    "   - **Feature Selection**: Use regularization to identify and retain the most important features and interaction terms.\n",
    "\n",
    "7. **Advanced Modeling**\n",
    "   - **Generalized Additive Models (GAMs)**: Explore GAMs to capture non-linear relationships and interactions.\n",
    "   - **Tree-Based Methods**: Use Random Forests or Gradient Boosting to capture complex interactions and non-linearities.\n",
    "\n",
    "8. **Feature Importance and Interpretation**\n",
    "   - **Coefficient Analysis**: Examine the coefficients of the regression models to understand the impact of each feature and interaction term.\n",
    "   - **Variable Importance Metrics**: Use feature importance scores from tree-based models.\n",
    "   - **Partial Dependence Plots**: Visualize the effect of individual features and interactions on the predicted outcome.\n",
    "\n",
    "9. **Clustering and Group Analysis**\n",
    "   - **Clustering Techniques**: Use K-Means or Hierarchical Clustering to group similar locations based on feature variables and interaction terms.\n",
    "   - **Cluster Analysis**: Compare `pcs/m` across clusters to identify high-risk and low-risk groups.\n",
    "   - **Descriptive Statistics**: Summarize `pcs/m` within each cluster to understand the conditions associated with high and low litter counts.\n",
    "\n",
    "### Summary\n",
    "\n",
    "1. **Start with EDA**: Identify correlations and potential interactions early using the correlation matrix and visualizations.\n",
    "2. **Feature Scaling**: Normalize features appropriately.\n",
    "3. **Create Interaction Terms**: Generate interaction terms based on correlation analysis and domain knowledge before model fitting.\n",
    "4. **Initial Model Fitting**: Fit models including interaction terms from the start.\n",
    "5. **Evaluate and Refine Models**: Use goodness-of-fit metrics, cross-validation, and regularization to refine models.\n",
    "6. **Advanced Modeling**: Explore GAMs and tree-based methods for capturing complex interactions.\n",
    "7. **Interpret Results**: Analyze feature importance and interactions to understand their impact on `pcs/m`.\n",
    "8. **Group Analysis**: Use clustering to identify and analyze similar locations.\n",
    "\n",
    "This approach ensures that interaction effects are incorporated and assessed from the beginning, leading to a more comprehensive understanding of the factors affecting litter counts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a44ba3f-8372-451f-afa4-71078100924e",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.19"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
