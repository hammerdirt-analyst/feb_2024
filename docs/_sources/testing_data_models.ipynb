{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c16819-72da-4b47-a3aa-988d3f5a8203",
   "metadata": {},
   "source": [
    "%load_ext watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Type, Optional, Callable\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from review_methods_tests import collect_vitals, find_missing, find_missing_loc_dates\n",
    "from review_methods_tests import use_gfrags_gfoams_gcaps, make_a_summary,combine_survey_files\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "import matplotlib.colors\n",
    "from matplotlib.colors import LinearSegmentedColormap, ListedColormap\n",
    "\n",
    "from setvariables import *"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4558dc36-9186-4446-a24e-22cd22678f63",
   "metadata": {
    "editable": true,
    "jupyter": {
     "source_hidden": true
    },
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "def slice_data_by_date(data: pd.DataFrame, start: str, end: str):\n",
    "    mask = (data.date >= start) & (data.date <= end)\n",
    "    return data[mask]\n",
    "\n",
    "top = mpl.colormaps['Oranges'].resampled(2000)\n",
    "bottom = mpl.colormaps['Greys'].resampled(2000)\n",
    "\n",
    "newcolors = np.vstack((bottom(np.linspace(0, 1, 2000)),\n",
    "                   top(np.linspace(0, 1, 2000))))\n",
    "newcmp = ListedColormap(newcolors, name='OrangeBlue')\n",
    "\n",
    "my_cmap = newcmp(np.arange(newcmp.N))\n",
    "my_cmap[:, -1] = np.linspace(0, 1, newcmp.N)\n",
    "newcmp = ListedColormap(my_cmap)\n",
    "\n",
    "format_kwargs = dict(precision=2, thousands=\"'\", decimal=\",\")\n",
    "\n",
    "# this defines the css rules for the note-book table displays\n",
    "header_row = {'selector': 'th:nth-child(1)', 'props': f'background-color: #FFF; font-size:12px; text-align:left;'}\n",
    "even_rows = {\"selector\": 'tr:nth-child(even)', 'props': f'background-color: rgba(139, 69, 19, 0.08);'}\n",
    "odd_rows = {'selector': 'tr:nth-child(odd)', 'props': 'background: #FFF;'}\n",
    "table_font = {'selector': 'tr', 'props': 'font-size: 10px;'}\n",
    "table_data = {'selector': 'td', 'props': 'padding:4px; font-size:12px;'}\n",
    "table_css_styles = [even_rows, odd_rows, table_font, header_row, table_data]\n",
    "\n",
    "def aggregate_dataframe(df: pd.DataFrame,\n",
    "                        groupby_columns: List[str],\n",
    "                        aggregation_functions: Dict[str, Union[str, callable]],\n",
    "                        index: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate specified columns in a Pandas DataFrame using given aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        groupby_columns (List[str]): List of column names to group by.\n",
    "        aggregation_functions (Dict[str, Union[str, callable]]): \n",
    "            A dictionary where keys are column names to aggregate, \n",
    "            and values are either aggregation functions (e.g., 'sum', 'mean', 'max', 'min')\n",
    "            or custom aggregation functions (callable functions).\n",
    "        index (bool, optional): Whether to use the groupby columns as an index.\n",
    "            Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with aggregated values.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(groupby_columns, as_index=index).agg(aggregation_functions)\n",
    "    \n",
    "    return grouped\n",
    "    \n",
    "def merge_dataframes_on_column_and_index(left_df: pd.DataFrame,\n",
    "                                         right_df: pd.DataFrame,\n",
    "                                         left_column: str,\n",
    "                                         how: str = 'inner',\n",
    "                                         validate: str = 'many_to_one') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge two DataFrames where the left DataFrame is merged on a specified column and \n",
    "    the right DataFrame is merged on its index.\n",
    "\n",
    "    Args:\n",
    "        left_df (pd.DataFrame): The left DataFrame to be merged.\n",
    "        right_df (pd.DataFrame): The right DataFrame to be merged on its index.\n",
    "        left_column (str): The column in the left DataFrame to merge on.\n",
    "        how (str, optional): The type of merge to be performed ('left', 'right', 'outer', or 'inner'). \n",
    "            Default is 'inner'.\n",
    "        validate (str, optional): Whether to perform merge validation checks. \n",
    "            Default is 'many_to_one'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame resulting from the merge operation.\n",
    "    \"\"\"\n",
    "  \n",
    "    merged_df = left_df.merge(right_df, left_on=left_column, right_index=True, how=how)\n",
    "    return merged_df\n",
    "\n",
    "def get_top_x_records_with_max_quantity(df: pd.DataFrame, quantity_column: str, id_column: str, x: int):\n",
    "    \"\"\"\n",
    "    Get the top x records with the greatest quantity and their proportion to the total from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        quantity_column (str): The name of the quantity column.\n",
    "        id_column (str): The name of the ID column.\n",
    "        x (int): The number of records to return.\n",
    "\n",
    "    Returns:\n",
    "        A data frame\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by the quantity column in descending order, take the top x records, and select the ID column\n",
    "    top_x_records = df.nlargest(x, quantity_column)[[id_column, quantity_column]]\n",
    "    top_x_records[\"%\"] = top_x_records[quantity_column]/top_x_records[quantity_column].sum()\n",
    "    \n",
    "    return top_x_records[[id_column, quantity_column, \"%\"]]\n",
    "\n",
    "\n",
    "\n",
    "def calculate_rate_per_unit(df: pd.DataFrame,\n",
    "                            objects_to_check: List[str],\n",
    "                            column_of_interest: str = \"code\",\n",
    "                            groupby_columns: List[str] = ['code'],\n",
    "                            method: Dict[str, str] = {\"pcs_m\": \"median\", \"quantity\":\"sum\"},\n",
    "                            )-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate the rate of occurence of object(s) for a given unit measurement. Adds the label\n",
    "    'all' to each record.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "        objects_to_check (List[str]): The list of objects to calculate proportions for.\n",
    "        column_of_interest (str): The column label of the objects being compared.\n",
    "        groupby_columns Dict[str]: The columns used for the aggregation.\n",
    "        method (Dict[str]): Dictionary specifying the aggregation functions for the unit_measurement.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A dataframe where index is column_of_interest and the value column is the rate\n",
    "            and the label is 'all'.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where 'object' is in 'objects_to_check'\n",
    "    filtered_df = df[df[column_of_interest].isin(objects_to_check)]\n",
    "\n",
    "    # Calculate the total quantity for each object\n",
    "    object_rates = filtered_df.groupby(groupby_columns, as_index=False).agg(method)\n",
    "\n",
    "    # Calculate the proportion for each object\n",
    "    rates = object_rates[[column_of_interest, *method.keys()]].set_index(column_of_interest, drop=True)\n",
    "    rates[\"label\"] = \"all\"    \n",
    "\n",
    "    return rates\n",
    "\n",
    "\n",
    "# fail rate: quantity > 0\n",
    "def count_objects_with_positive_quantity(df: pd.DataFrame, value_column: str = 'quantity', object_column: str = 'code') -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count how many times each object had a quantity greater than zero in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the count of positive quantity occurrences for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where quantity is greater than zero\n",
    "    positive_quantity_df = df[df[value_column] > 0]\n",
    "    no_count_df = df[(df[value_column] == 0)]\n",
    "\n",
    "    # Count the occurrences of positive quantities for each object\n",
    "    object_counts = positive_quantity_df[object_column].value_counts()\n",
    "    failed = object_counts/df.loc_date.nunique()\n",
    "\n",
    "    # identify the objects with a zero count\n",
    "    no_counts = no_count_df[object_column].value_counts()\n",
    "    zeroes = no_counts[~no_counts.index.isin(object_counts.index)]\n",
    "    zeroes.loc[:] = 0\n",
    "\n",
    "    return pd.concat([failed, zeroes])\n",
    "\n",
    "# pieces per merter for a set of data\n",
    "def rate_per_unit_cumulative(df: pd.DataFrame, groupby_columns: List, object_labels: List, objects: List, agg_methods: Dict)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculate cumulative rates per unit for specific objects and aggregation methods.\n",
    "\n",
    "    This function takes a DataFrame and calculates cumulative rates per unit based on\n",
    "    the specified groupby columns, object labels, objects of interest, and aggregation methods.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data for analysis.\n",
    "        groupby_columns (List): List of columns to group by in the DataFrame.\n",
    "        object_labels (List): List of labels to identify objects of interest.\n",
    "        objects (List): List of objects for which cumulative rates are calculated.\n",
    "        agg_methods (Dict): Dictionary specifying aggregation methods for calculating rates.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing the cumulative rates per unit.\n",
    "\n",
    "    Example:\n",
    "        groupby_columns = ['Region', 'Year']\n",
    "        object_labels = ['Object A', 'Object B']\n",
    "        objects = ['A', 'B']\n",
    "        agg_methods = {'Value': 'sum', 'Count': 'count'}\n",
    "\n",
    "        cumulative_rates = rate_per_unit_cumulative(df, groupby_columns, object_labels, objects, agg_methods)\n",
    "    \"\"\"\n",
    "    parent_summary = aggregate_dataframe(df, groupby_columns, agg_methods)\n",
    "    parent_boundary_summary = calculate_rate_per_unit(parent_summary, object_labels, objects[0], objects)\n",
    "    parent_boundary_summary.reset_index(drop=False, inplace=True)\n",
    "\n",
    "    return parent_boundary_summary\n",
    "\n",
    "\n",
    "def aggregate_boundaries(df: pd.DataFrame, unit_columns: list, unit_agg: dict, boundary_labels: list, boundary_columns: list, group_agg: dict)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate data from a dataframe by boundaries and groups.\n",
    "\n",
    "    Aggregates a dataframe in two steps. First, it performs\n",
    "    aggregation at the 'unit' level defined by 'unit_columns' and 'unit_agg' to obtain\n",
    "    test statistics. Then, it aggregates these 'unit' statistics further at the\n",
    "    'boundary' level defined by 'boundary_labels' and 'boundary_columns', and computes\n",
    "    the test statistics for each boundary.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data to be aggregated.\n",
    "        unit_columns (list): List of columns for 'unit' level aggregation.\n",
    "        unit_agg (dict): Dictionary specifying the aggregation functions for 'unit' level.\n",
    "        boundary_labels (list): List of boundary labels to define 'boundaries' for further aggregation.\n",
    "        boundary_columns (list): List of columns for 'boundary' level aggregation.\n",
    "        group_agg (dict): Dictionary specifying the aggregation functions for 'boundary' level.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame containing aggregated data at the 'boundary' level with\n",
    "        additional 'label' column indicating the boundary label. \n",
    "    \"\"\"\n",
    "    \n",
    "\n",
    "    unit_aggregate = aggregate_dataframe(df, unit_columns, unit_agg)\n",
    "    boundary_summaries = []\n",
    "    for label in boundary_labels:\n",
    "        boundary_mask = unit_aggregate[unit_columns[0]] == label\n",
    "        boundary_aggregate = unit_aggregate[boundary_mask].groupby(boundary_columns, as_index=False).agg(agg_groups)\n",
    "        boundary_aggregate['label'] = label\n",
    "        boundary_summaries.append(boundary_aggregate)\n",
    "\n",
    "    return pd.concat(boundary_summaries)\n",
    "\n",
    "def color_gradient(val, cmap: ListedColormap = None, min: float = 0.0, max: float = .9):\n",
    "    \"\"\"\n",
    "    Apply a color gradient to a numerical value for cell styling.\n",
    "\n",
    "    This function takes a numerical value 'val' and applies a color gradient based on the provided\n",
    "    colormap ('cmap') and the specified range defined by 'min' and 'max'. It returns a CSS style\n",
    "    for cell background color.\n",
    "\n",
    "    Args:\n",
    "        val (float): The numerical value to be colored using the gradient.\n",
    "        cmap (ListedColormap, optional): The colormap to use for the color gradient. Defaults to None.\n",
    "        min (float, optional): The minimum value of the data range. Defaults to 0.0.\n",
    "        max (float, optional): The maximum value of the data range. Defaults to 1.0.\n",
    "\n",
    "    Returns:\n",
    "        str: A CSS style string for cell background color and text color.\n",
    "\n",
    "    Example:\n",
    "        # Apply a color gradient using a custom colormap 'cmap' to the DataFrame\n",
    "        df.style.applymap(color_gradient, cmap=my_colormap, min=0.0, max=100.0)\n",
    "    \"\"\"\n",
    "    # print(type(cmap))\n",
    "    # Normalize the value to a range [0, 1] \n",
    "    # min, max should be the min max for the\n",
    "    # data frame in question\n",
    "    # print(type(val))\n",
    "    normalized_val = (val - min) / max\n",
    "    \n",
    "    r, g, b, a = cmap(normalized_val)\n",
    "\n",
    "    \n",
    "    # Calculate the color based on the normalized value    \n",
    "    hex_color = f\"rgba({int(r*255)},{int(g*255)},{int(b*255)}, .5)\"\n",
    "    \n",
    "       \n",
    "    # Return the CSS style with the background color\n",
    "    return f'background-color: {hex_color}; color:black'\n",
    "\n",
    "def apply_gradient_style(df, gradient_func: Callable = None, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply a custom gradient style to a DataFrame using a specified gradient function.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        gradient_func (callable): A function that takes a value and returns CSS styles for styling.\n",
    "\n",
    "    Returns:\n",
    "        pd.io.formats.style.Styler: A styled representation of the DataFrame.\n",
    "\n",
    "    Example:\n",
    "        def gradient(val):\n",
    "            cmap = 'coolwarm'\n",
    "            vmin = df.values.min().min()\n",
    "            vmax = df.values.max().max()\n",
    "            norm = plt.Normalize(vmin, vmax)\n",
    "            colormap = plt.cm.get_cmap(cmap)\n",
    "            color = colormap(norm(val))\n",
    "            hex_color = \"#{:02x}{:02x}{:02x}\".format(int(color[0] * 255), int(color[1] * 255), int(color[2] * 255))\n",
    "            return f'background-color: {hex_color}'\n",
    "\n",
    "        styled_df = apply_gradient_style(df, gradient)\n",
    "    \"\"\"\n",
    "    # Use the .style.applymap method to apply the gradient function to each cell in the DataFrame\n",
    "    styled_df = df.style.applymap(gradient_func, **kwargs)\n",
    "\n",
    "    return styled_df\n",
    "    \n",
    "def apply_table_style_format(df, style: List = None, format_kwargs: Dict = None):\n",
    "    \n",
    "       \n",
    "    return df.style.set_table_styles(style).format(**format_kwargs)\n",
    "\n",
    "# a decorator for data frames\n",
    "def apply_style_to_dataframe(mymethod, **kwargs):\n",
    "    \"\"\"\n",
    "    Apply styling to a DataFrame using a custom method or gradient function.\n",
    "    \n",
    "    This decorator function allows you to apply styling to a DataFrame returned by a function.\n",
    "    It takes a custom method and a gradient function as input and applies these functions to\n",
    "    the DataFrame to style it based on specified criteria.\n",
    "    \n",
    "    Args:\n",
    "    mymethod (callable): A custom method that applies styling to the DataFrame.\n",
    "    gradient_func (callable): A function that defines a color gradient for styling.\n",
    "    **kwargs: Additional keyword arguments to pass to the custom method.\n",
    "    \n",
    "    Returns:\n",
    "    callable: A decorator function that can be used to style a DataFrame returned by a function.\n",
    "    \n",
    "    Example:\n",
    "    def my_custom_styling(df, gradient_func, **kwargs):\n",
    "        # Apply styling to the DataFrame using the gradient function and optional kwargs\n",
    "        styled_df = df.style.applymap(gradient_func, **kwargs)\n",
    "        return styled_df\n",
    "    \n",
    "    @apply_style_to_dataframe(my_custom_styling, my_gradient_function, colormap='coolwarm')\n",
    "    def create_dataframe():\n",
    "        # Create and return a DataFrame\n",
    "        ...\n",
    "    \n",
    "    styled_result = create_dataframe()\n",
    "    \"\"\"\n",
    "    def decorator(function):\n",
    "    \n",
    "        def wrapper(*args):\n",
    "            # Call the original function to get the DataFrame\n",
    "            df = function(*args)\n",
    "                                 \n",
    "            return mymethod(df,**kwargs)\n",
    "        \n",
    "        return wrapper\n",
    "    return decorator\n",
    "\n",
    "\n",
    "# @apply_style_to_dataframe(apply_gradient_style, gradient_func=color_gradient, min=0, max=1, cmap=newcmp)\n",
    "def boundary_summary(parent_boundary: pd.DataFrame, boundary_summary: pd.DataFrame, object_columns: List, unit: str)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Create a boundary summary DataFrame based on parent and boundary summaries.\n",
    "\n",
    "    This function combines parent and boundary summaries to create a consolidated boundary summary\n",
    "    DataFrame. The aggregation is based on the specified object columns and the 'unit' of interest.\n",
    "\n",
    "    Args:\n",
    "        parent_boundary (pd.DataFrame): The parent boundary summary DataFrame.\n",
    "        boundary_summaries (pd.DataFrame): The boundary summaries for individual objects.\n",
    "        object_columns (List): List of columns identifying the objects.\n",
    "        unit (str): The unit of interest for aggregation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A boundary summary DataFrame that combines parent and individual object summaries.\n",
    "\n",
    "    Example:\n",
    "        parent_boundary = ...\n",
    "        boundary_summaries = ...\n",
    "        object_columns = ['Object']\n",
    "        unit = 'pcs_m'\n",
    "\n",
    "        boundary_result = boundary_summary(parent_boundary, boundary_summaries, object_columns, unit)\n",
    "    \"\"\"\n",
    "    boundary_limits = pd.concat([parent_boundary, boundary_summary])\n",
    "    objects = boundary_limits[object_columns[0]].nunique()\n",
    "    boundaries = boundary_limits.label.nunique()\n",
    "    if objects >= boundaries:\n",
    "        b = boundary_limits.pivot(index=object_columns[0], columns=\"label\", values=unit)\n",
    "        b = b[[*boundary_summary.label.unique(), *parent_boundary.label.unique()]]\n",
    "    else:\n",
    "        b = boundary_limits.pivot(columns=object_columns[0], index=\"label\", values=unit)\n",
    "        \n",
    "    return b\n",
    "\n",
    "\n",
    "def translate_word(X: str, map: pd.DataFrame, lan: str):\n",
    "    \"\"\"\n",
    "    Translate a word or phrase using a language mapping DataFrame.\n",
    "\n",
    "    This function takes a word or phrase 'X' and attempts to translate it into another language\n",
    "    specified by 'lan' using a language mapping DataFrame 'map'. If the word is found in the index\n",
    "    of the mapping DataFrame, the translation is returned; otherwise, the original word is returned.\n",
    "\n",
    "    Args:\n",
    "        X (str): The word or phrase to be translated.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "\n",
    "    Returns:\n",
    "        str: The translated word or phrase, or the original word if not found in the mapping.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame for language mapping\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana', 'cherry'],\n",
    "                                    'French': ['pomme', 'banane', 'cerise']})\n",
    "\n",
    "        # Translate a word into French\n",
    "        translated_word = translate_word('apple', language_map, 'French')\n",
    "    \"\"\"    \n",
    "    if X in map.index:\n",
    "        return map.loc[X,  lan]\n",
    "    else:\n",
    "        return X\n",
    "\n",
    "def capitalize_index(X):\n",
    "    return X.title()\n",
    "\n",
    "def translate_for_display(df: pd.DataFrame, map: pd.DataFrame, lan: str):\n",
    "    \"\"\"\n",
    "    Translate column names and index labels of a DataFrame for display.\n",
    "\n",
    "    This function takes a DataFrame 'df' and translates its column names and index labels using a\n",
    "    language mapping DataFrame 'map' for display in a specified language 'lan'. The translated\n",
    "    column names are used as new column names in the DataFrame, and the index labels are replaced\n",
    "    with their translations.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame for translation.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The DataFrame with translated column names and index labels for display.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame to be translated\n",
    "        data = {'apple': [1, 2, 3], 'banana': [4, 5, 6]}\n",
    "        original_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create a language mapping DataFrame\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana'],\n",
    "                                    'French': ['pomme', 'banane']})\n",
    "\n",
    "        # Translate the column names and index labels for display in French\n",
    "        translated_df = translate_for_display(original_df, language_map, 'French')\n",
    "    \"\"\"\n",
    "    \n",
    "    old_columns = df.columns    \n",
    "    changed_c = [x for x in old_columns if x in map.index]\n",
    "        \n",
    "    new_columns = {x: map.loc[x, lan] for x in changed_c}\n",
    "    df.rename(columns=new_columns, inplace=True)\n",
    "    \n",
    "    new_index = {x:translate_word(x, map, lan) for x in df.index}\n",
    "    df['new_index'] = df.index.map(lambda x: new_index[x])\n",
    "    df.set_index('new_index', drop=True, inplace=True)\n",
    "    df.index.name = None\n",
    "    df.columns.name = None\n",
    "    \n",
    "    return df\n",
    "\n",
    "def translated_and_style_for_display(df, map, lan, gradient: bool = True):\n",
    "    \"\"\"\n",
    "    Translate, style, and format a DataFrame for display.\n",
    "\n",
    "    This function translates column names and index labels, applies styling, and optionally\n",
    "    adds a color gradient to a DataFrame to prepare it for display in a specified language 'lan'.\n",
    "    \n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame to be translated and styled.\n",
    "        map (pd.DataFrame): A DataFrame containing language mappings.\n",
    "        lan (str): The target language code for translation.\n",
    "        gradient (bool, optional): Whether to apply a color gradient to the DataFrame. Defaults to True.\n",
    "\n",
    "    Returns:\n",
    "        Styler: A styled DataFrame ready for display with translated labels and styling.\n",
    "\n",
    "    Example:\n",
    "        # Create a DataFrame to be translated and styled\n",
    "        data = {'apple': [1, 2, 3], 'banana': [4, 5, 6]}\n",
    "        original_df = pd.DataFrame(data)\n",
    "\n",
    "        # Create a language mapping DataFrame\n",
    "        language_map = pd.DataFrame({'English': ['apple', 'banana'],\n",
    "                                    'French': ['pomme', 'banane']})\n",
    "\n",
    "        # Translate, style, and format the DataFrame for display in French\n",
    "        styled_df = translated_and_style_for_display(original_df, language_map, 'French', gradient=True)\n",
    "    \"\"\"\n",
    "    d = translate_for_display(df, map, lan)\n",
    "    d = d.style.format(**format_kwargs).set_table_styles(table_css_styles)\n",
    "    if gradient:\n",
    "        d = d.applymap(color_gradient, cmap=newcmp)\n",
    "    return d.format_index(str.title, axis=1).format_index(str.title, axis=0)\n",
    "\n",
    "\n",
    "# @apply_style_to_dataframe(apply_table_style_format, style=table_css_styles, format_kwargs=format_kwargs)\n",
    "def display_tabular_data_by_column_values(df, column_one: dict, column_two: dict, index: str):\n",
    "    \"\"\"\n",
    "    Display tabular data based on column values.\n",
    "\n",
    "    This function filters a DataFrame 'df' to include rows where either 'column_one' or 'column_two'\n",
    "    meet specified conditions. The resulting DataFrame is then set to have 'index' as the index, and\n",
    "    the index name is removed for cleaner tabular display.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing tabular data.\n",
    "        column_one (dict): A dictionary specifying the column and value condition for 'column_one'.\n",
    "        column_two (dict): A dictionary specifying the column and value condition for 'column_two'.\n",
    "        index (str): The column to be set as the index for the resulting DataFrame.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: The filtered DataFrame with 'index' as the index and the index name removed.\n",
    "\n",
    "    Example:\n",
    "        # Create a sample DataFrame 'data_df'\n",
    "        data_df = pd.DataFrame({'Name': ['Alice', 'Bob', 'Charlie'],\n",
    "                                'Age': [25, 30, 35],\n",
    "                                'Salary': [50000, 60000, 70000]})\n",
    "\n",
    "        # Define filtering conditions for 'Age' and 'Salary'\n",
    "        column_one = {'column': 'Age', 'val': 30}\n",
    "        column_two = {'column': 'Salary', 'val': 65000}\n",
    "\n",
    "        # Display filtered tabular data by 'Name' where either 'Age' or 'Salary' meets the conditions\n",
    "        filtered_data = display_tabular_data_by_column_values(data_df, column_one, column_two, 'Name')\n",
    "    \"\"\"\n",
    "    d = df[(df[column_one[\"column\"]] >= column_one[\"val\"]) | (df[column_two[\"column\"]] >= column_two[\"val\"])].copy()\n",
    "    d.set_index(index, inplace=True, drop=True)\n",
    "    d.index.name = None       \n",
    "    return d\n",
    "\n",
    "\n",
    "\n",
    "def summary_of_parent_and_child_features(df: pd.DataFrame,\n",
    "                                         cumulative_columns: List = None,\n",
    "                                         boundary_labels: List = None,\n",
    "                                         object_labels: List = None,\n",
    "                                         object_columns: List = None,\n",
    "                                         unit_agg: dict = None,\n",
    "                                         unit_columns: List = None,\n",
    "                                         agg_groups: dict = None)-> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Generate a summary of parent and child features based on a DataFrame.\n",
    "\n",
    "    This function computes a summary of parent and child features based on the provided DataFrame 'df'.\n",
    "    It calculates cumulative values, aggregates boundary summaries, and generates a comprehensive summary\n",
    "    DataFrame that includes both parent and child features.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame containing data for analysis.\n",
    "        cumulative_columns (List, optional): List of columns to be considered for cumulative values.\n",
    "        boundary_labels (List, optional): List of labels for boundary summaries.\n",
    "        object_labels (List, optional): List of labels for individual objects.\n",
    "        object_columns (List, optional): List of columns identifying objects.\n",
    "        unit_agg (dict, optional): Aggregation methods for unit summaries.\n",
    "        unit_columns (List, optional): List of columns for unit summaries.\n",
    "        agg_groups (dict, optional): Aggregation methods for boundary summaries.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A summary of parent and child features with comprehensive information.\n",
    "\n",
    "    Example:\n",
    "        # Define parameters for generating the summary\n",
    "        cumulative_columns = ['quantity', 'total_weight']\n",
    "        boundary_labels = ['Boundary 1', 'Boundary 2']\n",
    "        object_labels = ['Object 1', 'Object 2']\n",
    "        object_columns = ['object_id', 'object_name']\n",
    "        unit_agg = {'quantity': 'sum', 'total_weight': 'mean'}\n",
    "        unit_columns = ['unit_id', 'unit_name']\n",
    "        agg_groups = {'quantity': 'sum', 'total_weight': 'mean'}\n",
    "\n",
    "        # Generate the summary of parent and child features\n",
    "        summary_df = summary_of_parent_and_child_features(data_df, cumulative_columns, boundary_labels,\n",
    "                                                         object_labels, object_columns, unit_agg, unit_columns, agg_groups)\n",
    "    \"\"\"\n",
    "                                            \n",
    "\n",
    "    parent_boundary = rate_per_unit_cumulative(df, cumulative_columns, object_labels, object_columns, unit_agg)\n",
    "    boundary_summaries = aggregate_boundaries(df, unit_columns, unit_agg, boundary_labels, object_columns, agg_groups)\n",
    "    x = boundary_summary(parent_boundary, boundary_summaries, object_columns, \"pcs_m\")\n",
    "\n",
    "    return x\n",
    "\n",
    "def a_summary_of_one_vector(df, unit_columns, unit_agg, describe='pcs_m'):\n",
    "    \n",
    "    sample_totals = aggregate_dataframe(code_result_df.copy(), unit_columns, unit_agg)\n",
    "    sample_summary = sample_totals[describe].describe()\n",
    "    sample_summary[\"total\"] = sample_totals.quantity.sum()\n",
    "    sample_summary = pd.DataFrame(sample_summary)\n",
    "    sample_summary[describe] = sample_summary[describe].astype(object)\n",
    "    sample_summary.loc['count', describe] = int(sample_summary.loc['count', describe])\n",
    "    sample_summary.loc['total', describe] = int(sample_summary.loc['total',describe])\n",
    "    \n",
    "    return sample_summary\n",
    "\n",
    "def summarize_work_data(df, labels):\n",
    "    dfs = []\n",
    "    for alabel in labels:\n",
    "        data = df[df.canton == alabel].copy()\n",
    "        n_rivers = data[data.feature_type == 'r'].feature_name.nunique()\n",
    "        n_lakes = data[data.feature_type == 'l'].feature_name.nunique()\n",
    "        n_parks = data[data.feature_type == 'p'].feature_name.nunique()\n",
    "        n_cities = data.city.nunique()\n",
    "        n_samples = data.loc_date.nunique()\n",
    "        total = data.quantity.sum()\n",
    "        big_picture = {alabel: {'rivers':n_rivers, 'lakes':n_lakes, 'parks': n_parks, 'cities': n_cities, 'samples': n_samples, 'quantity': total}}\n",
    "        dfs.append(pd.DataFrame(big_picture))\n",
    "    return pd.concat(dfs, axis=1).T"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "7bca0862-a1e5-4fd3-ae17-65ac2fd7cd0a",
   "metadata": {},
   "source": [
    "# Testing data models\n",
    "\n",
    "The methods used in the version of the federal report were tested, but their was not a specific set of validation criteria beforehand. Test were done as the work progressed. This wasted alot of time\n",
    "\n",
    "here we test the land use and survey data models.\n",
    "\n",
    "1. is the land use data complete for each survey location?\n",
    "2. does the survey data aggregate correctly to sample level?\n",
    "   * what happens to objects with a quantity of zero?\n",
    "   * aggregating to cantonal, municipal or survey area\n",
    "     * are all locations included?\n",
    "     * are lakes and rivers distinguished?\n",
    "3. Does the aggregated data for iqaasl match the federal report?\n",
    "\n",
    "### Gfoams, Gfrags, Gcaps\n",
    "\n",
    "These are aggregate groups. It is difficult to infer how well a participant differentiates between size or use of the following codes.\n",
    "\n",
    "1. Gfrags: G79, G78, G75\n",
    "2. Gfoams: G81, G82, G76\n",
    "3. Gcaps: G21, G22, G23, G24\n",
    "\n",
    "These aggregate groups are used when comparing values between sampling campaigns.\n",
    "\n",
    "### Sampling campaigns\n",
    "\n",
    "The dates of the sampling campaigns are expanded to include the surveys that happened between large organized campaigns. The start and end dates are defined below.\n",
    "\n",
    "__Attention!!__ The codes used for each survey campaign are different. Different groups organized and conducted surveys using the MLW protocol. The data was then sent to us.\n",
    "\n",
    "__MCBP:__ November 2015 - November 2016. The initial sampling campaign. Fragmented plastics (Gfrags/G79/G78/G76) were not sorted by size. All unidentified hard plastic items were classified in this manner.\n",
    "\n",
    "* start_date = 2015-11-15\n",
    "* end_date = 2017-03-31\n",
    "\n",
    "__SLR:__ April 2017 - May 2018. Sampling campaign by the WWF. Objects less than 2.5 cm were not counted.\n",
    "\n",
    "* start_date = 2017-04-01\n",
    "* end_date = 2020-03-31\n",
    "\n",
    "__IQAASL:__ April 2020 - May 2021. Sampling campaign mandated by the Swiss confederation. Additional codes were added for regional objects.\n",
    "\n",
    "* start_date = 2020-04-01\n",
    "* end_date = 2021-05-31\n",
    "\n",
    "__Plastock (not added yet):__ January 2022 - December 2022. Sampling campaign from the Association pour la Sauvegarde du Léman. Not all objects were counted, They only identified a limited number of objects.\n",
    "\n",
    "### Feature name\n",
    "\n",
    "The feature name is the name of a river lake or other regional label that you would find on a map. People in the region know the name.\n",
    "\n",
    "### Feature type\n",
    "\n",
    "The feature type is a label that applies to general conditions of use for the location and other locations in the region\n",
    "\n",
    "* r: rivers: surveys on river banks\n",
    "* l: lake: surveys on the lake shore\n",
    "* p: parcs: surveys in recreational areas\n",
    "\n",
    "### Parent boundary\n",
    "\n",
    "Designates the larger geographic region of the survey location. For lakes and rivers it is the name of the catchment area or river basin. For parcs it is the the type of park ie.. les Alpes. Recall that each feature has a name, for example Alpes Lépontines is the the name of a feature in the geographic region of _Les Alpes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54148c36-ff96-4891-b230-479c5598f430",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "surveys = combine_survey_files(survey_files)\n",
    "codes = pd.read_csv(code_data).set_index(\"code\")\n",
    "beaches = pd.read_csv(beach_data).set_index(\"slug\")\n",
    "land_cover = pd.read_csv(land_cover_data)\n",
    "land_use = pd.read_csv(land_use_data)\n",
    "streets = pd.read_csv(street_data)\n",
    "river_intersect_lakes = pd.read_csv(intersection_attributes)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5e61c05f-baa2-4e9f-91ab-4d6840198579",
   "metadata": {},
   "source": [
    "## Aggregate a set of data by sample (location and date)\n",
    "\n",
    "Use the loc_date column in the survey data. Use the IQAASL period and four river baisns test against the federal report.\n",
    "\n",
    "### Before aggregating does the number of locations, cities, samples and quantity match the federal report?\n",
    "\n",
    "__The feature types include lakes and rivers, alpes were condsidered separately__\n",
    "\n",
    "From https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/lakes_rivers.html#\n",
    "\n",
    "1. cities = yes\n",
    "2. samples = yes\n",
    "3. locations = yes\n",
    "4. quantity = No it is short 50 pieces\n",
    "5. start and end date = yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bed0606-48a3-4d23-8de3-330e10b6f140",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# startint varaibles\n",
    "period = \"iqaasl\"\n",
    "survey_areas = [\"rhone\", \"ticino\", \"linth\", \"aare\"]\n",
    "start, end = [*period_dates[period]]\n",
    "# the surveys from the survey areas of intersest\n",
    "survey_data = slice_data_by_date(surveys.copy(), start, end)\n",
    "\n",
    "# the survey data sliced by the start and end data\n",
    "feature_d= survey_data[survey_data.parent_boundary.isin(survey_areas)].copy()\n",
    "\n",
    "# convert codes to gfrags, gcaps and gfoams\n",
    "feature_data = use_gfrags_gfoams_gcaps(feature_d.copy(), codes)\n",
    "\n",
    "# check the numbers\n",
    "feature_vitals = collect_vitals(feature_d)\n",
    "print(make_a_summary(feature_vitals))"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "e992515b-5a46-424a-987e-9565cbe69b31",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "# when the codes are changed to gfrags, gfoams and gcaps that creates \n",
    "# multiple code results for the same code at the same sample\n",
    "# note that the code_result_columns do not have the groupname column\n",
    "# this is because the code is changed and not the groupname\n",
    "code_result_df = aggregate_dataframe(feature_data.copy(), code_result_columns, unit_agg)\n",
    "code_result_df = code_result_df.merge(codes.groupname, left_on=\"code\", right_index=True)\n",
    "code_result_df = code_result_df.merge(beaches[[\"canton\",\"feature_type\"]], left_on='slug', right_index=True, validate=\"many_to_one\")"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "57569da8-0cbe-4acc-bfbd-95d660052c5e",
   "metadata": {},
   "source": [
    "### Number of lakes, rivers, parcs, cities and cantons"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "cd0087b3-5b25-44c9-963d-0a265fb0c36d",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "# load the language maps\n",
    "l_map = pd.read_csv('data/end_process/fr_labels.csv')\n",
    "lang = 'fr'\n",
    "l_mapi = l_map.set_index('en', drop=True)\n",
    "\n",
    "summary_of_work_data = summarize_work_data(code_result_df, code_result_df.canton.unique())\n",
    "summary_of_work_data = summary_of_work_data[['samples', 'cities', 'lakes', 'rivers', 'parks', 'quantity']]\n",
    "translated_and_style_for_display(summary_of_work_data, l_mapi, lang, gradient=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "cec69cc1-b826-47a6-a2fc-abe599e4481c",
   "metadata": {},
   "source": [
    "### aggregate to sample\n",
    "\n",
    "The assessments are made on a per sample basis. That means that we can look at an individual object value at each sample. The sum of all the individual objects in a survey is the total for that survey. Dividing the totals by the length of the survey gives the assessment metric: _pieces of trash per meter_.\n",
    "\n",
    "1. Are the quantiles of the current data  = to the federal report? Yes\n",
    "2. Are the material totals = to the federal report? No,plastics if off by 50 pcs\n",
    "3. Are the fail rates of the most common objects = to the federal report? Yes\n",
    "4. Is the % of total of the most common objects = to the fedral report? yes\n",
    "5. Is the median pieces/meter of the most common objects = to the federal report? yes\n",
    "6. Is the quantity of the most common objects = to the federal report? yes\n",
    "\n",
    "#### The summary of survey totals\n",
    "\n",
    "fig 1.5 in IQAASL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cfec738d-1a5b-4e14-98a5-765cd0a285b9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# the sample is the basic unit\n",
    "# loc_date is the unique identifier for each sample\n",
    "unit_columns = [\"loc_date\", \"slug\", \"parent_boundary\"]\n",
    "\n",
    "# the quantiles of the sample-total pcs/m  \n",
    "vector_summary = a_summary_of_one_vector(code_result_df.copy(), unit_columns, unit_agg, describe='pcs_m')\n",
    "\n",
    "translated_and_style_for_display(vector_summary,l_mapi, lang, gradient=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "5c6dd92c-60b1-47e9-a99f-036f0ab37272",
   "metadata": {},
   "source": [
    "#### Material totals and proportions\n",
    "\n",
    "fig 1.5 iqaal"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "92698c51-4640-41d1-be48-466c177d65f9",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# add the material label to each code\n",
    "merged_result = merge_dataframes_on_column_and_index(code_result_df.copy(), codes[\"material\"], 'code', how='inner', validate=True)\n",
    "\n",
    "# sum the materials for the data frame\n",
    "materials = aggregate_dataframe(merged_result.copy(), [\"material\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "# add 5 of total for display\n",
    "materials[\"%\"] = materials.quantity/materials.quantity.sum()\n",
    "\n",
    "translated_and_style_for_display(materials.set_index('material', drop=True),l_mapi, lang, gradient=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "086c5091-d3cd-45b3-b18f-43bffca65c11",
   "metadata": {},
   "source": [
    "#### Quantity, median pcs/m, fail rate, and % of total\n",
    "\n",
    "Sumary results for all the codes in the parent_boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "f996669d-61fb-480f-849b-a14acda37a03",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# sum the cumulative quantity for each code and calculate the median pcs/meter\n",
    "code_totals = aggregate_dataframe(code_result_df.copy(), [\"code\"], {\"quantity\":\"sum\", \"pcs_m\":\"median\"})\n",
    "\n",
    "# collect \n",
    "abundant = get_top_x_records_with_max_quantity(code_totals.copy(), \"quantity\", \"code\", len(code_totals))\n",
    "\n",
    "# identify the objects that were found in at least 50% of the samples\n",
    "# calculate the quantity per sample for each code and sample\n",
    "occurrences = aggregate_dataframe(code_result_df, [\"loc_date\", \"code\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "# count the number of times that an object was counted > 0\n",
    "# and divide it by the total number of samples \n",
    "event_counts  = count_objects_with_positive_quantity(occurrences)\n",
    "\n",
    "# calculate the rate of occurence per unit of measure\n",
    "rates = calculate_rate_per_unit(code_result_df, code_result_df.code.unique())\n",
    "\n",
    "# add the unit rates and fail rates\n",
    "abundance = merge_dataframes_on_column_and_index(abundant, rates[\"pcs_m\"], left_column=\"code\", validate=\"one_to_one\")\n",
    "abundance[\"fail rate\"] = abundance.code.apply(lambda x: event_counts.loc[x])\n",
    "\n",
    "# this is the complete inventory with summary\n",
    "# statistics for each objecabundance.sort_values(by=\"quantity\", inplace=True, ascending=False)\n",
    "abundance.reset_index(inplace=True, drop=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "2dd31a37-c5a7-40ec-9a8e-413a661530be",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### The most common objects\n",
    "\n",
    "fig 1.6 iqaasl"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "136b5ef1-7a8b-4bf0-8a64-e68d46e9b30c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "# arguments to slice the data by column\n",
    "column_one = {\n",
    "    'column': 'quantity',\n",
    "    'val': abundance.loc[10, 'quantity']\n",
    "}\n",
    "\n",
    "column_two = {\n",
    "    'column':'fail rate',\n",
    "    'val': 0.5\n",
    "}\n",
    "\n",
    "# use the inventory to find the most common objects\n",
    "the_most_common = display_tabular_data_by_column_values(abundance.copy(), column_one, column_two, 'code')\n",
    "\n",
    "translated_and_style_for_display(the_most_common.copy(),l_mapi, lang, gradient=False)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "060a06c9-fbc2-4764-b7d7-9074b43df7d8",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Results by groupname and feature boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "a5147fd7-7ed0-4fd3-8197-4e9df1554080",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "cumulative_columns = [\"loc_date\", \"groupname\"]\n",
    "unit_columns = [\"parent_boundary\", \"loc_date\", \"groupname\"]\n",
    "object_labels = code_result_df.groupname.unique()\n",
    "object_columns = [\"groupname\"]\n",
    "boundary_labels = code_result_df.parent_boundary.unique()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':object_labels,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(code_result_df.copy(), **args)\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "8b6b87fb-b4e3-49ec-8854-d6e3bb3baeb5",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Most common codes by feature boundary"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "a848a62e-48a7-4afe-8ef5-07b5407cda8c",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "cumulative_columns = [\"loc_date\", \"code\"]\n",
    "unit_columns = [\"parent_boundary\", \"loc_date\", \"code\"]\n",
    "codes_of_interest = the_most_common.index\n",
    "object_columns = [\"code\"]\n",
    "boundary_labels = code_result_df.parent_boundary.unique()\n",
    "\n",
    "data = code_result_df[code_result_df.code.isin(codes_of_interest)].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(data.copy(), **args)\n",
    "\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "a7b6f06d-a746-40f2-b947-571e173bba14",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Most common codes by canton\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7070f656-04e6-4cf0-b7cd-f36da7d040c2",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "unit_columns = [\"canton\", \"loc_date\", \"code\"]\n",
    "object_columns = [\"code\"]\n",
    "boundary_labels = code_result_df.canton.unique()\n",
    "\n",
    "data = code_result_df[code_result_df.code.isin(codes_of_interest)].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(data, **args)\n",
    "\n",
    "translated_and_style_for_display(tix.T,l_mapi, lang, gradient=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "09390343-28af-4e42-9a46-f65eeff30efa",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "### Most common codes: canton-municipal\n",
    "\n",
    "#### Bern"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "8309b868-964c-4f28-a9fd-d8dbbfda16af",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "canton = \"Bern\"\n",
    "\n",
    "with_cantons = code_result_df[code_result_df.canton == canton].copy()\n",
    "\n",
    "unit_columns = [\"city\", \"loc_date\", \"code\"]\n",
    "# the column that holds the labels of interest\n",
    "object_columns = [\"code\"]\n",
    "# the labels of interest for the boundary conditions\n",
    "boundary_labels = with_cantons.city.unique()\n",
    "\n",
    "ddata = with_cantons[(with_cantons.code.isin(codes_of_interest)) & (with_cantons.canton == \"Bern\")].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(ddata, **args)\n",
    "translated_and_style_for_display(tix.T,l_mapi, lang, gradient=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "id": "c2758c89-b920-46e9-b5b7-74bef904b377",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [
    "#### Valais"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "eab462fb-d5cb-4914-86cf-50e59aa26a07",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "hide-input"
    ]
   },
   "source": [
    "canton = \"Valais\"\n",
    "\n",
    "with_cantons = code_result_df[code_result_df.canton == canton].copy()\n",
    "\n",
    "unit_columns = [\"city\", \"loc_date\", \"code\"]\n",
    "# the column that holds the labels of interest\n",
    "object_columns = [\"code\"]\n",
    "# the labels of interest for the boundary conditions\n",
    "boundary_labels = with_cantons.city.unique()\n",
    "\n",
    "ddata = with_cantons[(with_cantons.code.isin(codes_of_interest))].copy()\n",
    "\n",
    "args = {\n",
    "    'cumulative_columns':cumulative_columns,\n",
    "    'object_labels':codes_of_interest,\n",
    "    'boundary_labels':boundary_labels,\n",
    "    'object_columns':object_columns,\n",
    "    'unit_agg':unit_agg,\n",
    "    'unit_columns':unit_columns,\n",
    "    'agg_groups':agg_groups\n",
    "}\n",
    "\n",
    "tix = summary_of_parent_and_child_features(ddata, **args)\n",
    "translated_and_style_for_display(tix,l_mapi, lang, gradient=True)"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "54fba949-faaf-45c5-a4b7-f0111a5c8872",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": [
     "remove-input"
    ]
   },
   "source": [
    "%watermark -a hammerdirt-analyst -co --iversions"
   ],
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ba1596c3-a6f5-4673-b9f6-c73099199d4a",
   "metadata": {
    "editable": true,
    "slideshow": {
     "slide_type": ""
    },
    "tags": []
   },
   "source": [],
   "outputs": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
