{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "64c16819-72da-4b47-a3aa-988d3f5a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Type, Optional\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "from review_methods_tests import collect_vitals, find_missing, find_missing_loc_dates\n",
    "from review_methods_tests import use_gfrags_gfoams_gcaps, make_a_summary,combine_survey_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "4558dc36-9186-4446-a24e-22cd22678f63",
   "metadata": {},
   "outputs": [],
   "source": [
    "def slice_data_by_date(data: pd.DataFrame, start: str, end: str):\n",
    "    mask = (data.date >= start) & (data.date <= end)\n",
    "    return data[mask]\n",
    "\n",
    "def aggregate_dataframe(df: pd.DataFrame,\n",
    "                        groupby_columns: List[str],\n",
    "                        aggregation_functions: Dict[str, Union[str, callable]],\n",
    "                        index: bool = False) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate specified columns in a Pandas DataFrame using given aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        groupby_columns (List[str]): List of column names to group by.\n",
    "        aggregation_functions (Dict[str, Union[str, callable]]): \n",
    "            A dictionary where keys are column names to aggregate, \n",
    "            and values are either aggregation functions (e.g., 'sum', 'mean', 'max', 'min')\n",
    "            or custom aggregation functions (callable functions).\n",
    "        index (bool, optional): Whether to use the groupby columns as an index.\n",
    "            Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with aggregated values.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(groupby_columns, as_index=index).agg(aggregation_functions)\n",
    "    \n",
    "    return grouped\n",
    "    \n",
    "def merge_dataframes_on_column_and_index(left_df: pd.DataFrame,\n",
    "                                         right_df: pd.DataFrame,\n",
    "                                         left_column: str,\n",
    "                                         how: str = 'inner',\n",
    "                                         validate: str = 'many_to_one') -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge two DataFrames where the left DataFrame is merged on a specified column and \n",
    "    the right DataFrame is merged on its index.\n",
    "\n",
    "    Args:\n",
    "        left_df (pd.DataFrame): The left DataFrame to be merged.\n",
    "        right_df (pd.DataFrame): The right DataFrame to be merged on its index.\n",
    "        left_column (str): The column in the left DataFrame to merge on.\n",
    "        how (str, optional): The type of merge to be performed ('left', 'right', 'outer', or 'inner'). \n",
    "            Default is 'inner'.\n",
    "        validate (str, optional): Whether to perform merge validation checks. \n",
    "            Default is 'many_to_one'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame resulting from the merge operation.\n",
    "    \"\"\"\n",
    "  \n",
    "    merged_df = left_df.merge(right_df, left_on=left_column, right_index=True, how=how)\n",
    "    return merged_df\n",
    "\n",
    "def get_top_x_records_with_max_quantity(df: pd.DataFrame, quantity_column: str, id_column: str, x: int):\n",
    "    \"\"\"\n",
    "    Get the top x records with the greatest quantity and their associated ID from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        quantity_column (str): The name of the quantity column.\n",
    "        id_column (str): The name of the ID column.\n",
    "        x (int): The number of records to return.\n",
    "\n",
    "    Returns:\n",
    "        An array of the top x objects by quantitye\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by the quantity column in descending order, take the top x records, and select the ID column\n",
    "    top_x_records = df.nlargest(x, quantity_column)[[id_column, quantity_column]]\n",
    "    return top_x_records[id_column].values\n",
    "\n",
    "def calculate_object_occurrence_rates(df: pd.DataFrame,\n",
    "                                      objects_to_check: List[str],\n",
    "                                      y: int,\n",
    "                                      j: int) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate the rate of occurrence for each object in a group of objects 'X' for a given quantity 'y' and number of samples 'j'\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "        objects_to_check (List[str]): The list of objects to calculate occurrence rates for.\n",
    "        y (int): The minimum quantity required for objects to be considered.\n",
    "        j (int): The total number of samples.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary where keys are objects and values are the rates of occurrence for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where 'object' is in 'objects_to_check' and quantity is greater than or equal to 'y'\n",
    "    filtered_df = df[(df['code'].isin(objects_to_check)) & (df['quantity'] >= y)]\n",
    "\n",
    "    # Calculate the occurrence rates for each object\n",
    "    occurrence_rates = {}\n",
    "    for obj in objects_to_check:\n",
    "        object_filtered_df = filtered_df[filtered_df['code'] == obj]\n",
    "        rate = len(object_filtered_df) / j if j > 0 else 0\n",
    "        occurrence_rates[obj] = rate\n",
    "\n",
    "    return occurrence_rates\n",
    "\n",
    "def calculate_quantity_proportions(df: pd.DataFrame,\n",
    "                                   objects_to_check: List[str]\n",
    "                                   ) -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate the proportion of the quantity of each object in a group of objects from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "        objects_to_check (List[str]): The list of objects to calculate proportions for.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary where keys are objects and values are their quantity proportions.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where 'object' is in 'objects_to_check'\n",
    "    filtered_df = df[df['code'].isin(objects_to_check)]\n",
    "\n",
    "    # Calculate the total quantity for each object\n",
    "    object_quantities = filtered_df.groupby('code')['quantity'].sum().to_dict()\n",
    "\n",
    "    # Calculate the proportion for each object\n",
    "    total_quantity = df.quantity.sum()\n",
    "    proportions = {obj: quantity / total_quantity for obj, quantity in object_quantities.items()}\n",
    "\n",
    "    return proportions\n",
    "\n",
    "\n",
    "\n",
    "def count_objects_with_positive_quantity(df: pd.DataFrame) -> Dict[str, int]:\n",
    "    \"\"\"\n",
    "    Count how many times each object had a quantity greater than zero in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the count of positive quantity occurrences for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where quantity is greater than zero\n",
    "    positive_quantity_df = df[df['quantity'] > 0]\n",
    "\n",
    "    # Count the occurrences of positive quantities for each object\n",
    "    object_counts = positive_quantity_df['code'].value_counts()\n",
    "\n",
    "    return object_counts"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca0862-a1e5-4fd3-ae17-65ac2fd7cd0a",
   "metadata": {},
   "source": [
    "# Testing data models\n",
    "\n",
    "The methods used in the version of the federal report were tested, but their was not a specific set of validation criteria beforehand. Test were done as the work progressed. This wasted alot of time\n",
    "\n",
    "here we test the land use and survey data models.\n",
    "\n",
    "1. is the land use data complete for each survey location?\n",
    "2. does the survey data aggregate correctly to sample level?\n",
    "   * what happens to objects with a quantity of zero?\n",
    "   * aggregating to cantonal, municipal or survey area\n",
    "     * are all locations included?\n",
    "     * are lakes and rivers distinguished?\n",
    "3. Does the aggregated data for iqaasl match the federal report?\n",
    "\n",
    "### Gfoams, Gfrags, Gcaps\n",
    "\n",
    "These are aggregate groups. It is difficult to infer how well a participant differentiates between size or use of the following codes.\n",
    "\n",
    "1. Gfrags: G79, G78, G75\n",
    "2. Gfoams: G81, G82, G76\n",
    "3. Gcaps: G21, G22, G23, G24\n",
    "\n",
    "These aggregate groups are used when comparing values between sampling campaigns.\n",
    "\n",
    "### Sampling campaigns\n",
    "\n",
    "The dates of the sampling campaigns are expanded to include the surveys that happened between large organized campaigns. The start and end dates are defined below.\n",
    "\n",
    "__Attention!!__ The codes used for each survey campaign are different. Different groups organized and conducted surveys using the MLW protocol. The data was then sent to us.\n",
    "\n",
    "__MCBP:__ November 2015 - November 2016. The initial sampling campaign. Fragmented plastics (Gfrags/G79/G78/G76) were not sorted by size. All unidentified hard plastic items were classified in this manner.\n",
    "\n",
    "* start_date = 2015-11-15\n",
    "* end_date = 2017-03-31\n",
    "\n",
    "__SLR:__ April 2017 - May 2018. Sampling campaign by the WWF. Objects less than 2.5 cm were not counted.\n",
    "\n",
    "* start_date = 2017-04-01\n",
    "* end_date = 2020-03-31\n",
    "\n",
    "__IQAASL:__ April 2020 - May 2021. Sampling campaign mandated by the Swiss confederation. Additional codes were added for regional objects.\n",
    "\n",
    "* start_date = 2020-04-01\n",
    "* end_date = 2021-05-31\n",
    "\n",
    "__Plastock (not added yet):__ January 2022 - December 2022. Sampling campaign from the Association pour la Sauvegarde du Léman. Not all objects were counted, They only identified a limited number of objects.\n",
    "\n",
    "### Feature type\n",
    "\n",
    "The feature type is a label that applies to general conditions of use for the location and other locations in the region\n",
    "\n",
    "* r: rivers: surveys on river banks\n",
    "* l: lake: surveys on the lake shore\n",
    "* p: parcs: surveys in recreational areas\n",
    "\n",
    "### Parent boundary\n",
    "\n",
    "Designates the larger geographic region of the survey location. For lakes and rivers it is the name of the catchment area or river basin. For parcs it is the the type of park ie.. les Alpes. Recall that each feature has a name, for example Alpes Lépontines is the the name of a feature in the geographic region of _Les Alpes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54148c36-ff96-4891-b230-479c5598f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# period dates\n",
    "period_dates = {\n",
    "    \"mcbp\":[\"2015-11-15\", \"2017-03-31\"],\n",
    "    \"slr\": [\"2017-04-01\", \"2020-02-28\"],\n",
    "    \"iqaasl\": [\"2020-03-01\", \"2021-05-31\"],\n",
    "    \"2022\": [\"2021-06-01\", \"2022-12-01\"]\n",
    "}\n",
    "code_cols = ['material', 'description', 'source', 'parent_code', 'single_use', 'groupname']\n",
    "\n",
    "group_by_columns = [\n",
    "    'loc_date', \n",
    "    'date',\n",
    "    'parent_boundary',\n",
    "    'feature_name',\n",
    "    'city',\n",
    "    'slug',\n",
    "    'length',\n",
    "    'groupname',\n",
    "    'code', \n",
    "]\n",
    "agg_this = {\n",
    "    \"quantity\":\"sum\",\n",
    "    \"pcs_m\": \"sum\"\n",
    "}\n",
    "\n",
    "survey_data = [\n",
    "    \"data/end_process/after_may_2021.csv\",\n",
    "    \"data/end_process/iqaasl.csv\",\n",
    "    \"data/end_process/mcbp.csv\",\n",
    "    \"data/end_process/slr.csv\",\n",
    "]\n",
    "\n",
    "source_data = \"data/end_process/new_allx.csv\"\n",
    "\n",
    "code_data =  \"data/end_process/codes.csv\"\n",
    "beach_data = \"data/end_process/beaches.csv\"\n",
    "land_cover_data = \"data/end_process/land_cover.csv\"\n",
    "land_use_data = \"data/end_process/land_use.csv\"\n",
    "street_data = \"data/end_process/streets.csv\"\n",
    "intersection_attributes = \"data/end_process/river_intersect_lakes.csv\"\n",
    "surveys = combine_survey_files(survey_data)\n",
    "codes = pd.read_csv(code_data).set_index(\"code\")\n",
    "beaches = pd.read_csv(beach_data).set_index(\"slug\")\n",
    "land_cover = pd.read_csv(land_cover_data)\n",
    "land_use = pd.read_csv(land_use_data)\n",
    "streets = pd.read_csv(street_data)\n",
    "river_intersect_lakes = pd.read_csv(intersection_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61c05f-baa2-4e9f-91ab-4d6840198579",
   "metadata": {},
   "source": [
    "## Aggregate a set of data by sample (location and date)\n",
    "\n",
    "Use the loc_date column in the survey data. Use the IQAASL period and four river baisns test against the federal report.\n",
    "\n",
    "### Before aggregating does the number of locations, cities, samples and quantity match the federal report?\n",
    "\n",
    "__The feature types include lakes and rivers, alpes were condsidered separately__\n",
    "\n",
    "From https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/lakes_rivers.html#\n",
    "\n",
    "1. cities = yes\n",
    "2. samples = yes\n",
    "3. locations = yes\n",
    "4. quantity = No it is short 50 pieces\n",
    "5. start and end date = yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bed0606-48a3-4d23-8de3-330e10b6f140",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Number of objects: 54694\n",
      "    \n",
      "    Median pieces/meter: 0.0\n",
      "    \n",
      "    Number of samples: 386\n",
      "    \n",
      "    Number of unique codes: 235\n",
      "    \n",
      "    Number of sample locations: 143\n",
      "    \n",
      "    Number of features: 28\n",
      "    \n",
      "    Number of cities: 77\n",
      "    \n",
      "    Start date: 2020-03-08\n",
      "    \n",
      "    End date: 2021-05-12\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "# startint varaibles\n",
    "period = \"iqaasl\"\n",
    "survey_areas = [\"rhone\", \"ticino\", \"linth\", \"aare\"]\n",
    "start, end = [*period_dates[period]]\n",
    "\n",
    "# the surveys from the survey areas of intersest\n",
    "survey_data = surveys[surveys.parent_boundary.isin(survey_areas)].copy()\n",
    "\n",
    "# the survey data sliced by the start and end data\n",
    "feature_d= slice_data_by_date(survey_data.copy(), start, end)\n",
    "\n",
    "# convert codes to gfrags, gcaps and gfoams\n",
    "feature_data = use_gfrags_gfoams_gcaps(feature_d.copy(), codes)\n",
    "\n",
    "# check the numbers\n",
    "feature_vitals = collect_vitals(feature_d)\n",
    "print(make_a_summary(feature_vitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69cc1-b826-47a6-a2fc-abe599e4481c",
   "metadata": {},
   "source": [
    "### aggregate to sample\n",
    "\n",
    "The assessments are made on a per sample basis. That means that we can look at an individual object value at each sample. The sum of all the individual objects in a survey is the total for that survey. Dividing the totals by the length of the survey gives the assessment metric: _pieces of trash per meter_.\n",
    "\n",
    "1. Are the quantiles of the current data  = to the federal report? Yes\n",
    "2. Are the material totals = to the federal report? No,plastics if off by 50 pcs\n",
    "3. Are the fail rates of the most common objects = to the federal report? Yes\n",
    "4. Is the % of total of the most common objects = to the fedral report? yes\n",
    "5. Is the median pieces/meter of the most common objects = to the federal report? yes\n",
    "6. Is the quantity of the most common objects = to the federal report? yes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfec738d-1a5b-4e14-98a5-765cd0a285b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>pcs_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>386.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>3.952073</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>7.063422</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.020000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.822500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>1.895000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>3.865000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>66.170000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>total</th>\n",
       "      <td>54694.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              pcs_m\n",
       "count    386.000000\n",
       "mean       3.952073\n",
       "std        7.063422\n",
       "min        0.020000\n",
       "25%        0.822500\n",
       "50%        1.895000\n",
       "75%        3.865000\n",
       "max       66.170000\n",
       "total  54694.000000"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# when the codes are changed to gfrags, gfoams and gcaps that creates \n",
    "# multiple code results for the same code at the same sample\n",
    "# note that the code_result_columns do not have the groupname column\n",
    "# this is because the code is changed and not the groupname\n",
    "code_result_columns = [\n",
    "    'loc_date', \n",
    "    'date',\n",
    "    'parent_boundary',\n",
    "    'feature_name',\n",
    "    'city',\n",
    "    'slug',\n",
    "    'length',\n",
    "    'code']\n",
    "\n",
    "code_result_df = aggregate_dataframe(feature_data.copy(), code_result_columns, agg_this)\n",
    "\n",
    "# aggregate the code totals on the sample day and check against the federal report\n",
    "sample_totals = aggregate_dataframe(code_result_df.copy(), [\"loc_date\", \"slug\", \"parent_boundary\"], agg_this)\n",
    "sample_summary = sample_totals.pcs_m.describe()\n",
    "sample_summary[\"total\"] = sample_totals.quantity.sum()\n",
    "pd.DataFrame(sample_summary)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "92698c51-4640-41d1-be48-466c177d65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material</th>\n",
       "      <th>quantity</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chemicals</td>\n",
       "      <td>140</td>\n",
       "      <td>0.002560</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cloth</td>\n",
       "      <td>343</td>\n",
       "      <td>0.006271</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass</td>\n",
       "      <td>2919</td>\n",
       "      <td>0.053370</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metal</td>\n",
       "      <td>1874</td>\n",
       "      <td>0.034263</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>1527</td>\n",
       "      <td>0.027919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>plastic</td>\n",
       "      <td>47093</td>\n",
       "      <td>0.861027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rubber</td>\n",
       "      <td>390</td>\n",
       "      <td>0.007131</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unidentified</td>\n",
       "      <td>2</td>\n",
       "      <td>0.000037</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wood</td>\n",
       "      <td>406</td>\n",
       "      <td>0.007423</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       material  quantity         %\n",
       "0     chemicals       140  0.002560\n",
       "1         cloth       343  0.006271\n",
       "2         glass      2919  0.053370\n",
       "3         metal      1874  0.034263\n",
       "4         paper      1527  0.027919\n",
       "5       plastic     47093  0.861027\n",
       "6        rubber       390  0.007131\n",
       "7  unidentified         2  0.000037\n",
       "8          wood       406  0.007423"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# add the material label to each code\n",
    "merged_result = merge_dataframes_on_column_and_index(code_result_df.copy(), codes[\"material\"], 'code', how='inner', validate=True)\n",
    "\n",
    "# sum the materials for the data frame\n",
    "materials = aggregate_dataframe(merged_result.copy(), [\"material\"], {\"quantity\":\"sum\"})\n",
    "materials[\"%\"] = materials.quantity/materials.quantity.sum()\n",
    "materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "40e45f36-28d6-4456-8e8f-ff02e2d5f0e5",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G30': 0.8523316062176166,\n",
       " 'G27': 0.8782383419689119,\n",
       " 'Gfoams': 0.6865284974093264,\n",
       " 'G95': 0.5077720207253886,\n",
       " 'Gfrags': 0.8626943005181347,\n",
       " 'G200': 0.6502590673575129,\n",
       " 'Gcaps': 0.6528497409326425,\n",
       " 'G178': 0.5233160621761658,\n",
       " 'G89': 0.5207253886010362,\n",
       " 'G67': 0.6968911917098446,\n",
       " 'G112': 0.3082901554404145,\n",
       " 'G74': 0.533678756476684}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# sum the cumulative quantity for each code and calculate the median pcs/meter\n",
    "code_totals = aggregate_dataframe(code_result_df.copy(), [\"code\"], {\"quantity\":\"sum\", \"pcs_m\":\"median\"})\n",
    "\n",
    "# find the top ten codes\n",
    "abundant = get_top_x_records_with_max_quantity(code_totals.copy(), \"quantity\", \"code\", 10)\n",
    "\n",
    "# identify the objects that were found in at least 50% of the samples\n",
    "# calculate the quantity per sample for each code and sample\n",
    "code_r = aggregate_dataframe(code_result_df, [\"loc_date\", \"code\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "# count the number of times that any object was found and\n",
    "# and divide it by the total number of samples \n",
    "result = count_objects_with_positive_quantity(code_r)/386\n",
    "\n",
    "# select the objects that were found in at least 50% of the surveys\n",
    "common = result[result >= .5]\n",
    "\n",
    "# combine the common with the abundant to make the most common objects\n",
    "most_common = list(set([*common.index, *abundant]))\n",
    "\n",
    "# return the fail rate for each of the most common\n",
    "the_most_common = code_totals[code_totals.code.isin(most_common)].copy()\n",
    "occurrence_rates = calculate_object_occurrence_rates(code_result_df[[\"code\", \"quantity\"]].copy(), most_common, 1, 386)\n",
    "occurrence_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "136b5ef1-7a8b-4bf0-8a64-e68d46e9b30c",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>code</th>\n",
       "      <th>quantity</th>\n",
       "      <th>pcs_m</th>\n",
       "      <th>fail_rate</th>\n",
       "      <th>%</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>14</th>\n",
       "      <td>G112</td>\n",
       "      <td>1968</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.308290</td>\n",
       "      <td>0.035982</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>76</th>\n",
       "      <td>G178</td>\n",
       "      <td>700</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.523316</td>\n",
       "      <td>0.012798</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>97</th>\n",
       "      <td>G200</td>\n",
       "      <td>2136</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.650259</td>\n",
       "      <td>0.039054</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>111</th>\n",
       "      <td>G27</td>\n",
       "      <td>8485</td>\n",
       "      <td>0.20</td>\n",
       "      <td>0.878238</td>\n",
       "      <td>0.155136</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>115</th>\n",
       "      <td>G30</td>\n",
       "      <td>3325</td>\n",
       "      <td>0.09</td>\n",
       "      <td>0.852332</td>\n",
       "      <td>0.060793</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147</th>\n",
       "      <td>G67</td>\n",
       "      <td>2534</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.696891</td>\n",
       "      <td>0.046330</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165</th>\n",
       "      <td>G74</td>\n",
       "      <td>1656</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.533679</td>\n",
       "      <td>0.030278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>170</th>\n",
       "      <td>G89</td>\n",
       "      <td>992</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.520725</td>\n",
       "      <td>0.018137</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>217</th>\n",
       "      <td>G95</td>\n",
       "      <td>1406</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.507772</td>\n",
       "      <td>0.025707</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>223</th>\n",
       "      <td>Gcaps</td>\n",
       "      <td>1844</td>\n",
       "      <td>0.03</td>\n",
       "      <td>0.652850</td>\n",
       "      <td>0.033715</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>224</th>\n",
       "      <td>Gfoams</td>\n",
       "      <td>5559</td>\n",
       "      <td>0.05</td>\n",
       "      <td>0.686528</td>\n",
       "      <td>0.101638</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>225</th>\n",
       "      <td>Gfrags</td>\n",
       "      <td>7400</td>\n",
       "      <td>0.18</td>\n",
       "      <td>0.862694</td>\n",
       "      <td>0.135298</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       code  quantity  pcs_m  fail_rate         %\n",
       "14     G112      1968   0.00   0.308290  0.035982\n",
       "76     G178       700   0.01   0.523316  0.012798\n",
       "97     G200      2136   0.03   0.650259  0.039054\n",
       "111     G27      8485   0.20   0.878238  0.155136\n",
       "115     G30      3325   0.09   0.852332  0.060793\n",
       "147     G67      2534   0.05   0.696891  0.046330\n",
       "165     G74      1656   0.01   0.533679  0.030278\n",
       "170     G89       992   0.01   0.520725  0.018137\n",
       "217     G95      1406   0.01   0.507772  0.025707\n",
       "223   Gcaps      1844   0.03   0.652850  0.033715\n",
       "224  Gfoams      5559   0.05   0.686528  0.101638\n",
       "225  Gfrags      7400   0.18   0.862694  0.135298"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# get the proportion of each code to the total quantity\n",
    "proportions = calculate_quantity_proportions(code_result_df, occurrence_rates.keys())\n",
    "\n",
    "# combine the results\n",
    "x  = code_totals[code_totals.code.isin(occurrence_rates.keys())].copy()\n",
    "x[\"fail_rate\"] = x.code.apply(lambda x: occurrence_rates[x])\n",
    "x[\"%\"] = x.code.apply(lambda x: proportions[x])\n",
    "x"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "54fba949-faaf-45c5-a4b7-f0111a5c8872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: hammerdirt-analyst\n",
      "\n",
      "conda environment: cantonal_report\n",
      "\n",
      "pandas: 2.0.3\n",
      "numpy : 1.25.2\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -a hammerdirt-analyst -co --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a947ae-9ba8-425e-bcf0-47c8207dfb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
