{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "64c16819-72da-4b47-a3aa-988d3f5a8203",
   "metadata": {},
   "outputs": [],
   "source": [
    "%load_ext watermark\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "from review_methods_tests import collect_vitals, find_missing, find_missing_loc_dates\n",
    "from review_methods_tests import use_gfrags_gfoams_gcaps, make_a_summary,combine_survey_files"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bca0862-a1e5-4fd3-ae17-65ac2fd7cd0a",
   "metadata": {},
   "source": [
    "# Testing data models\n",
    "\n",
    "The methods used in the version of the federal report were tested, but their was not a specific set of validation criteria beforehand. Test were done as the work progressed. This wasted alot of time\n",
    "\n",
    "here we test the land use and survey data models.\n",
    "\n",
    "1. is the land use data complete for each survey location?\n",
    "2. does the survey data aggregate correctly to sample level?\n",
    "   * what happens to objects with a quantity of zero?\n",
    "   * aggregating to cantonal, municipal or survey area\n",
    "     * are all locations included?\n",
    "     * are lakes and rivers distinguished?\n",
    "3. Does the aggregated data for iqaasl match the federal report?\n",
    "\n",
    "### Gfoams, Gfrags, Gcaps\n",
    "\n",
    "These are aggregate groups. It is difficult to infer how well a participant differentiates between size or use of the following codes.\n",
    "\n",
    "1. Gfrags: G79, G78, G75\n",
    "2. Gfoams: G81, G82, G76\n",
    "3. Gcaps: G21, G22, G23, G24\n",
    "\n",
    "These aggregate groups are used when comparing values between sampling campaigns.\n",
    "\n",
    "### Sampling campaigns\n",
    "\n",
    "The dates of the sampling campaigns are expanded to include the surveys that happened between large organized campaigns. The start and end dates are defined below.\n",
    "\n",
    "__Attention!!__ The codes used for each survey campaign are different. Different groups organized and conducted surveys using the MLW protocol. The data was then sent to us.\n",
    "\n",
    "__MCBP:__ November 2015 - November 2016. The initial sampling campaign. Fragmented plastics (Gfrags/G79/G78/G76) were not sorted by size. All unidentified hard plastic items were classified in this manner.\n",
    "\n",
    "* start_date = 2015-11-15\n",
    "* end_date = 2017-03-31\n",
    "\n",
    "__SLR:__ April 2017 - May 2018. Sampling campaign by the WWF. Objects less than 2.5 cm were not counted.\n",
    "\n",
    "* start_date = 2017-04-01\n",
    "* end_date = 2020-03-31\n",
    "\n",
    "__IQAASL:__ April 2020 - May 2021. Sampling campaign mandated by the Swiss confederation. Additional codes were added for regional objects.\n",
    "\n",
    "* start_date = 2020-04-01\n",
    "* end_date = 2021-05-31\n",
    "\n",
    "__Plastock (not added yet):__ January 2022 - December 2022. Sampling campaign from the Association pour la Sauvegarde du Léman. Not all objects were counted, They only identified a limited number of objects.\n",
    "\n",
    "### Feature type\n",
    "\n",
    "The feature type is a label that applies to general conditions of use for the location and other locations in the region\n",
    "\n",
    "* r: rivers: surveys on river banks\n",
    "* l: lake: surveys on the lake shore\n",
    "* p: parcs: surveys in recreational areas\n",
    "\n",
    "### Parent boundary\n",
    "\n",
    "Designates the larger geographic region of the survey location. For lakes and rivers it is the name of the catchment area or river basin. For parcs it is the the type of park ie.. les Alpes. Recall that each feature has a name, for example Alpes Lépontines is the the name of a feature in the geographic region of _Les Alpes_."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "54148c36-ff96-4891-b230-479c5598f430",
   "metadata": {},
   "outputs": [],
   "source": [
    "# period dates\n",
    "period_dates = {\n",
    "    \"mcbp\":[\"2015-11-15\", \"2017-03-31\"],\n",
    "    \"slr\": [\"2017-04-01\", \"2020-02-28\"],\n",
    "    \"iqaasl\": [\"2020-03-01\", \"2021-05-31\"],\n",
    "    \"2022\": [\"2021-06-01\", \"2022-12-01\"]\n",
    "}\n",
    "code_cols = ['material', 'description', 'source', 'parent_code', 'single_use', 'groupname']\n",
    "\n",
    "group_by_columns = [\n",
    "    'loc_date', \n",
    "    'date', \n",
    "    'feature_name', \n",
    "    'slug',     \n",
    "    'parent_boundary',\n",
    "    'length',\n",
    "    'groupname',\n",
    "    'city',\n",
    "    'code', \n",
    "]\n",
    "agg_this = {\n",
    "    \"quantity\":\"sum\",\n",
    "    \"pcs_m\": \"sum\"\n",
    "}\n",
    "\n",
    "survey_data = [\n",
    "    \"data/end_process/after_may_2021.csv\",\n",
    "    \"data/end_process/iqaasl.csv\",\n",
    "    \"data/end_process/mcbp.csv\",\n",
    "    \"data/end_process/slr.csv\",\n",
    "]\n",
    "\n",
    "source_data = \"data/end_process/new_allx.csv\"\n",
    "\n",
    "code_data =  \"data/end_process/codes.csv\"\n",
    "beach_data = \"data/end_process/beaches.csv\"\n",
    "land_cover_data = \"data/end_process/land_cover.csv\"\n",
    "land_use_data = \"data/end_process/land_use.csv\"\n",
    "street_data = \"data/end_process/streets.csv\"\n",
    "intersection_attributes = \"data/end_process/river_intersect_lakes.csv\"\n",
    "surveys = combine_survey_files(survey_data)\n",
    "codes = pd.read_csv(code_data).set_index(\"code\")\n",
    "beaches = pd.read_csv(beach_data).set_index(\"slug\")\n",
    "land_cover = pd.read_csv(land_cover_data)\n",
    "land_use = pd.read_csv(land_use_data)\n",
    "streets = pd.read_csv(street_data)\n",
    "river_intersect_lakes = pd.read_csv(intersection_attributes)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e61c05f-baa2-4e9f-91ab-4d6840198579",
   "metadata": {},
   "source": [
    "## Aggregate a set of data by sample (location and date)\n",
    "\n",
    "Use the loc_date column in the survey data. Use the IQAASL period and test against the federal report\n",
    "\n",
    "### Before aggregating does the number of locations, cities, samples and quantity match the federal report?\n",
    "\n",
    "__The feature types include lakes and rivers, alpes were condsidered separately__\n",
    "\n",
    "From https://hammerdirt-analyst.github.io/IQAASL-End-0f-Sampling-2021/lakes_rivers.html#\n",
    "\n",
    "1. cities = yes\n",
    "2. samples = yes\n",
    "3. locations = yes\n",
    "4. quantity = No it is short 50 pieces"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "9bed0606-48a3-4d23-8de3-330e10b6f140",
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "    Number of objects: 54694\n",
      "    \n",
      "    Median pieces/meter: 0.0\n",
      "    \n",
      "    Number of samples: 386\n",
      "    \n",
      "    Number of unique codes: 235\n",
      "    \n",
      "    Number of sample locations: 143\n",
      "    \n",
      "    Number of features: 28\n",
      "    \n",
      "    Number of cities: 77\n",
      "    \n",
      "    Start date: 2020-03-08\n",
      "    \n",
      "    End date: 2021-05-12\n",
      "    \n",
      "    \n"
     ]
    }
   ],
   "source": [
    "period = \"iqaasl\"\n",
    "survey_areas = [\"rhone\", \"ticino\", \"linth\", \"aare\"]\n",
    "start, end = [*period_dates[period]]\n",
    "survey_data = surveys[surveys.parent_boundary.isin(survey_areas)].copy()\n",
    "\n",
    "def slice_data_by_date(data, start, end):\n",
    "    mask = (data.date >= start) & (data.date <= end)\n",
    "    return data[mask]\n",
    "\n",
    "feature_d= slice_data_by_date(survey_data.copy(), start, end)\n",
    "\n",
    "\n",
    "feature_data = use_gfrags_gfoams_gcaps(feature_d.copy(), codes)\n",
    "feature_vitals = collect_vitals(feature_d)\n",
    "print(make_a_summary(feature_vitals))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cec69cc1-b826-47a6-a2fc-abe599e4481c",
   "metadata": {},
   "source": [
    "### aggregate to sample\n",
    "\n",
    "The assessments are made on a per sample basis. That means that we can look at an individual object value at each sample. The sum of all the individual objects in a survey is the total for that survey. Dividing the totals by the length of the survey gives the assessment metric: _pieces of trash per meter_.\n",
    "\n",
    "1. Are the quantiles of the current data  = to the federal report? Yes\n",
    "2. Are the material totals = to the federal report? No,plastics if off by 50 pcs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "cfec738d-1a5b-4e14-98a5-765cd0a285b9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "count    386.000000\n",
       "mean       3.952073\n",
       "std        7.063422\n",
       "min        0.020000\n",
       "25%        0.822500\n",
       "50%        1.895000\n",
       "75%        3.865000\n",
       "max       66.170000\n",
       "Name: pcs_m, dtype: float64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from typing import Type, Optional\n",
    "from typing import List, Dict, Union\n",
    "\n",
    "def aggregate_dataframe(\n",
    "    df: pd.DataFrame, \n",
    "    groupby_columns: List[str], \n",
    "    aggregation_functions: Dict[str, Union[str, callable]],\n",
    "    index: bool = False\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Aggregate specified columns in a Pandas DataFrame using given aggregation functions.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        groupby_columns (List[str]): List of column names to group by.\n",
    "        aggregation_functions (Dict[str, Union[str, callable]]): \n",
    "            A dictionary where keys are column names to aggregate, \n",
    "            and values are either aggregation functions (e.g., 'sum', 'mean', 'max', 'min')\n",
    "            or custom aggregation functions (callable functions).\n",
    "        index (bool, optional): Whether to use the groupby columns as an index.\n",
    "            Default is False.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame with aggregated values.\n",
    "    \"\"\"\n",
    "    grouped = df.groupby(groupby_columns, as_index=index).agg(aggregation_functions)\n",
    "    \n",
    "    return grouped\n",
    "    \n",
    "def merge_dataframes_on_column_and_index(\n",
    "    left_df: pd.DataFrame, \n",
    "    right_df: pd.DataFrame, \n",
    "    left_column: str, \n",
    "    how: str = 'inner',\n",
    "    validate: str = 'many_to_one'\n",
    ") -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Merge two DataFrames where the left DataFrame is merged on a specified column and \n",
    "    the right DataFrame is merged on its index.\n",
    "\n",
    "    Args:\n",
    "        left_df (pd.DataFrame): The left DataFrame to be merged.\n",
    "        right_df (pd.DataFrame): The right DataFrame to be merged on its index.\n",
    "        left_column (str): The column in the left DataFrame to merge on.\n",
    "        how (str, optional): The type of merge to be performed ('left', 'right', 'outer', or 'inner'). \n",
    "            Default is 'inner'.\n",
    "        validate (str, optional): Whether to perform merge validation checks. \n",
    "            Default is 'many_to_one'.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A new DataFrame resulting from the merge operation.\n",
    "    \"\"\"\n",
    "  \n",
    "    merged_df = left_df.merge(right_df, left_on=left_column, right_index=True, how=how)\n",
    "    return merged_df\n",
    "\n",
    "code_result_df = aggregate_dataframe(feature_data.copy(), group_by_columns, agg_this)\n",
    "sample_totals = aggregate_dataframe(code_result_df.copy(), [\"loc_date\", \"slug\", \"parent_boundary\"], agg_this)\n",
    "sample_totals.pcs_m.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "92698c51-4640-41d1-be48-466c177d65f9",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>material</th>\n",
       "      <th>quantity</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>chemicals</td>\n",
       "      <td>140</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>cloth</td>\n",
       "      <td>343</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>glass</td>\n",
       "      <td>2919</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>metal</td>\n",
       "      <td>1874</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>paper</td>\n",
       "      <td>1527</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>plastic</td>\n",
       "      <td>47093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6</th>\n",
       "      <td>rubber</td>\n",
       "      <td>390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7</th>\n",
       "      <td>unidentified</td>\n",
       "      <td>2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>8</th>\n",
       "      <td>wood</td>\n",
       "      <td>406</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       material  quantity\n",
       "0     chemicals       140\n",
       "1         cloth       343\n",
       "2         glass      2919\n",
       "3         metal      1874\n",
       "4         paper      1527\n",
       "5       plastic     47093\n",
       "6        rubber       390\n",
       "7  unidentified         2\n",
       "8          wood       406"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "merged_result = merge_dataframes_on_column_and_index(code_result_df.copy(), codes[\"material\"], 'code', how='inner', validate=True)\n",
    "materials = aggregate_dataframe(merged_result.copy(), [\"material\"], {\"quantity\":\"sum\"})\n",
    "materials"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "id": "0651eb12-de4d-4d39-8229-3ed011028f1e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['G27' 'Gfrags' 'Gfoams' 'G30' 'G67' 'G200' 'G112' 'Gcaps' 'G74' 'G95']\n"
     ]
    }
   ],
   "source": [
    "def get_top_x_records_with_max_quantity(df, quantity_column, id_column, x):\n",
    "    \"\"\"\n",
    "    Get the top x records with the greatest quantity and their associated ID from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame.\n",
    "        quantity_column (str): The name of the quantity column.\n",
    "        id_column (str): The name of the ID column.\n",
    "        x (int): The number of records to return.\n",
    "\n",
    "    Returns:\n",
    "        pd.DataFrame: A DataFrame with the top x records having the greatest quantity and the associated ID.\n",
    "    \"\"\"\n",
    "    # Sort the DataFrame by the quantity column in descending order, take the top x records, and select the ID column\n",
    "    top_x_records = df.nlargest(x, quantity_column)[[id_column, quantity_column]]\n",
    "    return top_x_records[id_column].values\n",
    "\n",
    "\n",
    "\n",
    "code_totals = aggregate_dataframe(code_result_df.copy(), [\"code\"], {\"quantity\":\"sum\", \"pcs_m\":\"median\"})\n",
    "abundant = get_top_x_records_with_max_quantity(code_totals.copy(), \"quantity\", \"code\", 10)\n",
    "print(abundant)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "0cb07553-f36b-48b5-a521-59ac3abac799",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'G74': 0.694300518134715,\n",
       " 'G200': 0.6502590673575129,\n",
       " 'G67': 0.6968911917098446,\n",
       " 'G112': 0.3082901554404145,\n",
       " 'G30': 0.8523316062176166,\n",
       " 'Gfoams': 0.9119170984455959,\n",
       " 'Gfrags': 1.2642487046632125,\n",
       " 'G95': 0.5077720207253886,\n",
       " 'Gcaps': 1.2202072538860103,\n",
       " 'G89': 0.5207253886010362,\n",
       " 'G27': 0.8782383419689119,\n",
       " 'G178': 0.5233160621761658}"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def count_objects_with_positive_quantity(df):\n",
    "    \"\"\"\n",
    "    Count how many times each object had a quantity greater than zero in a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "\n",
    "    Returns:\n",
    "        pd.Series: A Series with the count of positive quantity occurrences for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where quantity is greater than zero\n",
    "    positive_quantity_df = df[df['quantity'] > 0]\n",
    "\n",
    "    # Count the occurrences of positive quantities for each object\n",
    "    object_counts = positive_quantity_df['code'].value_counts()\n",
    "\n",
    "    return object_counts\n",
    "code_r = aggregate_dataframe(code_result_df, [\"loc_date\", \"code\"], {\"quantity\":\"sum\"})\n",
    "\n",
    "result = count_objects_with_positive_quantity(code_r)/386\n",
    "common = result[result >= .5]\n",
    "\n",
    "most_common = list(set([*common.index, *abundant]))\n",
    "\n",
    "the_most_common = code_totals[code_totals.code.isin(most_common)].copy()\n",
    "\n",
    "import pandas as pd\n",
    "from typing import List, Dict\n",
    "\n",
    "def calculate_object_occurrence_rates(\n",
    "    df: pd.DataFrame,\n",
    "    objects_to_check: List[str],\n",
    "    y: int,\n",
    "    j: int\n",
    ") -> Dict[str, float]:\n",
    "    \"\"\"\n",
    "    Calculate the rate of occurrence for each object in a group of objects 'X' for a given quantity 'y' and number of samples 'j'\n",
    "    from a DataFrame.\n",
    "\n",
    "    Args:\n",
    "        df (pd.DataFrame): The input DataFrame with columns 'sample,' 'object,' and 'quantity.'\n",
    "        objects_to_check (List[str]): The list of objects to calculate occurrence rates for.\n",
    "        y (int): The minimum quantity required for objects to be considered.\n",
    "        j (int): The total number of samples.\n",
    "\n",
    "    Returns:\n",
    "        Dict[str, float]: A dictionary where keys are objects and values are the rates of occurrence for each object.\n",
    "    \"\"\"\n",
    "    # Filter the DataFrame to include rows where 'object' is in 'objects_to_check' and quantity is greater than or equal to 'y'\n",
    "    filtered_df = df[(df['code'].isin(objects_to_check)) & (df['quantity'] >= y)]\n",
    "\n",
    "    # Calculate the occurrence rates for each object\n",
    "    occurrence_rates = {}\n",
    "    for obj in objects_to_check:\n",
    "        object_filtered_df = filtered_df[filtered_df['code'] == obj]\n",
    "        rate = len(object_filtered_df) / j if j > 0 else 0\n",
    "        occurrence_rates[obj] = rate\n",
    "\n",
    "    return occurrence_rates\n",
    "\n",
    "\n",
    "occurrence_rates = calculate_object_occurrence_rates(code_result_df, most_common, 1, 386)\n",
    "occurrence_rates"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "id": "650825e5-3f9d-425c-a977-d7eef02e61da",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>loc_date</th>\n",
       "      <th>date</th>\n",
       "      <th>feature_name</th>\n",
       "      <th>slug</th>\n",
       "      <th>parent_boundary</th>\n",
       "      <th>length</th>\n",
       "      <th>groupname</th>\n",
       "      <th>city</th>\n",
       "      <th>code</th>\n",
       "      <th>quantity</th>\n",
       "      <th>pcs_m</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>aabach</td>\n",
       "      <td>linth</td>\n",
       "      <td>30</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Schmerikon</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>6</td>\n",
       "      <td>0.20</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>149</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>aabach</td>\n",
       "      <td>linth</td>\n",
       "      <td>32</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Schmerikon</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>226</th>\n",
       "      <td>('aabach', '2020-10-22')</td>\n",
       "      <td>2020-10-22</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>aabach</td>\n",
       "      <td>linth</td>\n",
       "      <td>33</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Schmerikon</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>1</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>237</th>\n",
       "      <td>('aare-limmatspitz', '2020-07-13')</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>aare</td>\n",
       "      <td>aare-limmatspitz</td>\n",
       "      <td>aare</td>\n",
       "      <td>100</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Gebenstorf</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>385</th>\n",
       "      <td>('aare-limmatspitz', '2020-07-13')</td>\n",
       "      <td>2020-07-13</td>\n",
       "      <td>aare</td>\n",
       "      <td>aare-limmatspitz</td>\n",
       "      <td>aare</td>\n",
       "      <td>114</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Gebenstorf</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88239</th>\n",
       "      <td>('zurichsee_wollishofen_langendorfm', '2020-11...</td>\n",
       "      <td>2020-11-12</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>linth</td>\n",
       "      <td>100</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>2</td>\n",
       "      <td>0.02</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88248</th>\n",
       "      <td>('zurichsee_wollishofen_langendorfm', '2020-12...</td>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>linth</td>\n",
       "      <td>66</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>2</td>\n",
       "      <td>0.03</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88387</th>\n",
       "      <td>('zurichsee_wollishofen_langendorfm', '2020-12...</td>\n",
       "      <td>2020-12-10</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>linth</td>\n",
       "      <td>68</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88626</th>\n",
       "      <td>('zurichsee_wollishofen_langendorfm', '2021-01...</td>\n",
       "      <td>2021-01-10</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>linth</td>\n",
       "      <td>43</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88856</th>\n",
       "      <td>('zurichsee_wollishofen_langendorfm', '2021-02...</td>\n",
       "      <td>2021-02-12</td>\n",
       "      <td>zurichsee</td>\n",
       "      <td>zurichsee_wollishofen_langendorfm</td>\n",
       "      <td>linth</td>\n",
       "      <td>55</td>\n",
       "      <td>plastic pieces</td>\n",
       "      <td>Zürich</td>\n",
       "      <td>Gfrags</td>\n",
       "      <td>0</td>\n",
       "      <td>0.00</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>750 rows × 11 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                loc_date        date  \\\n",
       "3                               ('aabach', '2020-10-22')  2020-10-22   \n",
       "149                             ('aabach', '2020-10-22')  2020-10-22   \n",
       "226                             ('aabach', '2020-10-22')  2020-10-22   \n",
       "237                   ('aare-limmatspitz', '2020-07-13')  2020-07-13   \n",
       "385                   ('aare-limmatspitz', '2020-07-13')  2020-07-13   \n",
       "...                                                  ...         ...   \n",
       "88239  ('zurichsee_wollishofen_langendorfm', '2020-11...  2020-11-12   \n",
       "88248  ('zurichsee_wollishofen_langendorfm', '2020-12...  2020-12-10   \n",
       "88387  ('zurichsee_wollishofen_langendorfm', '2020-12...  2020-12-10   \n",
       "88626  ('zurichsee_wollishofen_langendorfm', '2021-01...  2021-01-10   \n",
       "88856  ('zurichsee_wollishofen_langendorfm', '2021-02...  2021-02-12   \n",
       "\n",
       "      feature_name                               slug parent_boundary  length  \\\n",
       "3        zurichsee                             aabach           linth      30   \n",
       "149      zurichsee                             aabach           linth      32   \n",
       "226      zurichsee                             aabach           linth      33   \n",
       "237           aare                   aare-limmatspitz            aare     100   \n",
       "385           aare                   aare-limmatspitz            aare     114   \n",
       "...            ...                                ...             ...     ...   \n",
       "88239    zurichsee  zurichsee_wollishofen_langendorfm           linth     100   \n",
       "88248    zurichsee  zurichsee_wollishofen_langendorfm           linth      66   \n",
       "88387    zurichsee  zurichsee_wollishofen_langendorfm           linth      68   \n",
       "88626    zurichsee  zurichsee_wollishofen_langendorfm           linth      43   \n",
       "88856    zurichsee  zurichsee_wollishofen_langendorfm           linth      55   \n",
       "\n",
       "            groupname        city    code  quantity  pcs_m  \n",
       "3      plastic pieces  Schmerikon  Gfrags         6   0.20  \n",
       "149    plastic pieces  Schmerikon  Gfrags         0   0.00  \n",
       "226    plastic pieces  Schmerikon  Gfrags         1   0.03  \n",
       "237    plastic pieces  Gebenstorf  Gfrags         2   0.02  \n",
       "385    plastic pieces  Gebenstorf  Gfrags         0   0.00  \n",
       "...               ...         ...     ...       ...    ...  \n",
       "88239  plastic pieces      Zürich  Gfrags         2   0.02  \n",
       "88248  plastic pieces      Zürich  Gfrags         2   0.03  \n",
       "88387  plastic pieces      Zürich  Gfrags         0   0.00  \n",
       "88626  plastic pieces      Zürich  Gfrags         0   0.00  \n",
       "88856  plastic pieces      Zürich  Gfrags         0   0.00  \n",
       "\n",
       "[750 rows x 11 columns]"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "code_result_df[code_result_df.code == \"Gfrags\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "id": "00b13a35-0de3-4ceb-ac4a-c95dadd2549e",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['loc_date',\n",
       " 'date',\n",
       " 'feature_name',\n",
       " 'slug',\n",
       " 'parent_boundary',\n",
       " 'length',\n",
       " 'groupname',\n",
       " 'city',\n",
       " 'code']"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "group_by_columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "54fba949-faaf-45c5-a4b7-f0111a5c8872",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Author: hammerdirt-analyst\n",
      "\n",
      "conda environment: cantonal_report\n",
      "\n",
      "numpy : 1.25.2\n",
      "pandas: 2.0.3\n",
      "\n"
     ]
    }
   ],
   "source": [
    "%watermark -a hammerdirt-analyst -co --iversions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "61a947ae-9ba8-425e-bcf0-47c8207dfb8d",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
